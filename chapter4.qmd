---
title: "Linear Model with R: Chapter 4"
author: Han Chenyue
editor: source
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  # echo = FALSE,
  digits = 4,
  scipen = 2
)
```

# Chapter 4

## Exercises

### Q1

```{r}
set.seed(101)
n <- 30
n_simulations <- 4000
x <- seq(from = 0, to = 1, length.out = n)
y <- x + rcauchy(n = n, location = 0, scale = 1)
lm_q1a <- lm(y ~ x)
summary(lm_q1a)
summary(lm_q1a)$coef[2, ]
plot(x, y)
abline(lm_q1a)

slope <- matrix(data = NA, nrow = n_simulations, ncol = 2)

for (i in 1:n_simulations) {
  y <- x + rcauchy(n = n, location = 0, scale = 1)
  lm_q1b <- lm(y ~ x)
  slope[i, 1] <- summary(lm_q1b)$coef[2, 1] # First column contains estimates
  slope[i, 2] <- summary(lm_q1b)$coef[2, 2] # Second column contains std errors
}
```

```{r}
summary(slope[, 1])
```

```{r}
summary(slope[, 2])
```

-   Although the median of $0.8765$ and the mean of $1.4804$ align reasonably well with the expected value of one, the minimum value of $-4203.4762$ and the maximum value of $7815.6476$ indicate the presence of extreme outlier estimates.
-   The estimated standard errors display a large mean of $21.2135$ and presence of extremely large values with a maximum ceiling of $4509.8138$.
-   The theoretical properties of the least squares estimator require the error distribution to have finite mean and variance, but the Cauchy distribution violates these conditions, leading to unstable and sometimes extreme estimation results.
    -   In statistics, the Gaussâ€“Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).
-   Long-tailed error distributions are problematic because most simulated observations appear ordinary, creating an impression of stability. However, rare but severe outliers inevitably occur, and the estimator is not robust enough to handle these extreme values.

### Q2

#### q2a

The problem uses a Student's *t*-distribution with three degrees of freedom. The formula for the variance of a *t*-distribution with $\nu$ degree(s) of freedom is

$$
\operatorname{Var}(X) = \frac{\nu}{\nu - 2}, \qquad \nu > 2
$$

**Reference**: *Statistical Inference* (Casella and Berger, 2nd ed., p. 507).

Plugging in $\nu = 3$:

$$
\operatorname{Var}(t_3) = \frac{3}{3 - 2} = 3
$$

To force our error term $\epsilon$ to have a standard variance of one, we use the property of variance scaling:

$$
\operatorname{Var}(\frac{X}{c}) = \frac{1}{c^2} \cdot \operatorname{Var}(X)
$$

$$
\operatorname{Var}(\frac{t_3}{\sqrt{3}}) = \frac{1}{(\sqrt{3})^2} \cdot \operatorname{Var}(t_3)
$$

$$
\operatorname{Var}(\frac{t_3}{\sqrt{3}}) = \frac{1}{3} \cdot 3 = 1
$$

By dividing the generated numbers by $\sqrt{3}$, we retain the shape of the *t*-distribution (heavier tails, where the probability mass in the tails is controlled by the parameter $\nu$). For $\nu = 1$, the Student's *t*-distribution $t_v$ becomes the standard Cauchy distributions, which has very **fat** tails; whereas for $\nu \rightarrow \infty$, it converges to the standard normal distribution $N (0, 1)$

```{r}
set.seed(37)
n <- 30
n_simulations <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = n_simulations, ncol = 4)

for (i in 1:n_simulations) {
  y <- rt(n = n, df = 3) / sqrt(3)
  lm_q2a <- lm(y ~ x)
  slope[i, ] <- summary(lm_q2a)$coef[2, ]
  # slope[i, 1] <- summary(lm_q2a)$coef[2, 1]  # First column contains estimates
  # slope[i, 2] <- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors
  # slope[i, 3] <- summary(lm_q2a)$coef[2, 3]  # Third column contains t values
  # slope[i, 4] <- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values
}

summary(lm_q2a)$coef[2, ]
```

```{r}
plot(x, y)
abline(lm_q2a)
```

#### q2b

```{r}
mean(slope[, 1])
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = 0.0145$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

$$
\bar{\hat{\beta_1}} \approx \beta_1
$$

$$
\bar{\hat{\beta_1}} \approx E(\hat{\beta_1})
$$

#### q2c

```{r}
sd(slope[, 1])
```

The standard deviation of $\hat{\beta_1} = 0.6064$, which is close to the square root of the theoretical variance that was computed with Equation 2.21, $\operatorname{Var}(\hat{\beta}) = (X^T X)^{-1} \sigma^2$, on page 57.

```{r}
mean(slope[, 2])
```

The mean of simulated $SE(\hat{\beta_1}) = 0.5622$ does not agree with computed standard deviation of $\hat{\beta_1} = 0.6064$.

In fact, it reported a value less than our previously computed result.

Remember, $0.6064$ is the actual standard deviation generated from our estimates and $0.5622$ is the standard errors reported by the `lm()` function in each run. Take note that `lm()` assumes that the errors are normally distributed and does not have outliers (*However, it is more precise to say that Normal distribution has "thin tails" which suggests outliers are possible but rare*).

Now, a skeptic might ask: "*Maybe* $0.5622$ is just different from $0.6064$ because of random chance (simulation noise)."

To address this, we compute a $95\%$ confidence interval:

```{r}
t.test(slope[, 2])$conf
```

In this specific case, the null hypothesis for the above test was: "The `lm()` model's reported error is accurate (*equal to 0.6064*)"

The confidence interval rests between $0.5559$ and $0.5686$ of which $0.6064$ is effectively far below the actual standard deviation. The difference between the "**Reported Error** and the "**Actual Error**" is **statistically significant,** proving that when errors have heavy tails ($t_3$), the standard linear model underestimates the uncertainty.

If we use standard least squares regression on data that has heavy-tailed errors ($t_3$):

-   Our slope estimates are still approximately unbiased (*close to zero*).
-   But our standard errors are too small (*biased downwards*).
-   This meant that our confidence intervals will be too narrow and we will claim statistical significance too often, as our p-values will be artificially low due to presence of outlier(s).

#### q2d

```{r}
mean(slope[, 4] < 0.05)
```

#### q2e

-   Initially, we generated the data with the assumption that there is no relationship to be found.
    -   Null hypothesis: $\beta = 0$.
-   If the model finds a "*statistically significant*" relationship, it is making a **Type I error**, or **false positive error** (*Defined as the incorrect rejection of the true null hypothesis in statistical hypothesis testing*).
-   If we set our confidence level to $95\%, \quad (\alpha = 0.05)$, we implicitly agreed to accept a $5\%$ error rate.
    -   That is, we expect to be wrong $5\%$ of the time (*nominal size of the error rate*).
-   In $4.2\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant (*actual size of the error rate*).
-   Since the figures are close to each another, the hypothesis test is behaving exactly as we initially assumed.
-   Do take note, that in part (c), we have proved that the standard errors were biased downwards.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.
    -   However, the result here indicates that despite the errors being non-normal ($t_3$) and the standard errors being biased, the t-test is remarkly robust as it managed to preserve the correct Type I error rate.

### Q3

#### q3a

```{r}
set.seed(37)
n <- 30
n_simulations <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = n_simulations, ncol = 4)

for (i in 1:n_simulations) {
  y <- arima.sim(
    model = list(
      order = c(1, 0, 0),
      ar = 0.9
    ),
    n = 30,
  )

  lm_q3a <- lm(y ~ x)
  slope[i, ] <- summary(lm_q3a)$coef[2, ]
  # slope[i, 1] <- summary(lm_q2a)$coef[2, 1]  # First column contains estimates
  # slope[i, 2] <- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors
  # slope[i, 3] <- summary(lm_q2a)$coef[2, 3]  # Third column contains t values
  # slope[i, 4] <- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values
}

summary(lm_q3a)$coef[2, ]
```

```{r}
plot(x, y)
abline(lm_q3a)
```

Yes, the correlated errors are apparent.

#### q3b

```{r}
mean(slope[, 1])
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = -0.0167$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

#### q3c

```{r}
sd(slope[, 1])
```

SD of the simulated $\hat{\beta_1}$ is $3.4614$.

```{r}
mean(slope[, 2])
```

Mean of simulated $\SE(hat{\beta_1})$ is $0.8354$.

The estimated standard error generated by `lm()` substantially underestimate the true standard error.

#### q3d

```{r}
round(mean(slope[, 4] < 0.05) * 100, 1)
```

-   In $62.6\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated SE for $\beta_1$.
    -   The `lm()` function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.

#### q3e

-   In statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor (*referred to as a "common response variable", "confounding factor", or "lurking variable"*).
-   In $62.6\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated SE for $\beta_1$.
    -   The `lm()` function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.
-   This time, unlike the previous question, we observed that when errors are strongly correlated, inference to a divergence from the uncorrelated errors assumption becomes inaccurate.

### Q4

#### q4a

-   To enforce the requirement "*SD proportional to* $x$", we multiply the standard deviation with $x$ such that as $x$ gets larger, the spread also get larger

```{r}
set.seed(37)
n <- 30
n_simulations <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = n_simulations, ncol = 4)

for (i in 1:n_simulations) {
  y <- rnorm(n = n, mean = 0, sd = x * 1)
  lm_q4a <- lm(y ~ x)
  slope[i, ] <- summary(lm_q4a)$coef[2, ]
}

summary(lm_q4a)$coef[2, ]
```

```{r}
plot(x, y)
abline(lm_q4a)
```

Yes, as $x$ increases, the dispersion becomes much more apparent. In other words, the data exhibits clear heteroscedasticity.

#### q4b

```{r}
round(mean(slope[, 1]), 4)
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = -0.0027$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

#### q4c

```{r}
sd(slope[, 1])
```

The standard deviation of the simulated $\hat{\beta_1}$ is $0.3961$.

```{r}
mean(slope[, 2])
```

The mean of the simulated $SE(hat{\beta_1})$ is `r mean(slope[, 2])`, which is slightly smaller than standard deviation of the simulated $\hat{\beta_1}$. Thus, it underestimates the true standard error.

#### q4d

```{r}
round(mean(slope[, 4] < 0.05) * 100, 1)
```

-   In `r round(mean(slope[, 4] < 0.05) * 100, 1)`% of our `r n_simulations` simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated standard error for $\beta_1$.
    -   The `lm()` function assumes homoscedasticity. It treats the high-variance data points (large $x$) as having the same precision as the low-variance data points. Because it fails to account for the extra noise introduced as $x$ increases, the standard error formula yields a value that is too small.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.

### Q5

#### q5a

```{r}
edf <- data.frame(
  y <- c(1.21, 1.13, 1.42, 1.01, 1.11, 0.94, 1.23, 1.04),
  g <- c(0, 0, 0, 0, 1, 1, 1, 1)
)
```

```{r}
lm_q5a <- lm(y ~ g, data = edf)
lm_q5a_summary <- summary(lm_q5a)
lm_q5a_summary
```

```{r}
lm_q5a_summary$fstatistic[1]
```

$\text{F-statistics} = `r lm_q5a_summary$fstatistic[1]`$

```{r}
1 -
  pf(
    q = lm_q5a_summary$fstatistic[1],
    df1 = lm_q5a_summary$fstatistic[2],
    df2 = lm_q5a_summary$fstatistic[3]
  )
```

$\text{p-value} = `r 1 - pf(q = lm_q5a_summary$fstatistic[1], df1 = lm_q5a_summary$fstatistic[2], df2 = lm_q5a_summary$fstatistic[3])`$

#### q5b

```{r}
allcomb <- combn(x = 8, m = 4)
ncol(allcomb)
```

`r ncol(allcomb)` combinations exist.

#### q5c

We use combinations instead of permutations because **order within the group does not matter**, the results are identical regardless of whether group 1 is defined as $\{1, 3, 5, 9\}$ or $\{9, 5, 1, 3\}$. Using combinations reduces the computational load significantly without losing information.

#### q5d

```{r}
fstats <- numeric(ncol(allcomb))
for (i in 1:ncol(allcomb)) {
  col_i <- allcomb[, i]
  selected_y_based_on_col_i <- c(edf$y[col_i], edf$y[-col_i]) # The first 4 items belong to g = 0, the rest belong to g = 1
  fstats[i] <- summary(lm(selected_y_based_on_col_i ~ edf$g))$fstatistic[1]
}
```

#### q5e

```{r}
mean(fstats > lm_q5a_summary$fstatistic[1])
```

As discussed in section 4.3, since the p-values computed based on the assumption of normal errors and those based on permutations (*in this case, by means of combinations*) are so close, many would prefer the normal assumption-based tests because they are quicker and easier to compute while the latter approach requires more work to compute despite making fewer assumptions.

### Q6

#### q6a

```{r}
data(happy, package = "faraway")
lm_q6a <- lm(happy ~ ., data = happy)
summary(lm_q6a)
```

Only the predictor `love` was statistically significant at the $1\%$ level. The predictor work was significant at the $5\%$ level, but not at the $1\%$ level."

#### q6b

```{r}
table(happy$happy)
```

The summary shows the response variable is discrete, consisting only of integers. Since linear regression assumes a continuous response to generate normally distributed errors, this assumption is questionable here.

#### q6c

```{r}
set.seed(37)
n_repetitions <- 4000
tstats <- numeric(n_repetitions)
for (i in 1:n_repetitions) {
  lm_q6c <- lm(happy ~ sample(money) + sex + love + work, data = happy)
  tstats[i] <- summary(lm_q6c)$coef[2, 3]
}

mean(abs(tstats) > abs(summary(lm_q6a)$coef[2, 3]))
```

The outcome returns `r mean(abs(tstats) > abs(summary(lm_q6a)$coef[2, 3]))`, which is very close to the observed normal-based *p*-value of `r round(summary(lm_q6a)$coef[2, 4], 4)`.

#### q6d

```{r}
hist(tstats, freq = FALSE, main = "Distribution of Permuted t-Statistics")
```

#### q6e

```{r}
hist(tstats, freq = FALSE, main = "Distribution of Permuted t-Statistics")
grid <- seq(-3, 3, length = 300)
lines(grid, dt(grid, 34))
```

#### q6f

```{r}
set.seed(37)
coefficient_matrix <- matrix(data = NA, nrow = 4000, ncol = 5)
residual <- residuals(lm_q6a)
prediction <- fitted(lm_q6a)

for (i in 1:4000) {
  bootstrapped_responses <- prediction + sample(residual, rep = TRUE)
  bootstrapped_model <- update(lm_q6a, bootstrapped_responses ~ .)
  coefficient_matrix[i, ] = coef(bootstrapped_model)
}

colnames(coefficient_matrix) = c("Intercept", colnames(happy)[-1]) # Removed the first column name of the "happy" dataset is "happy" (the responses)
coefficient_matrix_data_frame <- data.frame(coefficient_matrix)
head(coefficient_matrix_data_frame)
```

```{r}
round(
  apply(
    X = coefficient_matrix_data_frame["money"],
    MARGIN = 2,
    FUN = function(x) quantile(x = x, probs = c(0.05, 0.95))
  ),
  5
)
```

Zero does not fall within the 90% intervals. We reject the null hypothesis.

```{r}
round(
  apply(
    X = coefficient_matrix_data_frame["money"],
    MARGIN = 2,
    FUN = function(x) quantile(x = x, probs = c(0.025, 0.975))
  ),
  5
)
```

Zero does fall within the 95% intervals. We do not reject the null hypothesis.

In the original model output, the p-value for the test of $\beta_{money}$ is `r round(summary(lm_q6a)$coefficients[2, 4], 3)` which lies between the 5% and 10% significance levels. Hence, we do not expect the 90% confidence interval to contain zero but we do for the 95% confidence interval.