---
title: "Linear_Model_with_R"
author: Han Chenyue
editor: source
format: html
---

# Chapter 4

## Exercises

### Q1

```{r}
set.seed(101)
n <- 30
nsim <-  4000
x <- seq(from = 0, to = 1, length.out = n)
y <- x + rcauchy(n = n, location = 0, scale = 1)
lm_q1a <- lm(y ~ x)
summary(lm_q1a)
summary(lm_q1a)$coef[2, ]
plot(x, y)
abline(lm_q1a)

slope <- matrix(data = NA, nrow = nsim, ncol = 2)

for(i in 1:nsim){
    y <- x + rcauchy(n = n, location = 0, scale = 1)
    lm_q1b <- lm(y ~ x)
    slope[i, 1] <- summary(lm_q1b)$coef[2, 1]  # First column contains estimates
    slope[i, 2] <- summary(lm_q1b)$coef[2, 2]  # Second column contains std errors
}
```

```{r}
round(summary(slope[, 1]), 4)
```

```{r}
round(summary(slope[, 2]), 4)
```

-   Although the median of $0.8765$ and the mean of $1.4804$ align reasonably well with the expected value of one, the minimum value of $-4203.4762$ and the maximum value of $7815.6476$ indicate the presence of extreme outlier estimates.
-   The estimated standard errors display a large mean of $21.2135$ and presence of extremely large values with a maximum ceiling of $4509.8138$.
-   The theoretical properties of the least squares estimator require the error distribution to have finite mean and variance, but the Cauchy distribution violates these conditions, leading to unstable and sometimes extreme estimation results.
    -   In statistics, the Gaussâ€“Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).
-   Long-tailed error distributions are problematic because most simulated observations appear ordinary, creating an impression of stability. However, rare but severe outliers inevitably occur, and the estimator is not robust enough to handle these extreme values.

### Q2

#### q2a

The problem uses a Student's *t*-distribution with three degrees of freedom. The formula for the variance of a *t*-distribution with $\nu$ degree(s) of freedom is

$$
\operatorname{Var}(X) = \frac{\nu}{\nu - 2}, \qquad \nu > 2
$$

**Reference**: *Statistical Inference* (Casella and Berger, 2nd ed., p. 507).

Plugging in $\nu = 3$:

$$
\operatorname{Var}(t_3) = \frac{3}{3 - 2} = 3
$$

To force our error term $\epsilon$ to have a standard variance of one, we use the property of variance scaling:

$$
\operatorname{Var}(\frac{X}{c}) = \frac{1}{c^2} \cdot \operatorname{Var}(X)
$$

$$
\operatorname{Var}(\frac{t_3}{\sqrt{3}}) = \frac{1}{(\sqrt{3})^2} \cdot \operatorname{Var}(t_3)
$$

$$
\operatorname{Var}(\frac{t_3}{\sqrt{3}}) = \frac{1}{3} \cdot 3 = 1
$$

By dividing the generated numbers by $\sqrt{3}$, we retain the shape of the *t*-distribution (heavier tails, where the probability mass in the tails is controlled by the parameter $\nu$). For $\nu = 1$, the Student's *t*-distribution $t_v$ becomes the standard Cauchy distributions, which has very **fat** tails; whereas for $\nu \rightarrow \infty$, it converges to the standard normal distribution $N (0, 1)$

```{r}
set.seed(37)
n <- 30
nsim <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = nsim, ncol = 4)

for(i in 1:nsim){
    y <- rt(n = n, df = 3) / sqrt(3)
    lm_q2a <- lm(y ~ x)
    slope[i, ] <- summary(lm_q2a)$coef[2, ]
    # slope[i, 1] <- summary(lm_q2a)$coef[2, 1]  # First column contains estimates
    # slope[i, 2] <- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors
    # slope[i, 3] <- summary(lm_q2a)$coef[2, 3]  # Third column contains t values
    # slope[i, 4] <- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values
}

round(summary(lm_q2a)$coef[2, ], 4)
```

```{r}
plot(x, y)
abline(lm_q2a)
```

#### q2b

```{r}
round(mean(slope[, 1]), 4)
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = 0.0145$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

$$
\bar{\hat{\beta_1}} \approx \beta_1
$$

$$
\bar{\hat{\beta_1}} \approx E(\hat{\beta_1})
$$

#### q2c

```{r}
round(sd(slope[, 1]), 4)
```

The standard deviation of $\hat{\beta_1} = 0.6064$, which is close to the square root of the theoretical variance that was computed with Equation 2.21, $\operatorname{Var}(\hat{\beta}) = (X^T X)^{-1} \sigma^2$, on page 57.

```{r}
round(mean(slope[, 2]), 4)
```

The mean of simulated $SE(\hat{\beta_1}) = 0.5622$ does not agree with computed standard deviation of $\hat{\beta_1} = 0.6064$.

In fact, it reported a value less than our previously computed result.

Remember, $0.6064$ is the actual standard deviation generated from our estimates and $0.5622$ is the standard errors reported by the `lm()` function in each run. Take note that `lm()` assumes that the errors are normally distributed and does not have outliers (*However, it is more precise to say that Normal distribution has "thin tails" which suggests outliers are possible but rare*).

Now, a skeptic might ask: "*Maybe* $0.5622$ is just different from $0.6064$ because of random chance (simulation noise)."

To address this, we compute a $95\%$ confidence interval:

```{r}
round(t.test(slope[, 2])$conf, 4)
```

In this specific case, the null hypothesis for the above test was: "The `lm()` model's reported error is accurate (*equal to 0.6064*)"

The confidence interval rests between $0.5559$ and $0.5686$ of which $0.6064$ is effectively far below the actual standard deviation. The difference between the "**Reported Error** and the "**Actual Error**" is **statistically significant,** proving that when errors have heavy tails ($t_3$), the standard linear model underestimates the uncertainty.

If we use standard least squares regression on data that has heavy-tailed errors ($t_3$):

-   Our slope estimates are still approximately unbiased (*close to zero*).
-   But our standard errors are too small (*biased downwards*).
-   This meant that our confidence intervals will be too narrow and we will claim statistical significance too often, as our p-values will be artificially low due to presence of outlier(s).

#### q2d

```{r}
round(mean(slope[, 4] < 0.05), 4)
```

#### q2e

-   Initially, we generated the data with the assumption that there is no relationship to be found.
    -   Null hypothesis: $\beta = 0$.
-   If the model finds a "*statistically significant*" relationship, it is making a **Type I error**, or **false positive error** (*Defined as the incorrect rejection of the true null hypothesis in statistical hypothesis testing*).
-   If we set our confidence level to $95\%, \quad (\alpha = 0.05)$, we implicitly agreed to accept a $5\%$ error rate.
    -   That is, we expect to be wrong $5\%$ of the time (*nominal size of the error rate*).
-   In $4.2\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant (*actual size of the error rate*).
-   Since the figures are close to each another, the hypothesis test is behaving exactly as we initially assumed.
-   Do take note, that in part (c), we have proved that the standard errors were biased downwards.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.
    -   However, the result here indicates that despite the errors being non-normal ($t_3$) and the standard errors being biased, the t-test is remarkly robust as it managed to preserve the correct Type I error rate.

### Q3

#### q3a

```{r}
set.seed(37)
n <- 30
nsim <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = nsim, ncol = 4)

for(i in 1:nsim){
    y <- arima.sim(
        model = list(
            order = c(1, 0, 0),
            ar = 0.9
        ),
        n = 30,
    )

    lm_q3a <- lm(y ~ x)
    slope[i, ] <- summary(lm_q3a)$coef[2, ]
    # slope[i, 1] <- summary(lm_q2a)$coef[2, 1]  # First column contains estimates
    # slope[i, 2] <- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors
    # slope[i, 3] <- summary(lm_q2a)$coef[2, 3]  # Third column contains t values
    # slope[i, 4] <- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values
}

round(summary(lm_q3a)$coef[2, ], 4)
```

```{r}
plot(x, y)
abline(lm_q3a)
```

Yes, the correlated errors are apparent.

#### q3b

```{r}
round(mean(slope[, 1]), 4)
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = -0.0167$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

#### q3c

```{r}
round(sd(slope[, 1]), 4)
```

SD of the simulated $\hat{\beta_1}$ is $3.4614$.

```{r}
round(mean(slope[, 2]), 4)
```

Mean of simulated $\SE(hat{\beta_1})$ is $0.8354$.

The estimated standard error generated by `lm()` substantially underestimate the true standard error.

#### q3d

```{r}
round(mean(slope[, 4] < 0.05), 4)
```

-  In $62.6\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated SE for $\beta_1$.
    -   The `lm()` function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.

#### q3e

-   In statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor (*referred to as a "common response variable", "confounding factor", or "lurking variable"*).
-   In $62.6\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated SE for $\beta_1$.
    -   The `lm()` function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.
-   This time, unlike the previous question, we observed that when errors are strongly correlated, inference to a divergence from the uncorrelated errors assumption becomes inaccurate.

### Q4

#### q4a

-   To enforce the requirement "*SD proportional to $x$*", we multiply the standard deviation with $x$ such that as $x$ gets larger, the spread also get larger

```{r}
set.seed(37)
n <- 30
nsim <- 4000
x <- seq(from = 0, to = 1, length.out = n)
slope <- matrix(data = NA, nrow = nsim, ncol = 4)

for (i in 1:nsim){
    y <- rnorm(n = n, mean = 0, sd = x * 1)
    lm_q4a <- lm(y ~ x)
    slope[i, ] <- summary(lm_q4a)$coef[2, ]
}

round(summary(lm_q4a)$coef[2, ], 4)
```

```{r}
plot(x, y)
abline(lm_q4a)
```

Yes, as $x$ increases, the dispersion becomes much more apparent. In other words, the data exhibits clear heteroscedasticity.

#### q4b

```{r}
round(mean(slope[, 1]), 4)
```

The mean of our simulated estimates, $\bar{\hat{\beta_1}} = -0.0027$, is close to the true theoretical value of $\beta_1 = 0$ and the theoretical expectation $E(\hat{\beta_1}) = 0$.

#### q4c

```{r}
round(sd(slope[, 1]), 4)
```

The standard deviation of the simulated $\hat{\beta_1}$ is $0.3961$.

```{r}
round(mean(slope[, 2]), 4)
```

The mean of the simulated $SE(hat{\beta_1})$ is $0.3502$, which is slightly smaller than standard deviation of the simulated $\hat{\beta_1}$. Thus, it underestimates the true standard error.

#### q4d

```{r}
round(mean(slope[, 4] < 0.05), 4)
```

-  In $8.25\%$ of our $4000$ simulations, the model claimed that the `slope` was statistically significant even though the null hypothesis is true.
    -   This is a consequence of the underestimated SE for $\beta_1$.
    -   The `lm()` function assumes homoscedasticity.  It treats the high-variance data points (large $x$) as having the same precision as the low-variance data points. Because it fails to account for the extra noise introduced as $x$ increases, the standard error formula yields a value that is too small.
    -   Standard errors are too small $\longrightarrow$ t-values are too big $\longrightarrow$ p-values are too small $\longrightarrow$ rejection rate should be high.

### Q5

#### q5a