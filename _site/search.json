[
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\ndata(prostate, package = \"faraway\")\nlm_q1a &lt;- lm(lpsa ~ ., data = prostate)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = lpsa ~ ., data = prostate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7331 -0.3713 -0.0170  0.4141  1.6381 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.669337   1.296387   0.516  0.60693    \nlcavol       0.587022   0.087920   6.677 2.11e-09 ***\nlweight      0.454467   0.170012   2.673  0.00896 ** \nage         -0.019637   0.011173  -1.758  0.08229 .  \nlbph         0.107054   0.058449   1.832  0.07040 .  \nsvi          0.766157   0.244309   3.136  0.00233 ** \nlcp         -0.105474   0.091013  -1.159  0.24964    \ngleason      0.045142   0.157465   0.287  0.77503    \npgg45        0.004525   0.004421   1.024  0.30886    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7084 on 88 degrees of freedom\nMultiple R-squared:  0.6548,    Adjusted R-squared:  0.6234 \nF-statistic: 20.86 on 8 and 88 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\ncat(\n  \"90% CI for age: \",\n  round(\n    summary(lm_q1a)$coef[4, 1] +\n      c(-1, 1) * qt(0.95, 97 - 9) * summary(lm_q1a)$coef[4, 2],\n    4\n  )\n)\n\n\n90% CI for age:  -0.0382 -0.0011\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\n  \"95% CI for age: \",\n  round(\n    summary(lm_q1a)$coef[4, 1] +\n      c(-1, 1) * qt(0.975, 97 - 9) * summary(lm_q1a)$coef[4, 2],\n    4\n  )\n)\n\n\n95% CI for age:  -0.0418 0.0026\n\n\nVisualising it:\n\n\nCode\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nci_90 &lt;- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.90) |&gt;\n  filter(term == \"age\") |&gt;\n  mutate(level = \"90% CI (p-value &lt;= 0.10, alpha = 0.10)\")\n\nci_95 &lt;- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.95) |&gt;\n  filter(term == \"age\") |&gt;\n  mutate(level = \"95% CI (p-value &lt;= 0.05, alpha = 0.05)\")\n\nci_combined &lt;- bind_rows(ci_90, ci_95)\n\nggplot(ci_combined, aes(x = estimate, y = level, colour = level)) +\n  geom_errorbarh(\n    aes(xmin = conf.low, xmax = conf.high),\n    height = 0.1,\n    linewidth = 1.5\n  ) +\n  geom_point(size = 5, shape = 19) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"black\",\n    linewidth = 1\n  ) +\n  labs(\n    title = \"Bracketing the p-value for the 'age' Coefficient\",\n    x = \"Coefficient Value\",\n  ) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"none\")\n\n\nWarning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n90% CI for age does not include 0 but 95% CI for age does, therefore the p-value lies within 0.05 and 0.10.\n\n\n\n\n\nCode\nlibrary(ellipse)\n\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n\nCode\nplot(ellipse(lm_q1a, c(4, 5)), type = \"l\")\npoints(coef(lm_q1a)[4], coef(lm_q1a)[5])\nabline(v = confint(lm_q1a)[4, c(1, 2)], lty = 4)\nabline(h = confint(lm_q1a)[5, c(1, 2)], lty = 4)\nabline(v = 0, lty = 2, col = \"blue\")\nabline(h = 0, lty = 2, col = \"blue\")\npoints(0, 0, pch = 21)\n\n\n\n\n\n\n\n\n\n\nOrigin (\\(\\beta_age = 0\\), \\(\\beta_lbph = 0\\)) falls within both the \\(95\\%\\) joint confidence ellipse and the rectangular region defined by the individual \\(95\\%\\) univariate confidence intervals.\nNeither predictor is statistically significant when considered in isolation within the full model.\nCrucially, because the origin lies inside the joint confidence ellipse, we do not reject the null hypothesis that both coefficients are simultaneously zero. This is consistent with the result of the formal F-test for nested models:\n\n\n\nCode\nlm_q1b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lcp + gleason + pgg45,\n  data = prostate\n)\nanova(lm_q1b, lm_q1a)\n\n\nAnalysis of Variance Table\n\nModel 1: lpsa ~ lcavol + lweight + svi + lcp + gleason + pgg45\nModel 2: lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + \n    pgg45\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     90 46.768                              \n2     88 44.163  2    2.6048 2.5951 0.08034 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe non-significant p-value from this F-test confirms the visual evidence from the ellipse: we fail to find sufficient statistical evidence to reject the null hypothesis.\n\n\n\n\n\n\nCode\nlm_q1c &lt;- lm(lpsa ~ lcavol + lweight + svi, data = prostate)\nsummary(lm_q1c)\n\n\n\nCall:\nlm(formula = lpsa ~ lcavol + lweight + svi, data = prostate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.72964 -0.45764  0.02812  0.46403  1.57013 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.26809    0.54350  -0.493  0.62298    \nlcavol       0.55164    0.07467   7.388  6.3e-11 ***\nlweight      0.50854    0.15017   3.386  0.00104 ** \nsvi          0.66616    0.20978   3.176  0.00203 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7168 on 93 degrees of freedom\nMultiple R-squared:  0.6264,    Adjusted R-squared:  0.6144 \nF-statistic: 51.99 on 3 and 93 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nanova(lm_q1c, lm_q1a)\n\n\nAnalysis of Variance Table\n\nModel 1: lpsa ~ lcavol + lweight + svi\nModel 2: lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + \n    pgg45\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     93 47.785                           \n2     88 44.163  5    3.6218 1.4434 0.2167\n\n\nSince p-value is much larger than \\(\\alpha = 0.05\\), we fail to reject the null hypothesis, \\(H_0: \\beta_{age} = \\beta_{lbph} = \\beta_{lcp} = \\beta_{gleason} = \\beta_{pgg45} = 0\\).\nWe conclude that the five predictors removed from the full model do not collectively contribute statistically significant explanatory power to the model, hence the smaller model is preferable.\n\n\n\n\n\n\n\n\nCode\ndata(cheddar, package = \"faraway\")\n\n\n\n\nCode\nlm_q2a &lt;- lm(taste ~ ., data = cheddar)\nsummary(lm_q2a)\n\n\n\nCall:\nlm(formula = taste ~ ., data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.390  -6.612  -1.009   4.908  25.449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -28.8768    19.7354  -1.463  0.15540   \nAcetic        0.3277     4.4598   0.073  0.94198   \nH2S           3.9118     1.2484   3.133  0.00425 **\nLactic       19.6705     8.6291   2.280  0.03108 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.13 on 26 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6116 \nF-statistic: 16.22 on 3 and 26 DF,  p-value: 3.81e-06\n\n\nH2S and Lactic predictors are statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\nlm_q2b &lt;- lm(taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)\nsummary(lm_q2b)\n\n\n\nCall:\nlm(formula = taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.209  -7.266  -1.651   7.385  26.335 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.897e+01  1.127e+01  -1.684   0.1042  \nexp(Acetic)  1.891e-02  1.562e-02   1.210   0.2371  \nexp(H2S)     7.668e-04  4.188e-04   1.831   0.0786 .\nLactic       2.501e+01  9.062e+00   2.760   0.0105 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.19 on 26 degrees of freedom\nMultiple R-squared:  0.5754,    Adjusted R-squared:  0.5264 \nF-statistic: 11.75 on 3 and 26 DF,  p-value: 4.746e-05\n\n\nLactic predictor is statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\nanova(lm_q2b, lm_q2a)\n\n\nAnalysis of Variance Table\n\nModel 1: taste ~ exp(Acetic) + exp(H2S) + Lactic\nModel 2: taste ~ Acetic + H2S + Lactic\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1     26 3253.6                      \n2     26 2668.4  0     585.2         \n\n\n\nNo, because an F-test is only valid for comparing nested models, where one model is a restricted version of the other (i.e. the predictors of the smaller model are a smaller subset of the larger model).\nIn this case, the models rely on different functional forms of the predictor [\\(log(x)\\) vs \\(x\\)], thus the column space of one model is not a linear subspace the other.\nSince \\(R^2_{lm\\_q2a} = 0.6518 &gt; R^2_{lm\\_q2b} = 0.5754\\) and \\(RSS_{lm\\_q2a} = 2668.4 &lt; RSS_{lm\\_q2b} = 3253.6\\), the first model provides a better fit to the data.\n\n\n\n\n\n\nCode\ncat(\"Taste would increase by\", round(0.01 * coef(lm_q2a)['H2S'], 3))\n\n\nTaste would increase by 0.039\n\n\n\n\n\n\n\nCode\ncat(\n  \"The H2S on the original scale will increase by \",\n  (exp(0.01) - 1) * 100,\n  \"% when adding 0.01 on the (natural) log scale as adding 0.01 to the log scale is equivalent to multiplying by exp(0.01) on the original scale.\",\n  sep = \"\"\n)\n\n\nThe H2S on the original scale will increase by 1.005017% when adding 0.01 on the (natural) log scale as adding 0.01 to the log scale is equivalent to multiplying by exp(0.01) on the original scale.\n\n\n\n\n\n\n\n\n\n\nCode\ndata(teengamb, package = \"faraway\")\nlm_q3a &lt;- lm(gamble ~ ., data = teengamb)\nsummary(lm_q3a)\n\n\n\nCall:\nlm(formula = gamble ~ ., data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n\nSex and Income predictors are statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\ncat(\n  \"For females, the difference in average weekly gambling between females and males, holding other factors constant, is\",\n  signif(summary(lm_q3a)$coefficients[2], 3),\n  \"pounds.\"\n)\n\n\nFor females, the difference in average weekly gambling between females and males, holding other factors constant, is -22.1 pounds.\n\n\n\n\n\n\n\nCode\nlm_q3c &lt;- lm(gamble ~ income, data = teengamb)\nsummary(lm_q3c)\n\n\n\nCall:\nlm(formula = gamble ~ income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.020 -11.874  -3.757  11.934 107.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.325      6.030  -1.049      0.3    \nincome         5.520      1.036   5.330 3.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.95 on 45 degrees of freedom\nMultiple R-squared:  0.387, Adjusted R-squared:  0.3734 \nF-statistic: 28.41 on 1 and 45 DF,  p-value: 3.045e-06\n\n\n\n\nCode\nanova(lm_q3c, lm_q3a)\n\n\nAnalysis of Variance Table\n\nModel 1: gamble ~ income\nModel 2: gamble ~ sex + status + income + verbal\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     45 28009                              \n2     42 21624  3    6384.8 4.1338 0.01177 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince p-value is much smaller than \\(\\alpha = 0.05\\), we reject the null hypothesis, \\(H_0: \\beta_{sex} = \\beta_{status} = \\beta_{verbal} = 0\\).\nWe conclude that the full model is preferred.\n\n\n\n\n\n\n\n\nCode\ndata(sat, package = \"faraway\")\n\n\n\n\nCode\nlm_q4a_i &lt;- lm(total ~ expend + ratio + salary, data = sat)\nsummary(lm_q4a_i)\n\n\n\nCall:\nlm(formula = total ~ expend + ratio + salary, data = sat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-140.911  -46.740   -7.535   47.966  123.329 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1069.234    110.925   9.639 1.29e-12 ***\nexpend        16.469     22.050   0.747   0.4589    \nratio          6.330      6.542   0.968   0.3383    \nsalary        -8.823      4.697  -1.878   0.0667 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68.65 on 46 degrees of freedom\nMultiple R-squared:  0.2096,    Adjusted R-squared:  0.1581 \nF-statistic: 4.066 on 3 and 46 DF,  p-value: 0.01209\n\n\nOptional:\n\n\nCode\n# Dropping salary\nlm_q4a_ii &lt;- lm(total ~ expend + ratio, data = sat)\nanova(lm_q4a_ii, lm_q4a_i)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ expend + ratio\nModel 2: total ~ expend + ratio + salary\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     47 233443                              \n2     46 216812  1     16631 3.5285 0.06667 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# Dropping all predictors\nlm_q4a_iii &lt;- lm(total ~ 1, data = sat)\nanova(lm_q4a_iii, lm_q4a_i)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ 1\nModel 2: total ~ expend + ratio + salary\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     49 274308                              \n2     46 216812  3     57496 4.0662 0.01209 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsalary is not statistically significant (\\(\\text{p-value}_\\text{salary} = 0.0667 &lt; 0.05\\)).\nAll the individual t-tests are not statistically significant yet the overall F-test is.\nThe p-value for the F-statistics is less than \\(\\text{p-value}_\\text{F-statistics} = 0.01209 &lt; 0.05\\), we reject the null hypothesis \\(H_0: \\beta_\\text{salary} = \\beta_\\text{ratio} = \\beta_\\text{expend} = 0\\).\nThis indicates that at least one of the predictor has a statistically significant effect on the response.\nThis usually happens when the predictors are highly correlated with each other and we have a situation where the whole is greater than the sum of its parts.\n\n\n\n\n\n\nCode\nlm_q4b &lt;- lm(total ~ expend + ratio + salary + takers, data = sat)\nsummary(lm_q4b)\n\n\n\nCall:\nlm(formula = total ~ expend + ratio + salary + takers, data = sat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-90.531 -20.855  -1.746  15.979  66.571 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1045.9715    52.8698  19.784  &lt; 2e-16 ***\nexpend         4.4626    10.5465   0.423    0.674    \nratio         -3.6242     3.2154  -1.127    0.266    \nsalary         1.6379     2.3872   0.686    0.496    \ntakers        -2.9045     0.2313 -12.559 2.61e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.7 on 45 degrees of freedom\nMultiple R-squared:  0.8246,    Adjusted R-squared:  0.809 \nF-statistic: 52.88 on 4 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nanova(lm_q4a_i, lm_q4b)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ expend + ratio + salary\nModel 2: total ~ expend + ratio + salary + takers\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 216812                                  \n2     45  48124  1    168688 157.74 2.607e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey output the same p-value.\nSince \\(t_i = \\frac{\\hat{\\beta}_i}{se(\\hat{\\beta}_i)}\\)\n\n\nCode\nlibrary(dplyr)\nnear(\n  summary(lm_q4b)$coefficients[5, 1] / summary(lm_q4b)$coefficients[5, 2],\n  summary(lm_q4b)$coefficients[5, 3]\n)\n\n\n[1] TRUE\n\n\nSince the sum-of-squares decomposition and F-statistics reduces to the usual equal-variance (pooled) two sample t-test in the case of \\(\\alpha = 2\\) treatments - with the realisation that an F-statistics with one numerator and \\(k\\) denominator degrees of freedom is equivalent to a t-statistics with \\(k\\) degrees of freedom, viz: \\(F_{1,k} = t^2_{k}\\)\nLink to Proof\n\n\n\n\n\nGoal: We aim to prove that for a one-way ANOVA with \\(a=2\\) groups, the F-statistic is exactly equal to the square of the t-statistic: \\[F_{1, N-2} = t_{N-2}^2\\]\n\n\n\nSample sizes: \\(n_1\\) and \\(n_2\\).\nTotal observations: \\(N = n_1 + n_2\\).\nGroup Means: \\(\\bar{y}_1\\) and \\(\\bar{y}_2\\).\nGlobal Mean: \\(\\bar{y}_{..} = \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_2}{N}\\).\n\n\n\n\n\nRecall that the Sum of Squares Error (SSE) is the sum of the squared deviations within each group. This can be written in terms of the individual group sample variances (\\(S_1^2\\) and \\(S_2^2\\)):\n\\[SSE = (n_1 - 1)S_1^2 + (n_2 - 1)S_2^2\\]\nThe Mean Square Error (\\(MSE\\)) is calculated by dividing \\(SSE\\) by its degrees of freedom (\\(N - a = N - 2\\)):\n\\[MSE = \\frac{SSE}{N-2} = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\\]\nKey Observation: This expression is the exact definition of the Pooled Variance (\\(S_p^2\\)) used in the two-sample t-test.\n\\[\\boxed{MSE = S_p^2}\\]\n\n\n\n\nThe Sum of Squares Treatment (SST) measures the weighted deviation of group means from the global mean. Since \\(a=2\\), the degrees of freedom is \\(2-1=1\\), so \\(MST = SST/1 = SST\\).\n\\[MST = SST = n_1(\\bar{y}_1 - \\bar{y}_{..})^2 + n_2(\\bar{y}_2 - \\bar{y}_{..})^2\\]\nTo simplify this efficiently, let’s analyze the deviation term \\((\\bar{y}_1 - \\bar{y}_{..})\\) separately before squaring.\n\n\nSubstitute the definition of the global mean:\n\\[\n\\begin{aligned}\n\\bar{y}_1 - \\bar{y}_{..} &= \\bar{y}_1 - \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_2}{N} \\\\\n&= \\frac{N\\bar{y}_1 - (n_1\\bar{y}_1 + n_2\\bar{y}_2)}{N} \\\\\n\\text{Since } N = n_1 + n_2: \\\\\n&= \\frac{(n_1 + n_2)\\bar{y}_1 - n_1\\bar{y}_1 - n_2\\bar{y}_2}{N} \\\\\n&= \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_1 - n_1\\bar{y}_1 - n_2\\bar{y}_2}{N} \\\\\n&= \\frac{n_2(\\bar{y}_1 - \\bar{y}_2)}{N}\n\\end{aligned}\n\\]\nBy symmetry, the deviation for the second group is: \\[\\bar{y}_2 - \\bar{y}_{..} = -\\frac{n_1}{N}(\\bar{y}_1 - \\bar{y}_2)\\]\n\n\n\nNow, substitute these simplified deviations back into the SST equation:\n\\[\n\\begin{aligned}\nSST &= n_1 \\left[ \\frac{n_2}{N}(\\bar{y}_1 - \\bar{y}_2) \\right]^2 + n_2 \\left[ -\\frac{n_1}{N}(\\bar{y}_1 - \\bar{y}_2) \\right]^2 \\\\\n&= n_1 \\frac{n_2^2}{N^2}(\\bar{y}_1 - \\bar{y}_2)^2 + n_2 \\frac{n_1^2}{N^2}(\\bar{y}_1 - \\bar{y}_2)^2\n\\end{aligned}\n\\]\nFactor out the common term \\(\\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2}\\) and \\(n_1 n_2\\):\n\\[\n\\begin{aligned}\nSST &= \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2} \\left( n_1 n_2^2 + n_2 n_1^2 \\right) \\\\\n&= \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2} \\cdot n_1 n_2 (n_2 + n_1)\n\\end{aligned}\n\\]\nSince \\((n_2 + n_1) = N\\), one \\(N\\) cancels out:\n\\[SST = \\frac{n_1 n_2}{N} (\\bar{y}_1 - \\bar{y}_2)^2\\]\nTo match the t-test format, we rewrite the coefficient \\(\\frac{n_1 n_2}{N}\\) using the reciprocal identity: \\[\\frac{n_1 n_2}{n_1 + n_2} = \\frac{1}{\\frac{n_1 + n_2}{n_1 n_2}} = \\frac{1}{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\nThus, we have our final expression for the Numerator:\n\\[\\boxed{MST = \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\n\n\n\n\nThe F-statistic is the ratio of the Numerator (Section 3) and the Denominator (Section 2):\n\\[\n\\begin{aligned}\nF &= \\frac{MST}{MSE} \\\\\nF &= \\frac{ \\left( \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{\\frac{1}{n_1} + \\frac{1}{n_2}} \\right) }{ S_p^2 }\n\\end{aligned}\n\\]\nWe can rearrange this fraction to isolate the squared terms:\n\\[F = \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{ S_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) }\\]\nThis is mathematically equivalent to the square of the t-statistic definition:\n\\[F = \\left( \\frac{\\bar{y}_1 - \\bar{y}_2}{ S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} } \\right)^2 = t^2\\]\nConclusion: We have proven that for \\(a=2\\), the ANOVA F-test yields the same result as the squared two-sample t-test.\n\n\nCode\nnear(summary(lm_q4b)$coefficients[5, 3]^2, anova(lm_q4a_i, lm_q4b)$F[2])\n\n\n[1] TRUE\n\n\n\n\n\n\\[\nF = \\frac{\\frac{TSS - RSS}{p - 1}}{\\frac{RSS}{n - p}}\n\\]\n\\[\nR^2 = 1 - \\frac{RSS}{TSS}\n\\]\nRearranging \\(F\\)\n\\[\nF \\cdot \\frac{RSS}{n - p} = \\frac{TSS - RSS}{p - 1}\n\\]\n\\[ F \\cdot \\frac{p - 1}{n - p} = \\frac{TSS - RSS}{RSS} \\]\n\\[\nF \\cdot \\frac{p - 1}{n - p} = \\frac{TSS}{RSS} - 1\n\\]\n\\[\n\\frac{TSS}{RSS} = F \\cdot \\frac{p - 1}{n - p} + 1\n\\]\n\\[\n\\frac{RSS}{TSS} = \\frac{n - p}{F(p - 1) + n - p}\n\\]\nSubstituting \\(\\frac{RSS}{TSS}\\)\n\\[\nR^2 = 1 - \\frac{n - p}{F(p - 1) + n - p}\n\\]\n\\[ R^2 = 1 - \\frac{n - p}{F(p - 1) + n - p} \\]\n\\[  F(p - 1) + n - p = \\frac{n - p}{1 - R^2} \\]\n\\[\nF(p - 1) = \\frac{n - p}{1 - R^2} + p - n\n\\]\n\\[\nF(p - 1) = \\frac{n - p}{1 - R^2} + \\frac{(p - n)(1 - R^2)}{1 - R^2}\n\\]\n\\[\nF(p - 1) = \\frac{n - p + p  - pR^2 - n + nR^2}{1 - R^2}\n\\]\n\\[\nF(p - 1) = \\frac{nR^2 - pR^2}{1 - R^2}\n\\]\n\\[\nF = \\frac{R^2 (n - p)}{(1 - R^2)(p - 1)}\n\\]\n\n\n\n\n\n\n\nCode\ndata(punting, package = \"faraway\")\n\n\n\n\nCode\nlm_q6a &lt;- lm(Distance ~ RStr + LStr + RFlex + LFlex, data = punting)\nsummary(lm_q6a)\n\n\n\nCall:\nlm(formula = Distance ~ RStr + LStr + RFlex + LFlex, data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.941  -8.958  -4.441  13.523  17.016 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -79.6236    65.5935  -1.214    0.259\nRStr          0.5116     0.4856   1.054    0.323\nLStr         -0.1862     0.5130  -0.363    0.726\nRFlex         2.3745     1.4374   1.652    0.137\nLFlex        -0.5277     0.8255  -0.639    0.541\n\nResidual standard error: 16.33 on 8 degrees of freedom\nMultiple R-squared:  0.7365,    Adjusted R-squared:  0.6047 \nF-statistic:  5.59 on 4 and 8 DF,  p-value: 0.01902\n\n\nNone.\n\n\n\n\\(\\text{p-value}_{F-test} = 0.01902 &lt; 0.05\\), hence we reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr} = \\beta_{RFlex} = \\beta_{LFlex} = 0\\).\n\n\n\n\n\nCode\nlm_q6c &lt;- lm(Distance ~ I(RStr + LStr) + RFlex + LFlex, data = punting)\nsummary(lm_q6c)\n\n\n\nCall:\nlm(formula = Distance ~ I(RStr + LStr) + RFlex + LFlex, data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.698  -9.494  -5.155   9.081  20.611 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    -71.2694    63.1447  -1.129    0.288\nI(RStr + LStr)   0.1741     0.1940   0.898    0.393\nRFlex            2.3137     1.4013   1.651    0.133\nLFlex           -0.5772     0.8035  -0.718    0.491\n\nResidual standard error: 15.94 on 9 degrees of freedom\nMultiple R-squared:  0.7174,    Adjusted R-squared:  0.6232 \nF-statistic: 7.615 on 3 and 9 DF,  p-value: 0.00769\n\n\nCode\nanova(lm_q6c, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ I(RStr + LStr) + RFlex + LFlex\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      9 2287.4                           \n2      8 2132.6  1    154.72 0.5804  0.468\n\n\n\\(\\text{p-value}_{F-test} = 0.468 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr}\\). We conclude that there is no significant difference between left leg strength and right leg strength.\n\n\n\n\n\nCode\nplot(ellipse(lm_q6a, c(2, 3)), type = \"l\")\npoints(lm_q6a$coef[\"RStr\"], lm_q6a$coef[\"LStr\"])\nabline(v = confint(lm_q6a)[2, c(1, 2)], lty = 4)\nabline(h = confint(lm_q6a)[3, c(1, 2)], lty = 4)\nabline(v = 0, lty = 2, col = \"blue\")\nabline(h = 0, lty = 2, col = \"blue\")\npoints(0, 0, pch = 21)\n\n\n\n\n\n\n\n\n\n\nOrigin (\\(\\beta_RStr = 0\\), \\(\\beta_LStr = 0\\)) falls within both the \\(95\\%\\) joint confidence ellipse and the rectangular region defined by the individual \\(95\\%\\) univariate confidence intervals.\nNeither predictor is statistically significant when considered in isolation within the full model.\nCrucially, because the origin lies inside the joint confidence ellipse, we do not reject the null hypothesis that both coefficients are simultaneously zero. This is consistent with our previous analysis.\n\n\n\n\nWe will obtain the same results as in q6c: Failing to reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr}\\) and return to the same conclusion that there is no significant difference between left leg strength and right leg strength, in other words, the total leg strength is sufficient (Total leg strength is defined by adding the right and left leg strengths together).\n\n\n\n\n\nCode\nlm_q6f &lt;- lm(Distance ~ RStr + LStr + I(RFlex + LFlex), data = punting)\nsummary(lm_q6f)\n\n\n\nCall:\nlm(formula = Distance ~ RStr + LStr + I(RFlex + LFlex), data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.510 -13.417   2.165   7.988  23.316 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      -44.26189   63.52790  -0.697    0.504\nRStr               0.70392    0.48904   1.439    0.184\nLStr               0.01518    0.51703   0.029    0.977\nI(RFlex + LFlex)   0.46194    0.43975   1.050    0.321\n\nResidual standard error: 17.15 on 9 degrees of freedom\nMultiple R-squared:  0.6728,    Adjusted R-squared:  0.5637 \nF-statistic: 6.168 on 3 and 9 DF,  p-value: 0.01451\n\n\nCode\nanova(lm_q6f, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ RStr + LStr + I(RFlex + LFlex)\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      9 2648.4                           \n2      8 2132.6  1    515.72 1.9346 0.2017\n\n\n\\(\\text{p-value}_{F-test} = 0.2017 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RFlex} = \\beta_{LFlex}\\). We conclude that there is no significant difference between left leg flexibility and right leg flexibility.\n\n\n\n\n\nCode\nlm_q6e &lt;- lm(Distance ~ I(RStr + LStr) + I(RFlex + LFlex), data = punting)\nsummary(lm_q6e)\n\n\n\nCall:\nlm(formula = Distance ~ I(RStr + LStr) + I(RFlex + LFlex), data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.948 -13.929   1.020   9.795  29.111 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)      -36.1525    60.9655  -0.593    0.566  \nI(RStr + LStr)     0.3700     0.1430   2.588    0.027 *\nI(RFlex + LFlex)   0.4093     0.4228   0.968    0.356  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.73 on 10 degrees of freedom\nMultiple R-squared:  0.6541,    Adjusted R-squared:  0.585 \nF-statistic: 9.457 on 2 and 10 DF,  p-value: 0.004948\n\n\nCode\nanova(lm_q6e, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ I(RStr + LStr) + I(RFlex + LFlex)\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq    F Pr(&gt;F)\n1     10 2799.1                         \n2      8 2132.6  2    666.43 1.25  0.337\n\n\n\\(\\text{p-value}_{F-test} = 0.337 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr} AND \\beta_{RFlex} = \\beta_{LFlex}\\). We conclude that there is no significant difference between left-right symmetry for both strength and flexibility. Hence, symmetry is a reasonable claim.\n\n\n\n\n\nCode\nlm_q6f &lt;- lm(Hang ~ RStr + LStr + RFlex + LFlex, data = punting)\nsummary(lm_q6f)\n\n\n\nCall:\nlm(formula = Hang ~ RStr + LStr + RFlex + LFlex, data = punting)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36297 -0.13528 -0.07849  0.09938  0.35893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.225239   1.032784  -0.218    0.833\nRStr         0.005153   0.007645   0.674    0.519\nLStr         0.007697   0.008077   0.953    0.369\nRFlex        0.019404   0.022631   0.857    0.416\nLFlex        0.004614   0.012998   0.355    0.732\n\nResidual standard error: 0.2571 on 8 degrees of freedom\nMultiple R-squared:  0.8156,    Adjusted R-squared:  0.7235 \nF-statistic: 8.848 on 4 and 8 DF,  p-value: 0.004925\n\n\nNo, because they have different responses."
  },
  {
    "objectID": "chapter3.html#exercises",
    "href": "chapter3.html#exercises",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\ndata(prostate, package = \"faraway\")\nlm_q1a &lt;- lm(lpsa ~ ., data = prostate)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = lpsa ~ ., data = prostate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7331 -0.3713 -0.0170  0.4141  1.6381 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.669337   1.296387   0.516  0.60693    \nlcavol       0.587022   0.087920   6.677 2.11e-09 ***\nlweight      0.454467   0.170012   2.673  0.00896 ** \nage         -0.019637   0.011173  -1.758  0.08229 .  \nlbph         0.107054   0.058449   1.832  0.07040 .  \nsvi          0.766157   0.244309   3.136  0.00233 ** \nlcp         -0.105474   0.091013  -1.159  0.24964    \ngleason      0.045142   0.157465   0.287  0.77503    \npgg45        0.004525   0.004421   1.024  0.30886    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7084 on 88 degrees of freedom\nMultiple R-squared:  0.6548,    Adjusted R-squared:  0.6234 \nF-statistic: 20.86 on 8 and 88 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\ncat(\n  \"90% CI for age: \",\n  round(\n    summary(lm_q1a)$coef[4, 1] +\n      c(-1, 1) * qt(0.95, 97 - 9) * summary(lm_q1a)$coef[4, 2],\n    4\n  )\n)\n\n\n90% CI for age:  -0.0382 -0.0011\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\n  \"95% CI for age: \",\n  round(\n    summary(lm_q1a)$coef[4, 1] +\n      c(-1, 1) * qt(0.975, 97 - 9) * summary(lm_q1a)$coef[4, 2],\n    4\n  )\n)\n\n\n95% CI for age:  -0.0418 0.0026\n\n\nVisualising it:\n\n\nCode\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nci_90 &lt;- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.90) |&gt;\n  filter(term == \"age\") |&gt;\n  mutate(level = \"90% CI (p-value &lt;= 0.10, alpha = 0.10)\")\n\nci_95 &lt;- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.95) |&gt;\n  filter(term == \"age\") |&gt;\n  mutate(level = \"95% CI (p-value &lt;= 0.05, alpha = 0.05)\")\n\nci_combined &lt;- bind_rows(ci_90, ci_95)\n\nggplot(ci_combined, aes(x = estimate, y = level, colour = level)) +\n  geom_errorbarh(\n    aes(xmin = conf.low, xmax = conf.high),\n    height = 0.1,\n    linewidth = 1.5\n  ) +\n  geom_point(size = 5, shape = 19) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"black\",\n    linewidth = 1\n  ) +\n  labs(\n    title = \"Bracketing the p-value for the 'age' Coefficient\",\n    x = \"Coefficient Value\",\n  ) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"none\")\n\n\nWarning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n90% CI for age does not include 0 but 95% CI for age does, therefore the p-value lies within 0.05 and 0.10.\n\n\n\n\n\nCode\nlibrary(ellipse)\n\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n\nCode\nplot(ellipse(lm_q1a, c(4, 5)), type = \"l\")\npoints(coef(lm_q1a)[4], coef(lm_q1a)[5])\nabline(v = confint(lm_q1a)[4, c(1, 2)], lty = 4)\nabline(h = confint(lm_q1a)[5, c(1, 2)], lty = 4)\nabline(v = 0, lty = 2, col = \"blue\")\nabline(h = 0, lty = 2, col = \"blue\")\npoints(0, 0, pch = 21)\n\n\n\n\n\n\n\n\n\n\nOrigin (\\(\\beta_age = 0\\), \\(\\beta_lbph = 0\\)) falls within both the \\(95\\%\\) joint confidence ellipse and the rectangular region defined by the individual \\(95\\%\\) univariate confidence intervals.\nNeither predictor is statistically significant when considered in isolation within the full model.\nCrucially, because the origin lies inside the joint confidence ellipse, we do not reject the null hypothesis that both coefficients are simultaneously zero. This is consistent with the result of the formal F-test for nested models:\n\n\n\nCode\nlm_q1b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lcp + gleason + pgg45,\n  data = prostate\n)\nanova(lm_q1b, lm_q1a)\n\n\nAnalysis of Variance Table\n\nModel 1: lpsa ~ lcavol + lweight + svi + lcp + gleason + pgg45\nModel 2: lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + \n    pgg45\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     90 46.768                              \n2     88 44.163  2    2.6048 2.5951 0.08034 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe non-significant p-value from this F-test confirms the visual evidence from the ellipse: we fail to find sufficient statistical evidence to reject the null hypothesis.\n\n\n\n\n\n\nCode\nlm_q1c &lt;- lm(lpsa ~ lcavol + lweight + svi, data = prostate)\nsummary(lm_q1c)\n\n\n\nCall:\nlm(formula = lpsa ~ lcavol + lweight + svi, data = prostate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.72964 -0.45764  0.02812  0.46403  1.57013 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.26809    0.54350  -0.493  0.62298    \nlcavol       0.55164    0.07467   7.388  6.3e-11 ***\nlweight      0.50854    0.15017   3.386  0.00104 ** \nsvi          0.66616    0.20978   3.176  0.00203 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7168 on 93 degrees of freedom\nMultiple R-squared:  0.6264,    Adjusted R-squared:  0.6144 \nF-statistic: 51.99 on 3 and 93 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nanova(lm_q1c, lm_q1a)\n\n\nAnalysis of Variance Table\n\nModel 1: lpsa ~ lcavol + lweight + svi\nModel 2: lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + \n    pgg45\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     93 47.785                           \n2     88 44.163  5    3.6218 1.4434 0.2167\n\n\nSince p-value is much larger than \\(\\alpha = 0.05\\), we fail to reject the null hypothesis, \\(H_0: \\beta_{age} = \\beta_{lbph} = \\beta_{lcp} = \\beta_{gleason} = \\beta_{pgg45} = 0\\).\nWe conclude that the five predictors removed from the full model do not collectively contribute statistically significant explanatory power to the model, hence the smaller model is preferable.\n\n\n\n\n\n\n\n\nCode\ndata(cheddar, package = \"faraway\")\n\n\n\n\nCode\nlm_q2a &lt;- lm(taste ~ ., data = cheddar)\nsummary(lm_q2a)\n\n\n\nCall:\nlm(formula = taste ~ ., data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.390  -6.612  -1.009   4.908  25.449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -28.8768    19.7354  -1.463  0.15540   \nAcetic        0.3277     4.4598   0.073  0.94198   \nH2S           3.9118     1.2484   3.133  0.00425 **\nLactic       19.6705     8.6291   2.280  0.03108 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.13 on 26 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6116 \nF-statistic: 16.22 on 3 and 26 DF,  p-value: 3.81e-06\n\n\nH2S and Lactic predictors are statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\nlm_q2b &lt;- lm(taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)\nsummary(lm_q2b)\n\n\n\nCall:\nlm(formula = taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.209  -7.266  -1.651   7.385  26.335 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.897e+01  1.127e+01  -1.684   0.1042  \nexp(Acetic)  1.891e-02  1.562e-02   1.210   0.2371  \nexp(H2S)     7.668e-04  4.188e-04   1.831   0.0786 .\nLactic       2.501e+01  9.062e+00   2.760   0.0105 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.19 on 26 degrees of freedom\nMultiple R-squared:  0.5754,    Adjusted R-squared:  0.5264 \nF-statistic: 11.75 on 3 and 26 DF,  p-value: 4.746e-05\n\n\nLactic predictor is statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\nanova(lm_q2b, lm_q2a)\n\n\nAnalysis of Variance Table\n\nModel 1: taste ~ exp(Acetic) + exp(H2S) + Lactic\nModel 2: taste ~ Acetic + H2S + Lactic\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1     26 3253.6                      \n2     26 2668.4  0     585.2         \n\n\n\nNo, because an F-test is only valid for comparing nested models, where one model is a restricted version of the other (i.e. the predictors of the smaller model are a smaller subset of the larger model).\nIn this case, the models rely on different functional forms of the predictor [\\(log(x)\\) vs \\(x\\)], thus the column space of one model is not a linear subspace the other.\nSince \\(R^2_{lm\\_q2a} = 0.6518 &gt; R^2_{lm\\_q2b} = 0.5754\\) and \\(RSS_{lm\\_q2a} = 2668.4 &lt; RSS_{lm\\_q2b} = 3253.6\\), the first model provides a better fit to the data.\n\n\n\n\n\n\nCode\ncat(\"Taste would increase by\", round(0.01 * coef(lm_q2a)['H2S'], 3))\n\n\nTaste would increase by 0.039\n\n\n\n\n\n\n\nCode\ncat(\n  \"The H2S on the original scale will increase by \",\n  (exp(0.01) - 1) * 100,\n  \"% when adding 0.01 on the (natural) log scale as adding 0.01 to the log scale is equivalent to multiplying by exp(0.01) on the original scale.\",\n  sep = \"\"\n)\n\n\nThe H2S on the original scale will increase by 1.005017% when adding 0.01 on the (natural) log scale as adding 0.01 to the log scale is equivalent to multiplying by exp(0.01) on the original scale.\n\n\n\n\n\n\n\n\n\n\nCode\ndata(teengamb, package = \"faraway\")\nlm_q3a &lt;- lm(gamble ~ ., data = teengamb)\nsummary(lm_q3a)\n\n\n\nCall:\nlm(formula = gamble ~ ., data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n\nSex and Income predictors are statistically significant at the \\(5\\%\\) level.\n\n\n\n\n\nCode\ncat(\n  \"For females, the difference in average weekly gambling between females and males, holding other factors constant, is\",\n  signif(summary(lm_q3a)$coefficients[2], 3),\n  \"pounds.\"\n)\n\n\nFor females, the difference in average weekly gambling between females and males, holding other factors constant, is -22.1 pounds.\n\n\n\n\n\n\n\nCode\nlm_q3c &lt;- lm(gamble ~ income, data = teengamb)\nsummary(lm_q3c)\n\n\n\nCall:\nlm(formula = gamble ~ income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.020 -11.874  -3.757  11.934 107.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.325      6.030  -1.049      0.3    \nincome         5.520      1.036   5.330 3.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.95 on 45 degrees of freedom\nMultiple R-squared:  0.387, Adjusted R-squared:  0.3734 \nF-statistic: 28.41 on 1 and 45 DF,  p-value: 3.045e-06\n\n\n\n\nCode\nanova(lm_q3c, lm_q3a)\n\n\nAnalysis of Variance Table\n\nModel 1: gamble ~ income\nModel 2: gamble ~ sex + status + income + verbal\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     45 28009                              \n2     42 21624  3    6384.8 4.1338 0.01177 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince p-value is much smaller than \\(\\alpha = 0.05\\), we reject the null hypothesis, \\(H_0: \\beta_{sex} = \\beta_{status} = \\beta_{verbal} = 0\\).\nWe conclude that the full model is preferred.\n\n\n\n\n\n\n\n\nCode\ndata(sat, package = \"faraway\")\n\n\n\n\nCode\nlm_q4a_i &lt;- lm(total ~ expend + ratio + salary, data = sat)\nsummary(lm_q4a_i)\n\n\n\nCall:\nlm(formula = total ~ expend + ratio + salary, data = sat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-140.911  -46.740   -7.535   47.966  123.329 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1069.234    110.925   9.639 1.29e-12 ***\nexpend        16.469     22.050   0.747   0.4589    \nratio          6.330      6.542   0.968   0.3383    \nsalary        -8.823      4.697  -1.878   0.0667 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68.65 on 46 degrees of freedom\nMultiple R-squared:  0.2096,    Adjusted R-squared:  0.1581 \nF-statistic: 4.066 on 3 and 46 DF,  p-value: 0.01209\n\n\nOptional:\n\n\nCode\n# Dropping salary\nlm_q4a_ii &lt;- lm(total ~ expend + ratio, data = sat)\nanova(lm_q4a_ii, lm_q4a_i)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ expend + ratio\nModel 2: total ~ expend + ratio + salary\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     47 233443                              \n2     46 216812  1     16631 3.5285 0.06667 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# Dropping all predictors\nlm_q4a_iii &lt;- lm(total ~ 1, data = sat)\nanova(lm_q4a_iii, lm_q4a_i)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ 1\nModel 2: total ~ expend + ratio + salary\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     49 274308                              \n2     46 216812  3     57496 4.0662 0.01209 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsalary is not statistically significant (\\(\\text{p-value}_\\text{salary} = 0.0667 &lt; 0.05\\)).\nAll the individual t-tests are not statistically significant yet the overall F-test is.\nThe p-value for the F-statistics is less than \\(\\text{p-value}_\\text{F-statistics} = 0.01209 &lt; 0.05\\), we reject the null hypothesis \\(H_0: \\beta_\\text{salary} = \\beta_\\text{ratio} = \\beta_\\text{expend} = 0\\).\nThis indicates that at least one of the predictor has a statistically significant effect on the response.\nThis usually happens when the predictors are highly correlated with each other and we have a situation where the whole is greater than the sum of its parts.\n\n\n\n\n\n\nCode\nlm_q4b &lt;- lm(total ~ expend + ratio + salary + takers, data = sat)\nsummary(lm_q4b)\n\n\n\nCall:\nlm(formula = total ~ expend + ratio + salary + takers, data = sat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-90.531 -20.855  -1.746  15.979  66.571 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1045.9715    52.8698  19.784  &lt; 2e-16 ***\nexpend         4.4626    10.5465   0.423    0.674    \nratio         -3.6242     3.2154  -1.127    0.266    \nsalary         1.6379     2.3872   0.686    0.496    \ntakers        -2.9045     0.2313 -12.559 2.61e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.7 on 45 degrees of freedom\nMultiple R-squared:  0.8246,    Adjusted R-squared:  0.809 \nF-statistic: 52.88 on 4 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nanova(lm_q4a_i, lm_q4b)\n\n\nAnalysis of Variance Table\n\nModel 1: total ~ expend + ratio + salary\nModel 2: total ~ expend + ratio + salary + takers\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 216812                                  \n2     45  48124  1    168688 157.74 2.607e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey output the same p-value.\nSince \\(t_i = \\frac{\\hat{\\beta}_i}{se(\\hat{\\beta}_i)}\\)\n\n\nCode\nlibrary(dplyr)\nnear(\n  summary(lm_q4b)$coefficients[5, 1] / summary(lm_q4b)$coefficients[5, 2],\n  summary(lm_q4b)$coefficients[5, 3]\n)\n\n\n[1] TRUE\n\n\nSince the sum-of-squares decomposition and F-statistics reduces to the usual equal-variance (pooled) two sample t-test in the case of \\(\\alpha = 2\\) treatments - with the realisation that an F-statistics with one numerator and \\(k\\) denominator degrees of freedom is equivalent to a t-statistics with \\(k\\) degrees of freedom, viz: \\(F_{1,k} = t^2_{k}\\)\nLink to Proof"
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\ndata(teengamb, package = \"faraway\")\nlm_q1a &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n\n\n\n\n\n\nCode\ncat(round(summary(lm_q1a)$r.squared * 100, 2), \"%\")\n\n\n52.67 %\n\n\nMultiple R-square: 52.67%\n\n\n\n\n\nCode\ncat(\n  \"Index: \",\n  which.max(residuals(lm_q1a)),\n  \"\\nResidual\",\n  max(residuals(lm_q1a))\n)\n\n\nIndex:  24 \nResidual 94.25222\n\n\n\n\n\n\n\nCode\ncat(\n  \"Mean of residuals: \",\n  signif(mean(residuals(lm_q1a)), 3),\n  \"\\nMedian of residuals: \",\n  signif(median(residuals(lm_q1a)), 3)\n)\n\n\nMean of residuals:  -1.56e-16 \nMedian of residuals:  -1.45\n\n\nWe expect the mean of the residuals from a linear model to be always zero (assuming we do not drop the intercept term, \\(\\beta_0\\)), \\(\\bar{\\hat{\\epsilon}} = 0\\). Unlike the mean of the residuals, which is mathematically guaranteed to be zero in an OLS model, the median is not. Consequently, the median is often close to zero but rarely, if ever, exactly zero.\n\n\n\n\n\nCode\ncat(\n  \"Correlation of the residuals with the fitted values: \",\n  signif(cor(residuals(lm_q1a), predict(lm_q1a)), 3)\n)\n\n\nCorrelation of the residuals with the fitted values:  4.76e-17\n\n\nThe correlation is effectively zero because the residual vector \\(\\hat{\\epsilon}\\) and fitted value vector \\(\\hat{y}\\) are orthogonal to each other.\n\n\n\n\n\nCode\ncat(\n  \"Correlation of the residuals with income: \",\n  signif(cor(residuals(lm_q1a), teengamb$income), 3)\n)\n\n\nCorrelation of the residuals with income:  3.25e-17\n\n\nYes, it will always be zero because geometrically, the OLS model projects the outcome vector \\(y\\) onto the plane spanned by the set of all predictor variables. In our case, the income vector is part of that plane. Consequently, the fitted values vector \\(\\hat{y}\\) lies on this plane while the residuals vector \\(\\hat{\\epsilon}\\) is orthogonal to the plane.\n\n\n\nFemale is coded as one and male is coded as zero.\n\n\nCode\ncat(\n  \"For females, the difference in average weekly gambling between females\",\n  \"and males, holding other factors constant, is\",\n  signif(summary(lm_q1a)$coefficients[2], 3),\n  \"pounds.\"\n)\n\n\nFor females, the difference in average weekly gambling between females and males, holding other factors constant, is -22.1 pounds.\n\n\n\n\n\n\n\n\n\n\nCode\ndata(uswages, package = \"faraway\")\nlm_q2a &lt;- lm(wage ~ educ + exper, data = uswages)\nsummary(lm_q2a)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1018.2  -237.9   -50.9   149.9  7228.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -242.7994    50.6816  -4.791 1.78e-06 ***\neduc          51.1753     3.3419  15.313  &lt; 2e-16 ***\nexper          9.7748     0.7506  13.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427.9 on 1997 degrees of freedom\nMultiple R-squared:  0.1351,    Adjusted R-squared:  0.1343 \nF-statistic:   156 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nCode\napply(uswages[, 1:3], 2, min)\n\n\n wage  educ exper \n50.39  0.00 -2.00 \n\n\n\n\n\n\n\nCode\n# When predictors are zero, the intercept is the prediction.\ncat(\n  \"Predicted wage for a worker with no education and no experience:\",\n  format(round(summary(lm_q2a)$coef[1], 2), nsmall = 2)\n)\n\n\nPredicted wage for a worker with no education and no experience: -242.80\n\n\nIt doesn’t make sense to report a negative wage value.\n\n\nCode\nsubset(uswages, educ == 0 | exper == 0)\n\n\n         wage educ exper race smsa ne mw so we pt\n12878  148.39   16     0    0    1  0  1  0  0  1\n21572  308.64   16     0    0    1  0  0  1  0  0\n6090   144.03   12     0    0    1  1  0  0  0  0\n4310   493.83   14     0    0    1  1  0  0  0  0\n6305    56.79   14     0    0    0  1  0  0  0  1\n2245   830.96   18     0    0    1  1  0  0  0  0\n3858   241.21   12     0    0    0  1  0  0  0  0\n27419  142.45   12     0    0    1  0  0  0  1  0\n10853  197.41   12     0    0    1  0  1  0  0  1\n11576  283.95   16     0    0    1  0  1  0  0  0\n27774  347.22   14     0    0    1  0  0  0  1  0\n19834   61.73   13     0    0    0  0  0  1  0  1\n20846  189.93   16     0    0    1  0  0  1  0  1\n8006    56.98   14     0    0    0  0  1  0  0  1\n27787  892.59   13     0    0    0  0  0  0  1  0\n4014    78.56   14     0    0    0  1  0  0  0  1\n27585  308.64   16     0    0    1  0  0  0  1  0\n15354  335.10   12     0    0    1  0  0  1  0  0\n18362  195.47   16     0    0    0  0  0  1  0  0\n24439  159.10    0    27    0    0  0  0  0  1  0\n3778   274.35   12     0    0    1  1  0  0  0  0\n26303  189.93    0    54    0    1  0  0  0  1  0\n6345    54.61   13     0    0    0  1  0  0  0  1\n21435  106.08   14     0    0    0  0  0  1  0  1\n4648   474.83   12     0    0    1  1  0  0  0  0\n12401  174.90   14     0    0    0  0  1  0  0  1\n24355  165.43   12     0    0    0  0  0  0  1  0\n3881   474.83   18     0    0    0  1  0  0  0  0\n21604  400.26    0    52    0    1  0  0  1  0  0\n20410  246.91   16     0    0    0  0  0  1  0  1\n27138   96.19   12     0    0    1  0  0  0  1  0\n17129  308.64    0    47    1    1  0  0  1  0  0\n10476  130.58   13     0    1    1  0  1  0  0  0\n13803   52.23   14     0    1    1  0  0  1  0  1\n8902    66.14   12     0    0    1  0  1  0  0  1\n16915  143.87   12     0    0    1  0  0  1  0  1\n2780  5144.03    0    18    0    1  1  0  0  0  0\n13686  395.06    0    42    0    1  0  0  1  0  0\n12711  205.76   15     0    0    1  0  1  0  0  0\n15548  200.62   12     0    0    1  0  0  1  0  0\n21269  284.90   13     0    0    1  0  0  1  0  1\n26160  166.19   13     0    0    1  0  0  0  1  1\n6148    74.07   12     0    0    1  1  0  0  0  0\n1376   569.80   13     0    0    1  1  0  0  0  0\n21479  237.42   16     0    1    0  0  0  1  0  1\n3337   144.44   15     0    0    1  1  0  0  0  1\n21367  100.78   13     0    0    1  0  0  1  0  1\n8485   427.98   14     0    0    1  0  1  0  0  1\n18047  154.32   13     0    0    1  0  0  1  0  0\n27780   92.59   12     0    0    1  0  0  0  1  1\n24297  216.05   15     0    0    1  0  0  0  1  0\n8948    77.16   14     0    0    1  0  1  0  0  1\n25196  227.02   15     0    0    1  0  0  0  1  1\n14153  108.02   13     0    1    1  0  0  1  0  1\n7651   284.90   16     0    0    0  0  1  0  0  1\n21436  356.13    0    54    0    1  0  0  1  0  0\n13754  197.22   16     0    0    1  0  0  1  0  1\n1620   493.83   14     0    0    0  1  0  0  0  0\n7739   465.80   18     0    0    1  0  1  0  0  0\n5444    54.01   12     0    0    1  1  0  0  0  1\n10738  102.88   12     0    0    1  0  1  0  0  1\n26011   86.13   12     0    0    1  0  0  0  1  1\n8235    85.66   13     0    1    1  0  1  0  0  1\n22810  142.45   13     0    0    0  0  0  0  1  0\n25047  593.54    0    56    0    1  0  0  0  1  0\n12408  178.60   16     0    0    0  0  1  0  0  1\n\n\n\n\nCode\nsubset(uswages, educ == 0 & exper == 0)\n\n\n [1] wage  educ  exper race  smsa  ne    mw    so    we    pt   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nAccording to the above output, the data confirms that while some workers have either zero education or zero experience, no worker has both simultaneously.\nTherefore, the model’s prediction for this scenario is a pure extrapolation, as it’s forecasting for a data point that does not exist in the dataset.\n\n\n\n\n\nCode\nall.equal(cor(predict(lm_q2a), uswages$wage)^2, summary(lm_q2a)$r.squared)\n\n\n[1] TRUE\n\n\n\n\n\n\n\nCode\nlm_q2e &lt;- lm(wage ~ educ + exper - 1, data = uswages)\nsummary(lm_q2e)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper - 1, data = uswages)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-903.6 -254.9  -70.9  140.8 7144.6 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \neduc    36.014      1.079   33.37   &lt;2e-16 ***\nexper    7.854      0.638   12.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 430.2 on 1998 degrees of freedom\nMultiple R-squared:  0.6819,    Adjusted R-squared:  0.6815 \nF-statistic:  2141 on 2 and 1998 DF,  p-value: &lt; 2.2e-16\n\n\n\\(R^2\\) has increased.\n\n\nCode\ncor(predict(lm_q2e), uswages$wage)^2\n\n\n[1] 0.1340295\n\n\nBut direct calculation provides a similar value to the initial model.\n\n\n\n\n\nCode\ncat(\n  \"RSS of Initial Model:\",\n  sum(residuals(lm_q2a)^2),\n  \"\\nRSS of Intercept Removed Model:\",\n  sum(residuals(lm_q2e)^2)\n)\n\n\nRSS of Initial Model: 365568644 \nRSS of Intercept Removed Model: 369769963\n\n\nRSS of Initial Model (More parameter, more flexibility) &lt; RSS of Intercept Removed Model (One less parameter)\n\n\n\n\n\nCode\nround(coef(lm_q2a)['educ'], 2)\n\n\n educ \n51.18 \n\n\n\n\n\n\n\nCode\nlm_q2h &lt;- lm(log(wage) ~ educ + exper, data = uswages)\nsummary(lm_q2h)\n\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe Residual standard error is only 427.9 in the first model but in the logged response model, it is 0.6615 due to scaling.\n\\(R^{2}_{\\text{first model}} = 0.1351 &lt; R^{2}_{\\text{logged model}} = 0.1749\\)\n\nSince \\(R^2\\) is unit free, the logged model is preferable.\n\n\n\n\n\nFirst we need to know the value of experience on the unlogged wage scale by exponentiating both side.\n\\[\n\\text{wage} = e^{\\beta_0 + \\beta_1 \\cdot \\text{education} + \\beta_2 \\cdot \\text{experience}}\n\\]\n\\[\n\\text{wage} = e^{\\beta_0} \\times e^{\\beta_1 \\cdot \\text{education}} \\times e^{\\beta_2 \\cdot \\text{experience}}\n\\]\n\n\nCode\n# Need to unlogged the scaled wage values\nround(exp(coef(lm_q2h)['educ']), 2)\n\n\neduc \n1.09 \n\n\nAn increase of one in education corresponds to multiplying the predicted response by \\(1.09\\). This indicates that if education were to be increased by one, holding experience constant, we expect a 9% increase in wage.\n\n\n\n\n\nCode\nlm_q2j &lt;- lm(wage ~ educ + exper + ne + mw + we + so, data = uswages)\nsummary(lm_q2j)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper + ne + mw + we + so, data = uswages)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-993.7 -238.8  -46.1  149.2 7244.1 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -259.9178    51.9863  -5.000 6.24e-07 ***\neduc          51.0931     3.3448  15.275  &lt; 2e-16 ***\nexper          9.8068     0.7507  13.064  &lt; 2e-16 ***\nne            23.8538    26.3327   0.906   0.3651    \nmw            -7.3453    25.7230  -0.286   0.7753    \nwe            66.5168    26.9880   2.465   0.0138 *  \nso                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427.3 on 1994 degrees of freedom\nMultiple R-squared:  0.1387,    Adjusted R-squared:  0.1366 \nF-statistic: 64.24 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\n\n\nLack of identifiability.\n\n\n\n\n\nCode\nrowsum &lt;- uswages$ne + uswages$mw + uswages$we + uswages$so\nhead(rowsum)\n\n\n[1] 1 1 1 1 1 1\n\n\nCode\nsd(rowsum)\n\n\n[1] 0\n\n\n\nAll four regional indicators sum to one because all mean are in one and only one region, thus by including all four indicators in the same model will run into the identifiability problem.\nTo resolve this, either drop one of the variable or drop the intercept term from the model.\n\n\n\n\n\n\n\n\n\nCode\ndata(prostate, package = \"faraway\")\nhead(prostate)\n\n\n      lcavol lweight age      lbph svi      lcp gleason pgg45     lpsa\n1 -0.5798185  2.7695  50 -1.386294   0 -1.38629       6     0 -0.43078\n2 -0.9942523  3.3196  58 -1.386294   0 -1.38629       6     0 -0.16252\n3 -0.5108256  2.6912  74 -1.386294   0 -1.38629       7    20 -0.16252\n4 -1.2039728  3.2828  58 -1.386294   0 -1.38629       6     0 -0.16252\n5  0.7514161  3.4324  62 -1.386294   0 -1.38629       6     0  0.37156\n6 -1.0498221  3.2288  50 -1.386294   0 -1.38629       6     0  0.76547\n\n\n\n\nCode\nlm_q3a &lt;- lm(lpsa ~ lcavol, data = prostate)\nlm_q3a_summary &lt;- summary(lm_q3a)\nsapply(lm_q3a_summary[c('sigma', 'r.squared')], round, 2)\n\n\n    sigma r.squared \n     0.79      0.54 \n\n\n\\(\\sigma = 0.79\\) \\(R^2 = 0.54\\)\n\n\n\n\n\nCode\nsigmas &lt;- lm_q3a_summary$sigma\nrsquares &lt;- lm_q3a_summary$r.squared\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45,\n  data = prostate\n)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason,\n  data = prostate\n)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nsapply(data.frame(npreds = 1:8, sigmas, rsquares), round, 4)\n\n\n     npreds sigmas rsquares\n[1,]      1 0.7875   0.5394\n[2,]      2 0.7506   0.5859\n[3,]      3 0.7168   0.6264\n[4,]      4 0.7108   0.6366\n[5,]      5 0.7073   0.6441\n[6,]      6 0.7102   0.6451\n[7,]      7 0.7048   0.6544\n[8,]      8 0.7084   0.6548\n\n\n\n\n\n\n\nCode\nplot(\n  1:8,\n  sigmas,\n  xlab = \"Number of Predictors\",\n  ylab = \"Residual Standard Error\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\n\nThe RSE shows a sharp initial decrease with the number of predictors. The reduction continues, though not monotonically, reaching its minimum value when 7 predictors are included.”\n\n\nCode\nplot(\n  1:8,\n  rsquares,\n  xlab = \"Number of Predictors\",\n  ylab = \"R-Squared\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\n\nBecause the Residual Sum of Squares (RSS) can only decrease or stay the same when predictors are added, R-squared is monotonically non-decreasing with the number of predictors\n\n\n\n\n\n\n\n\nCode\nlm_q4a_i &lt;- lm(lpsa ~ lcavol, data = prostate)\nlm_q4a_ii &lt;- lm(lcavol ~ lpsa, data = prostate)\n\nplot(lpsa ~ lcavol, data = prostate)\nabline(lm_q4a_i)\nabline(\n  a = -coef(lm_q4a_ii)[1] / coef(lm_q4a_ii)[2],\n  b = 1 / coef(lm_q4a_ii)[2],\n  col = \"red\"\n)\n\nmean_lcavol = mean(prostate$lcavol)\nmean_lpsa = mean(prostate$lpsa)\n\npoints(\n  x = mean_lcavol,\n  y = mean_lpsa,\n  pch = 19,\n  col = \"darkgreen\",\n  cex = 2.5\n)\n\nlabel_text = paste(\n  \"Mean (x̄, ȳ): (\",\n  round(mean_lcavol, 2),\n  \", \",\n  round(mean_lpsa, 2),\n  \")\",\n  sep = \"\"\n)\n\ntext(\n  x = -0.1, # X-coordinate: Place it near the left side of the plot\n  y = 3, # Y-coordinate: Place it near the top of the plot\n  labels = label_text,\n  col = \"darkgreen\", # Set a distinct color\n  adj = 0, # Left-align the text (0 = left, 0.5 = center, 1 = right)\n  cex = 1.5 # Character expansion (text size)\n)\n\n\n\n\n\n\n\n\n\nSince it is a lpsa against lcavol plot, we cannot simply call abline(lm(lcavol ~ lpsa, data = prostate)) for the second regression line since lm(lcavol ~ lpsa, data = prostate) predicts \\(x\\) from \\(y\\), that is, \\(x = a + by\\). Thus, to plot it against the existing \\(y\\) vs \\(x\\) plot, the equation needs to be re-arranged to:\n\\[x = a + by\\]\n\\[by = x - a\\]\n\\[y = \\frac{-a + x}{b}\\]\n\\[y = \\frac{-a}{b} + \\frac{1}{b} \\cdot x\\]\nSince \\(a\\) is the intercept and \\(b\\) is the slope.\nThe point of intersection for these two regression lines is the point of the means \\(\\bar{x}, \\bar{y}\\) because this point is a fundamental property of any simple linear regression model calculated using the Ordinary Least Squares (OLS) method.\n\n\n\n\n\n\n\n\nCode\ndata(cheddar, package = \"faraway\")\nhead(cheddar)\n\n\n  taste Acetic   H2S Lactic\n1  12.3  4.543 3.135   0.86\n2  20.9  5.159 5.043   1.53\n3  39.0  5.366 5.438   1.57\n4  47.9  5.759 7.496   1.81\n5   5.6  4.663 3.807   0.99\n6  25.9  5.697 7.601   1.09\n\n\n\n\nCode\nlm_q5a &lt;- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)\nsummary(lm_q5a)\n\n\n\nCall:\nlm(formula = taste ~ Acetic + H2S + Lactic, data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.390  -6.612  -1.009   4.908  25.449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -28.8768    19.7354  -1.463  0.15540   \nAcetic        0.3277     4.4598   0.073  0.94198   \nH2S           3.9118     1.2484   3.133  0.00425 **\nLactic       19.6705     8.6291   2.280  0.03108 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.13 on 26 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6116 \nF-statistic: 16.22 on 3 and 26 DF,  p-value: 3.81e-06\n\n\n\n\n\n\n\nCode\ncor(fitted(lm_q5a), cheddar$taste)^2\n\n\n[1] 0.6517747\n\n\nThis value appears in as the \\(R^2\\) of lm_q5a’s output.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nnear(summary(lm_q5a)$r.squared, cor(fitted(lm_q5a), cheddar$taste)^2)\n\n\n[1] TRUE\n\n\n\n\n\n\n\nCode\nlm_q5c &lt;- lm(taste ~ Acetic + H2S + Lactic - 1, data = cheddar)\nround(summary(lm_q5c)$r.squared, 3)\n\n\n[1] 0.888\n\n\nWithout the intercept term, \\(R^2 = 0.888\\).\nWhen compared to the correlation between the fitted values and the response squared, the latter is a much more plausible value.\n####q5d\n\n\nCode\nX &lt;- model.matrix(\n  ~ Acetic + H2S + Lactic,\n  data = cheddar\n)\ny &lt;- cheddar$taste\nqrx &lt;- qr(X)\nf = t(qr.Q(qrx)) %*% y\nbacksolve(qr.R(qrx), f)\n\n\n            [,1]\n[1,] -28.8767696\n[2,]   0.3277413\n[3,]   3.9118411\n[4,]  19.6705434\n\n\n\n\n\n\n\n\n\n\nCode\ndata(wafer, package = \"faraway\")\nhead(wafer)\n\n\n  x1 x2 x3 x4 resist\n1  -  -  -  -  193.4\n2  +  -  -  -  247.6\n3  -  +  -  -  168.2\n4  +  +  -  -  205.0\n5  -  -  +  -  303.4\n6  +  -  +  -  339.9\n\n\n\n\nCode\nlm_q6a &lt;- lm(resist ~ x1 + x2 + x3 + x4, data = wafer)\nsummary(lm_q6a, cor = T)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3 + x4, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.381 -17.119   4.825  16.644  33.769 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   236.78      14.77  16.032 5.65e-09 ***\nx1+            25.76      13.21   1.950 0.077085 .  \nx2+           -69.89      13.21  -5.291 0.000256 ***\nx3+            43.59      13.21   3.300 0.007083 ** \nx4+           -14.49      13.21  -1.097 0.296193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.42 on 11 degrees of freedom\nMultiple R-squared:  0.7996,    Adjusted R-squared:  0.7267 \nF-statistic: 10.97 on 4 and 11 DF,  p-value: 0.0007815\n\nCorrelation of Coefficients:\n    (Intercept) x1+   x2+   x3+  \nx1+ -0.45                        \nx2+ -0.45        0.00            \nx3+ -0.45        0.00  0.00      \nx4+ -0.45        0.00  0.00  0.00\n\n\n\n\nCode\nX &lt;- model.matrix(lm_q6a)\nhead(X)\n\n\n  (Intercept) x1+ x2+ x3+ x4+\n1           1   0   0   0   0\n2           1   1   0   0   0\n3           1   0   1   0   0\n4           1   1   1   0   0\n5           1   0   0   1   0\n6           1   1   0   1   0\n\n\nThe function automatically coded - and + as \\(0\\) and \\(1\\) respectively.\n\n\n\n\n\nCode\ncor(X)\n\n\nWarning in cor(X): the standard deviation is zero\n\n\n            (Intercept) x1+ x2+ x3+ x4+\n(Intercept)           1  NA  NA  NA  NA\nx1+                  NA   1   0   0   0\nx2+                  NA   0   1   0   0\nx3+                  NA   0   0   1   0\nx4+                  NA   0   0   0   1\n\n\nThe missing values are due to the intercept being constant across all observations. A constant variable has a standard deviation of zero. Because the correlation coefficient formula requires dividing by the product of the two variables’ standard deviations, the zero value in the denominator makes the correlation mathematically undefined.\nIn other words:\n\\[\\bar{x} = \\frac{1 + 1 + 1 + \\ldots}{n} = 1\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum{(x_i - \\bar{x})^2}}{n - 1}}\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum{(1 - 1)^2}}{n - 1}} = \\sqrt{\\frac{\\sum{(0)^2}}{n - 1}} = 0\\]\nGiven the formula:\n\\[\\text{Correlation} = \\frac{cov(x, y)}{\\sigma_x \\sigma_y}\\]\nwe can see that when the denominator is \\(0\\), it will leads to a division by zero error.\n\n\n\n\n\nCode\nround(summary(lm_q6a)$coefficients[2], 2)\n\n\n[1] 25.76\n\n\nAn increase in 25.76.\n\n\n\n\n\nCode\nlm_q6d &lt;- lm(resist ~ x1 + x2 + x3, data = wafer)\nsummary(lm_q6d)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.137 -20.550   3.575  18.463  41.012 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   229.54      13.32  17.231 7.88e-10 ***\nx1+            25.76      13.32   1.934 0.077047 .  \nx2+           -69.89      13.32  -5.246 0.000206 ***\nx3+            43.59      13.32   3.272 0.006677 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.64 on 12 degrees of freedom\nMultiple R-squared:  0.7777,    Adjusted R-squared:  0.7221 \nF-statistic: 13.99 on 3 and 12 DF,  p-value: 0.0003187\n\n\n\nEstimates for x1, x2, and x3 remain the same, but the intercept term’s estimate has slightly decreased.\nStandard errors have increased slightly for all estimands.\n\n\n\n\n\n\nCode\nsummary(lm_q6a, cor = T)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3 + x4, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.381 -17.119   4.825  16.644  33.769 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   236.78      14.77  16.032 5.65e-09 ***\nx1+            25.76      13.21   1.950 0.077085 .  \nx2+           -69.89      13.21  -5.291 0.000256 ***\nx3+            43.59      13.21   3.300 0.007083 ** \nx4+           -14.49      13.21  -1.097 0.296193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.42 on 11 degrees of freedom\nMultiple R-squared:  0.7996,    Adjusted R-squared:  0.7267 \nF-statistic: 10.97 on 4 and 11 DF,  p-value: 0.0007815\n\nCorrelation of Coefficients:\n    (Intercept) x1+   x2+   x3+  \nx1+ -0.45                        \nx2+ -0.45        0.00            \nx3+ -0.45        0.00  0.00      \nx4+ -0.45        0.00  0.00  0.00\n\n\nThe design matrix \\(X\\) is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix. Orthogonality ensures that the estimated effect of each predictor is decoupled from the others. Therefore, the coefficient estimate for any given predictor is invariant to the inclusion or exclusion of other predictors in the model.\n\n\n\n\n\n\n\n\nCode\ndata(truck, package = \"faraway\")\nhead(truck)\n\n\n  B C D E O height\n1 - - - - -   7.78\n2 + - - + -   8.15\n3 - + - + -   7.50\n4 + + - - -   7.59\n5 - - + + -   7.94\n6 + - + - -   7.69\n\n\n\n\nCode\ntruck$B &lt;- sapply(truck$B, function(x) ifelse(x == \"-\", -1, 1))\ntruck$C &lt;- sapply(truck$C, function(x) ifelse(x == \"-\", -1, 1))\ntruck$D &lt;- sapply(truck$D, function(x) ifelse(x == \"-\", -1, 1))\ntruck$E &lt;- sapply(truck$E, function(x) ifelse(x == \"-\", -1, 1))\ntruck$O &lt;- sapply(truck$O, function(x) ifelse(x == \"-\", -1, 1))\n\n\n\n\nCode\nlm_q7a &lt;- lm(height ~ B + C + D + E + O, data = truck)\nsummary(lm_q7a, cor = T)\n\n\n\nCall:\nlm(formula = height ~ B + C + D + E + O, data = truck)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.33125 -0.09427  0.01625  0.11917  0.25875 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.63604    0.02291 333.316  &lt; 2e-16 ***\nB            0.11062    0.02291   4.829 1.85e-05 ***\nC           -0.08813    0.02291  -3.847   0.0004 ***\nD           -0.01437    0.02291  -0.627   0.5337    \nE            0.05187    0.02291   2.264   0.0288 *  \nO           -0.12979    0.02291  -5.665 1.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1587 on 42 degrees of freedom\nMultiple R-squared:  0.6433,    Adjusted R-squared:  0.6008 \nF-statistic: 15.15 on 5 and 42 DF,  p-value: 1.681e-08\n\nCorrelation of Coefficients:\n  (Intercept) B    C    D    E   \nB 0.00                           \nC 0.00        0.00               \nD 0.00        0.00 0.00          \nE 0.00        0.00 0.00 0.00     \nO 0.00        0.00 0.00 0.00 0.00\n\n\nCode\ncoef(lm_q7a)\n\n\n(Intercept)           B           C           D           E           O \n  7.6360417   0.1106250  -0.0881250  -0.0143750   0.0518750  -0.1297917 \n\n\n\n\n\n\n\nCode\nlm_q7b &lt;- lm(height ~ B + C + D + E, data = truck)\nsummary(lm_q7b, cor = T)\n\n\n\nCall:\nlm(formula = height ~ B + C + D + E, data = truck)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46104 -0.12479 -0.00479  0.14396  0.34896 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.63604    0.03007 253.915  &lt; 2e-16 ***\nB            0.11062    0.03007   3.679 0.000648 ***\nC           -0.08813    0.03007  -2.930 0.005402 ** \nD           -0.01437    0.03007  -0.478 0.635071    \nE            0.05187    0.03007   1.725 0.091717 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2084 on 43 degrees of freedom\nMultiple R-squared:  0.3706,    Adjusted R-squared:  0.3121 \nF-statistic: 6.331 on 4 and 43 DF,  p-value: 0.0004258\n\nCorrelation of Coefficients:\n  (Intercept) B    C    D   \nB 0.00                      \nC 0.00        0.00          \nD 0.00        0.00 0.00     \nE 0.00        0.00 0.00 0.00\n\n\nCode\ncoef(lm_q7b)\n\n\n(Intercept)           B           C           D           E \n   7.636042    0.110625   -0.088125   -0.014375    0.051875 \n\n\nThe coefficients remains unchanged.\nExamining the \\(X\\) matrix:\n\n\nCode\ncor(model.matrix(lm_q7a))\n\n\nWarning in cor(model.matrix(lm_q7a)): the standard deviation is zero\n\n\n            (Intercept)  B  C  D  E  O\n(Intercept)           1 NA NA NA NA NA\nB                    NA  1  0  0  0  0\nC                    NA  0  1  0  0  0\nD                    NA  0  0  1  0  0\nE                    NA  0  0  0  1  0\nO                    NA  0  0  0  0  1\n\n\nThe design matrix \\(X\\) is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix\n\n\n\n\n\nCode\ntruck_transformed &lt;- transform(truck, A = B + C + D + E)\nhead(truck_transformed)\n\n\n   B  C  D  E  O height  A\n1 -1 -1 -1 -1 -1   7.78 -4\n2  1 -1 -1  1 -1   8.15  0\n3 -1  1 -1  1 -1   7.50  0\n4  1  1 -1 -1 -1   7.59  0\n5 -1 -1  1  1 -1   7.94  0\n6  1 -1  1 -1 -1   7.69  0\n\n\nCode\nlm_q7c &lt;- lm(height ~ A + B + C + D + E + O, data = truck_transformed)\ncoef(lm_q7c)\n\n\n(Intercept)           A           B           C           D           E \n  7.6360417   0.0518750   0.0587500  -0.1400000  -0.0662500          NA \n          O \n -0.1297917 \n\n\nE is not estimable; A = B + C + D + E makes the predictors collinear, so R excluded E due to identifiability issues (from pg31: Predictors occurring later in the model formula are preferred for removal …)\n\n\n\n\n\nCode\nX &lt;- model.matrix(lm_q7c)\nX\n\n\n   (Intercept)  A  B  C  D  E  O\n1            1 -4 -1 -1 -1 -1 -1\n2            1  0  1 -1 -1  1 -1\n3            1  0 -1  1 -1  1 -1\n4            1  0  1  1 -1 -1 -1\n5            1  0 -1 -1  1  1 -1\n6            1  0  1 -1  1 -1 -1\n7            1  0 -1  1  1 -1 -1\n8            1  4  1  1  1  1 -1\n9            1 -4 -1 -1 -1 -1  1\n10           1  0  1 -1 -1  1  1\n11           1  0 -1  1 -1  1  1\n12           1  0  1  1 -1 -1  1\n13           1  0 -1 -1  1  1  1\n14           1  0  1 -1  1 -1  1\n15           1  0 -1  1  1 -1  1\n16           1  4  1  1  1  1  1\n17           1 -4 -1 -1 -1 -1 -1\n18           1  0  1 -1 -1  1 -1\n19           1  0 -1  1 -1  1 -1\n20           1  0  1  1 -1 -1 -1\n21           1  0 -1 -1  1  1 -1\n22           1  0  1 -1  1 -1 -1\n23           1  0 -1  1  1 -1 -1\n24           1  4  1  1  1  1 -1\n25           1 -4 -1 -1 -1 -1  1\n26           1  0  1 -1 -1  1  1\n27           1  0 -1  1 -1  1  1\n28           1  0  1  1 -1 -1  1\n29           1  0 -1 -1  1  1  1\n30           1  0  1 -1  1 -1  1\n31           1  0 -1  1  1 -1  1\n32           1  4  1  1  1  1  1\n33           1 -4 -1 -1 -1 -1 -1\n34           1  0  1 -1 -1  1 -1\n35           1  0 -1  1 -1  1 -1\n36           1  0  1  1 -1 -1 -1\n37           1  0 -1 -1  1  1 -1\n38           1  0  1 -1  1 -1 -1\n39           1  0 -1  1  1 -1 -1\n40           1  4  1  1  1  1 -1\n41           1 -4 -1 -1 -1 -1  1\n42           1  0  1 -1 -1  1  1\n43           1  0 -1  1 -1  1  1\n44           1  0  1  1 -1 -1  1\n45           1  0 -1 -1  1  1  1\n46           1  0  1 -1  1 -1  1\n47           1  0 -1  1  1 -1  1\n48           1  4  1  1  1  1  1\nattr(,\"assign\")\n[1] 0 1 2 3 4 5 6\n\n\nCode\ny &lt;- truck$height\ny\n\n\n [1] 7.78 8.15 7.50 7.59 7.94 7.69 7.56 7.56 7.50 7.88 7.50 7.63 7.32 7.56 7.18\n[16] 7.81 7.78 8.18 7.56 7.56 8.00 8.09 7.62 7.81 7.25 7.88 7.56 7.75 7.44 7.69\n[31] 7.18 7.50 7.81 7.88 7.50 7.75 7.88 8.06 7.44 7.69 7.12 7.44 7.50 7.56 7.44\n[46] 7.62 7.25 7.59\n\n\n\n\nCode\ntry(solve(t(X) %*% X) %*% t(X) %*% y)\n\n\nError in solve.default(t(X) %*% X) : \n  Lapack routine dgesv: system is exactly singular: U[6,6] = 0\n\n\nThe model cannot be estimated due to linear dependence among the predictors, which makes the design matrix rank-deficient.\n\n\n\n\n\nCode\nqrx &lt;- qr(X)\ndim(qr.Q(qrx))\n\n\n[1] 48  7\n\n\nCode\nf &lt;- t(qr.Q(qrx)) %*% y\nbacksolve(qr.R(qrx), f)\n\n\n              [,1]\n[1,]  7.636042e+00\n[2,] -1.853252e+13\n[3,]  1.853252e+13\n[4,]  1.853252e+13\n[5,]  1.853252e+13\n[6,] -1.267479e-01\n[7,]  1.853252e+13\n\n\n\n\n\nWe will obtain the same results as coef(lm_q7c) running\n\n\nCode\nqr.coef(qrx, truck$height)\n\n\n(Intercept)           A           B           C           D           E \n  7.6360417   0.0518750   0.0587500  -0.1400000  -0.0662500          NA \n          O \n -0.1297917 \n\n\n\n\n\n\n\n\n\\[\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 0\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\n\n\nGiven the matrix \\(X\\) and vector \\(y\\):\n\\[\nX = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n\\]\nWe want to find the Least Squares estimator: \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\).\n\\[\nX^T = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\]\n\\[\nX^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 3 \\\\\n3 & 3\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} = \\frac{1}{9} \\begin{pmatrix}\n3 & -3 \\\\\n-3 & 6\n\\end{pmatrix} = \\begin{pmatrix}\n3/9 & -3/9 \\\\\n-3/9 & 6/9\n\\end{pmatrix} = \\begin{pmatrix}\n1/3 & -1/3 \\\\\n-1/3 & 2/3\n\\end{pmatrix}\n\\]\n\\[\nX^T y = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\\\\ny_4 + y_5 + y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} (X^T y) =\n\\hat{\\beta}\n= \\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1/3 & -1/3 \\\\\n-1/3 & 2/3\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6\\\\\ny_4 + y_5 + y_6\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{y_1 + y_2 + y_3}{3}\\\\\n\\frac{y_4 + y_5 + y_6}{3} - \\frac{y_1 + y_2 + y_3}{3}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{3} \\sum_{i = 1}^3 y_i\\\\\n\\frac{1}{3} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\end{pmatrix}\n\\]\n\\[\n\\hat{\\beta}_0 = \\frac{y_1 + y_2 + y_3}{3} = \\frac{1}{3} \\sum_{i = 1}^3 y_i\n\\]\nNotice that \\(\\hat{\\beta}_0\\) is the mean of all the observations from the first group. (Think of it as the reference group in linear regression)\n\\[\n\\hat{\\beta}_1 = \\frac{1}{3} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\]\nNotice that \\(\\hat{\\beta}_1\\) is the difference between the means of both groups.\n\n\n\n\\[\n\\begin{pmatrix}\n1 & -1\\\\\n1 & -1\\\\\n1 & -1\\\\\n1 & 1\\\\\n1 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\n\n\nGiven the matrix \\(X\\) and vector \\(y\\) (where Group 1 is -1 and Group 2 is +1):\n\\[\nX = \\begin{pmatrix}\n1 & -1 \\\\\n1 & -1 \\\\\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n\\]\nWe want to find the Least Squares estimator: \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\).\n\\[\nX^T = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\]\n\\[\nX^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & -1 \\\\\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 0 \\\\\n0 & 6\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} = \\begin{pmatrix}\n1/6 & 0 \\\\\n0 & 1/6\n\\end{pmatrix}\n\\]\n\\[\nX^T y = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\\\\n-y_1 - y_2 - y_3 + y_4 + y_5 + y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i - \\sum_{i=1}^3 y_i\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} (X^T y) =\n\\hat{\\beta}\n= \\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1/6 & 0 \\\\\n0 & 1/6\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i - \\sum_{i=1}^3 y_i\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{6} \\sum_{i = 1}^6 y_i \\\\\n\\frac{1}{6} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\end{pmatrix}\n\\]\n\\[\n\\hat{\\beta}_0 = \\frac{1}{6} \\sum_{i = 1}^6 y_i\n\\]\nNotice that \\(\\hat{\\beta}_0\\) is now the grand mean (the average of all observations combined). All because -1 and 1 are centred around 0, and the intercept represents the centre of the data.\n\\[\n\\hat{\\beta}_1 = \\frac{1}{6} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\]\nNotice that \\(\\hat{\\beta}_1\\) is half the difference between the means of the two groups. (Since the difference in the x-axis “run” is now 2 units (from -1 to 1), the slope is halved compared to the 0/1 coding).\n\nThe first coding computes the mean of one group (\\(\\hat{\\beta}_0\\)) and then the difference in their means (\\(\\hat{\\beta}_1\\)).\nThe second computes an overall mean (\\(\\hat{\\beta}_0\\)) while the second parameter represents the difference from that overall mean (\\(\\hat{\\beta}_1\\)).\n\nThe second coding is more symmetrical (but both result in the same fitted values and residuals)."
  },
  {
    "objectID": "chapter2.html#exercises",
    "href": "chapter2.html#exercises",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\ndata(teengamb, package = \"faraway\")\nlm_q1a &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n\n\n\n\n\n\nCode\ncat(round(summary(lm_q1a)$r.squared * 100, 2), \"%\")\n\n\n52.67 %\n\n\nMultiple R-square: 52.67%\n\n\n\n\n\nCode\ncat(\n  \"Index: \",\n  which.max(residuals(lm_q1a)),\n  \"\\nResidual\",\n  max(residuals(lm_q1a))\n)\n\n\nIndex:  24 \nResidual 94.25222\n\n\n\n\n\n\n\nCode\ncat(\n  \"Mean of residuals: \",\n  signif(mean(residuals(lm_q1a)), 3),\n  \"\\nMedian of residuals: \",\n  signif(median(residuals(lm_q1a)), 3)\n)\n\n\nMean of residuals:  -1.56e-16 \nMedian of residuals:  -1.45\n\n\nWe expect the mean of the residuals from a linear model to be always zero (assuming we do not drop the intercept term, \\(\\beta_0\\)), \\(\\bar{\\hat{\\epsilon}} = 0\\). Unlike the mean of the residuals, which is mathematically guaranteed to be zero in an OLS model, the median is not. Consequently, the median is often close to zero but rarely, if ever, exactly zero.\n\n\n\n\n\nCode\ncat(\n  \"Correlation of the residuals with the fitted values: \",\n  signif(cor(residuals(lm_q1a), predict(lm_q1a)), 3)\n)\n\n\nCorrelation of the residuals with the fitted values:  4.76e-17\n\n\nThe correlation is effectively zero because the residual vector \\(\\hat{\\epsilon}\\) and fitted value vector \\(\\hat{y}\\) are orthogonal to each other.\n\n\n\n\n\nCode\ncat(\n  \"Correlation of the residuals with income: \",\n  signif(cor(residuals(lm_q1a), teengamb$income), 3)\n)\n\n\nCorrelation of the residuals with income:  3.25e-17\n\n\nYes, it will always be zero because geometrically, the OLS model projects the outcome vector \\(y\\) onto the plane spanned by the set of all predictor variables. In our case, the income vector is part of that plane. Consequently, the fitted values vector \\(\\hat{y}\\) lies on this plane while the residuals vector \\(\\hat{\\epsilon}\\) is orthogonal to the plane.\n\n\n\nFemale is coded as one and male is coded as zero.\n\n\nCode\ncat(\n  \"For females, the difference in average weekly gambling between females\",\n  \"and males, holding other factors constant, is\",\n  signif(summary(lm_q1a)$coefficients[2], 3),\n  \"pounds.\"\n)\n\n\nFor females, the difference in average weekly gambling between females and males, holding other factors constant, is -22.1 pounds.\n\n\n\n\n\n\n\n\n\n\nCode\ndata(uswages, package = \"faraway\")\nlm_q2a &lt;- lm(wage ~ educ + exper, data = uswages)\nsummary(lm_q2a)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1018.2  -237.9   -50.9   149.9  7228.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -242.7994    50.6816  -4.791 1.78e-06 ***\neduc          51.1753     3.3419  15.313  &lt; 2e-16 ***\nexper          9.7748     0.7506  13.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427.9 on 1997 degrees of freedom\nMultiple R-squared:  0.1351,    Adjusted R-squared:  0.1343 \nF-statistic:   156 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nCode\napply(uswages[, 1:3], 2, min)\n\n\n wage  educ exper \n50.39  0.00 -2.00 \n\n\n\n\n\n\n\nCode\n# When predictors are zero, the intercept is the prediction.\ncat(\n  \"Predicted wage for a worker with no education and no experience:\",\n  format(round(summary(lm_q2a)$coef[1], 2), nsmall = 2)\n)\n\n\nPredicted wage for a worker with no education and no experience: -242.80\n\n\nIt doesn’t make sense to report a negative wage value.\n\n\nCode\nsubset(uswages, educ == 0 | exper == 0)\n\n\n         wage educ exper race smsa ne mw so we pt\n12878  148.39   16     0    0    1  0  1  0  0  1\n21572  308.64   16     0    0    1  0  0  1  0  0\n6090   144.03   12     0    0    1  1  0  0  0  0\n4310   493.83   14     0    0    1  1  0  0  0  0\n6305    56.79   14     0    0    0  1  0  0  0  1\n2245   830.96   18     0    0    1  1  0  0  0  0\n3858   241.21   12     0    0    0  1  0  0  0  0\n27419  142.45   12     0    0    1  0  0  0  1  0\n10853  197.41   12     0    0    1  0  1  0  0  1\n11576  283.95   16     0    0    1  0  1  0  0  0\n27774  347.22   14     0    0    1  0  0  0  1  0\n19834   61.73   13     0    0    0  0  0  1  0  1\n20846  189.93   16     0    0    1  0  0  1  0  1\n8006    56.98   14     0    0    0  0  1  0  0  1\n27787  892.59   13     0    0    0  0  0  0  1  0\n4014    78.56   14     0    0    0  1  0  0  0  1\n27585  308.64   16     0    0    1  0  0  0  1  0\n15354  335.10   12     0    0    1  0  0  1  0  0\n18362  195.47   16     0    0    0  0  0  1  0  0\n24439  159.10    0    27    0    0  0  0  0  1  0\n3778   274.35   12     0    0    1  1  0  0  0  0\n26303  189.93    0    54    0    1  0  0  0  1  0\n6345    54.61   13     0    0    0  1  0  0  0  1\n21435  106.08   14     0    0    0  0  0  1  0  1\n4648   474.83   12     0    0    1  1  0  0  0  0\n12401  174.90   14     0    0    0  0  1  0  0  1\n24355  165.43   12     0    0    0  0  0  0  1  0\n3881   474.83   18     0    0    0  1  0  0  0  0\n21604  400.26    0    52    0    1  0  0  1  0  0\n20410  246.91   16     0    0    0  0  0  1  0  1\n27138   96.19   12     0    0    1  0  0  0  1  0\n17129  308.64    0    47    1    1  0  0  1  0  0\n10476  130.58   13     0    1    1  0  1  0  0  0\n13803   52.23   14     0    1    1  0  0  1  0  1\n8902    66.14   12     0    0    1  0  1  0  0  1\n16915  143.87   12     0    0    1  0  0  1  0  1\n2780  5144.03    0    18    0    1  1  0  0  0  0\n13686  395.06    0    42    0    1  0  0  1  0  0\n12711  205.76   15     0    0    1  0  1  0  0  0\n15548  200.62   12     0    0    1  0  0  1  0  0\n21269  284.90   13     0    0    1  0  0  1  0  1\n26160  166.19   13     0    0    1  0  0  0  1  1\n6148    74.07   12     0    0    1  1  0  0  0  0\n1376   569.80   13     0    0    1  1  0  0  0  0\n21479  237.42   16     0    1    0  0  0  1  0  1\n3337   144.44   15     0    0    1  1  0  0  0  1\n21367  100.78   13     0    0    1  0  0  1  0  1\n8485   427.98   14     0    0    1  0  1  0  0  1\n18047  154.32   13     0    0    1  0  0  1  0  0\n27780   92.59   12     0    0    1  0  0  0  1  1\n24297  216.05   15     0    0    1  0  0  0  1  0\n8948    77.16   14     0    0    1  0  1  0  0  1\n25196  227.02   15     0    0    1  0  0  0  1  1\n14153  108.02   13     0    1    1  0  0  1  0  1\n7651   284.90   16     0    0    0  0  1  0  0  1\n21436  356.13    0    54    0    1  0  0  1  0  0\n13754  197.22   16     0    0    1  0  0  1  0  1\n1620   493.83   14     0    0    0  1  0  0  0  0\n7739   465.80   18     0    0    1  0  1  0  0  0\n5444    54.01   12     0    0    1  1  0  0  0  1\n10738  102.88   12     0    0    1  0  1  0  0  1\n26011   86.13   12     0    0    1  0  0  0  1  1\n8235    85.66   13     0    1    1  0  1  0  0  1\n22810  142.45   13     0    0    0  0  0  0  1  0\n25047  593.54    0    56    0    1  0  0  0  1  0\n12408  178.60   16     0    0    0  0  1  0  0  1\n\n\n\n\nCode\nsubset(uswages, educ == 0 & exper == 0)\n\n\n [1] wage  educ  exper race  smsa  ne    mw    so    we    pt   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nAccording to the above output, the data confirms that while some workers have either zero education or zero experience, no worker has both simultaneously.\nTherefore, the model’s prediction for this scenario is a pure extrapolation, as it’s forecasting for a data point that does not exist in the dataset.\n\n\n\n\n\nCode\nall.equal(cor(predict(lm_q2a), uswages$wage)^2, summary(lm_q2a)$r.squared)\n\n\n[1] TRUE\n\n\n\n\n\n\n\nCode\nlm_q2e &lt;- lm(wage ~ educ + exper - 1, data = uswages)\nsummary(lm_q2e)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper - 1, data = uswages)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-903.6 -254.9  -70.9  140.8 7144.6 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \neduc    36.014      1.079   33.37   &lt;2e-16 ***\nexper    7.854      0.638   12.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 430.2 on 1998 degrees of freedom\nMultiple R-squared:  0.6819,    Adjusted R-squared:  0.6815 \nF-statistic:  2141 on 2 and 1998 DF,  p-value: &lt; 2.2e-16\n\n\n\\(R^2\\) has increased.\n\n\nCode\ncor(predict(lm_q2e), uswages$wage)^2\n\n\n[1] 0.1340295\n\n\nBut direct calculation provides a similar value to the initial model.\n\n\n\n\n\nCode\ncat(\n  \"RSS of Initial Model:\",\n  sum(residuals(lm_q2a)^2),\n  \"\\nRSS of Intercept Removed Model:\",\n  sum(residuals(lm_q2e)^2)\n)\n\n\nRSS of Initial Model: 365568644 \nRSS of Intercept Removed Model: 369769963\n\n\nRSS of Initial Model (More parameter, more flexibility) &lt; RSS of Intercept Removed Model (One less parameter)\n\n\n\n\n\nCode\nround(coef(lm_q2a)['educ'], 2)\n\n\n educ \n51.18 \n\n\n\n\n\n\n\nCode\nlm_q2h &lt;- lm(log(wage) ~ educ + exper, data = uswages)\nsummary(lm_q2h)\n\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe Residual standard error is only 427.9 in the first model but in the logged response model, it is 0.6615 due to scaling.\n\\(R^{2}_{\\text{first model}} = 0.1351 &lt; R^{2}_{\\text{logged model}} = 0.1749\\)\n\nSince \\(R^2\\) is unit free, the logged model is preferable.\n\n\n\n\n\nFirst we need to know the value of experience on the unlogged wage scale by exponentiating both side.\n\\[\n\\text{wage} = e^{\\beta_0 + \\beta_1 \\cdot \\text{education} + \\beta_2 \\cdot \\text{experience}}\n\\]\n\\[\n\\text{wage} = e^{\\beta_0} \\times e^{\\beta_1 \\cdot \\text{education}} \\times e^{\\beta_2 \\cdot \\text{experience}}\n\\]\n\n\nCode\n# Need to unlogged the scaled wage values\nround(exp(coef(lm_q2h)['educ']), 2)\n\n\neduc \n1.09 \n\n\nAn increase of one in education corresponds to multiplying the predicted response by \\(1.09\\). This indicates that if education were to be increased by one, holding experience constant, we expect a 9% increase in wage.\n\n\n\n\n\nCode\nlm_q2j &lt;- lm(wage ~ educ + exper + ne + mw + we + so, data = uswages)\nsummary(lm_q2j)\n\n\n\nCall:\nlm(formula = wage ~ educ + exper + ne + mw + we + so, data = uswages)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-993.7 -238.8  -46.1  149.2 7244.1 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -259.9178    51.9863  -5.000 6.24e-07 ***\neduc          51.0931     3.3448  15.275  &lt; 2e-16 ***\nexper          9.8068     0.7507  13.064  &lt; 2e-16 ***\nne            23.8538    26.3327   0.906   0.3651    \nmw            -7.3453    25.7230  -0.286   0.7753    \nwe            66.5168    26.9880   2.465   0.0138 *  \nso                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427.3 on 1994 degrees of freedom\nMultiple R-squared:  0.1387,    Adjusted R-squared:  0.1366 \nF-statistic: 64.24 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\n\n\nLack of identifiability.\n\n\n\n\n\nCode\nrowsum &lt;- uswages$ne + uswages$mw + uswages$we + uswages$so\nhead(rowsum)\n\n\n[1] 1 1 1 1 1 1\n\n\nCode\nsd(rowsum)\n\n\n[1] 0\n\n\n\nAll four regional indicators sum to one because all mean are in one and only one region, thus by including all four indicators in the same model will run into the identifiability problem.\nTo resolve this, either drop one of the variable or drop the intercept term from the model.\n\n\n\n\n\n\n\n\n\nCode\ndata(prostate, package = \"faraway\")\nhead(prostate)\n\n\n      lcavol lweight age      lbph svi      lcp gleason pgg45     lpsa\n1 -0.5798185  2.7695  50 -1.386294   0 -1.38629       6     0 -0.43078\n2 -0.9942523  3.3196  58 -1.386294   0 -1.38629       6     0 -0.16252\n3 -0.5108256  2.6912  74 -1.386294   0 -1.38629       7    20 -0.16252\n4 -1.2039728  3.2828  58 -1.386294   0 -1.38629       6     0 -0.16252\n5  0.7514161  3.4324  62 -1.386294   0 -1.38629       6     0  0.37156\n6 -1.0498221  3.2288  50 -1.386294   0 -1.38629       6     0  0.76547\n\n\n\n\nCode\nlm_q3a &lt;- lm(lpsa ~ lcavol, data = prostate)\nlm_q3a_summary &lt;- summary(lm_q3a)\nsapply(lm_q3a_summary[c('sigma', 'r.squared')], round, 2)\n\n\n    sigma r.squared \n     0.79      0.54 \n\n\n\\(\\sigma = 0.79\\) \\(R^2 = 0.54\\)\n\n\n\n\n\nCode\nsigmas &lt;- lm_q3a_summary$sigma\nrsquares &lt;- lm_q3a_summary$r.squared\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45,\n  data = prostate\n)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nlm_q3b &lt;- lm(\n  lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason,\n  data = prostate\n)\nlm_q3b_summary &lt;- summary(lm_q3b)\nsigmas &lt;- c(sigmas, lm_q3b_summary$sigma)\nrsquares &lt;- c(rsquares, lm_q3b_summary$r.squared)\n\nsapply(data.frame(npreds = 1:8, sigmas, rsquares), round, 4)\n\n\n     npreds sigmas rsquares\n[1,]      1 0.7875   0.5394\n[2,]      2 0.7506   0.5859\n[3,]      3 0.7168   0.6264\n[4,]      4 0.7108   0.6366\n[5,]      5 0.7073   0.6441\n[6,]      6 0.7102   0.6451\n[7,]      7 0.7048   0.6544\n[8,]      8 0.7084   0.6548\n\n\n\n\n\n\n\nCode\nplot(\n  1:8,\n  sigmas,\n  xlab = \"Number of Predictors\",\n  ylab = \"Residual Standard Error\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\n\nThe RSE shows a sharp initial decrease with the number of predictors. The reduction continues, though not monotonically, reaching its minimum value when 7 predictors are included.”\n\n\nCode\nplot(\n  1:8,\n  rsquares,\n  xlab = \"Number of Predictors\",\n  ylab = \"R-Squared\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\n\nBecause the Residual Sum of Squares (RSS) can only decrease or stay the same when predictors are added, R-squared is monotonically non-decreasing with the number of predictors\n\n\n\n\n\n\n\n\nCode\nlm_q4a_i &lt;- lm(lpsa ~ lcavol, data = prostate)\nlm_q4a_ii &lt;- lm(lcavol ~ lpsa, data = prostate)\n\nplot(lpsa ~ lcavol, data = prostate)\nabline(lm_q4a_i)\nabline(\n  a = -coef(lm_q4a_ii)[1] / coef(lm_q4a_ii)[2],\n  b = 1 / coef(lm_q4a_ii)[2],\n  col = \"red\"\n)\n\nmean_lcavol = mean(prostate$lcavol)\nmean_lpsa = mean(prostate$lpsa)\n\npoints(\n  x = mean_lcavol,\n  y = mean_lpsa,\n  pch = 19,\n  col = \"darkgreen\",\n  cex = 2.5\n)\n\nlabel_text = paste(\n  \"Mean (x̄, ȳ): (\",\n  round(mean_lcavol, 2),\n  \", \",\n  round(mean_lpsa, 2),\n  \")\",\n  sep = \"\"\n)\n\ntext(\n  x = -0.1, # X-coordinate: Place it near the left side of the plot\n  y = 3, # Y-coordinate: Place it near the top of the plot\n  labels = label_text,\n  col = \"darkgreen\", # Set a distinct color\n  adj = 0, # Left-align the text (0 = left, 0.5 = center, 1 = right)\n  cex = 1.5 # Character expansion (text size)\n)\n\n\n\n\n\n\n\n\n\nSince it is a lpsa against lcavol plot, we cannot simply call abline(lm(lcavol ~ lpsa, data = prostate)) for the second regression line since lm(lcavol ~ lpsa, data = prostate) predicts \\(x\\) from \\(y\\), that is, \\(x = a + by\\). Thus, to plot it against the existing \\(y\\) vs \\(x\\) plot, the equation needs to be re-arranged to:\n\\[x = a + by\\]\n\\[by = x - a\\]\n\\[y = \\frac{-a + x}{b}\\]\n\\[y = \\frac{-a}{b} + \\frac{1}{b} \\cdot x\\]\nSince \\(a\\) is the intercept and \\(b\\) is the slope.\nThe point of intersection for these two regression lines is the point of the means \\(\\bar{x}, \\bar{y}\\) because this point is a fundamental property of any simple linear regression model calculated using the Ordinary Least Squares (OLS) method.\n\n\n\n\n\n\n\n\nCode\ndata(cheddar, package = \"faraway\")\nhead(cheddar)\n\n\n  taste Acetic   H2S Lactic\n1  12.3  4.543 3.135   0.86\n2  20.9  5.159 5.043   1.53\n3  39.0  5.366 5.438   1.57\n4  47.9  5.759 7.496   1.81\n5   5.6  4.663 3.807   0.99\n6  25.9  5.697 7.601   1.09\n\n\n\n\nCode\nlm_q5a &lt;- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)\nsummary(lm_q5a)\n\n\n\nCall:\nlm(formula = taste ~ Acetic + H2S + Lactic, data = cheddar)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.390  -6.612  -1.009   4.908  25.449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -28.8768    19.7354  -1.463  0.15540   \nAcetic        0.3277     4.4598   0.073  0.94198   \nH2S           3.9118     1.2484   3.133  0.00425 **\nLactic       19.6705     8.6291   2.280  0.03108 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.13 on 26 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6116 \nF-statistic: 16.22 on 3 and 26 DF,  p-value: 3.81e-06\n\n\n\n\n\n\n\nCode\ncor(fitted(lm_q5a), cheddar$taste)^2\n\n\n[1] 0.6517747\n\n\nThis value appears in as the \\(R^2\\) of lm_q5a’s output.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nnear(summary(lm_q5a)$r.squared, cor(fitted(lm_q5a), cheddar$taste)^2)\n\n\n[1] TRUE\n\n\n\n\n\n\n\nCode\nlm_q5c &lt;- lm(taste ~ Acetic + H2S + Lactic - 1, data = cheddar)\nround(summary(lm_q5c)$r.squared, 3)\n\n\n[1] 0.888\n\n\nWithout the intercept term, \\(R^2 = 0.888\\).\nWhen compared to the correlation between the fitted values and the response squared, the latter is a much more plausible value.\n####q5d\n\n\nCode\nX &lt;- model.matrix(\n  ~ Acetic + H2S + Lactic,\n  data = cheddar\n)\ny &lt;- cheddar$taste\nqrx &lt;- qr(X)\nf = t(qr.Q(qrx)) %*% y\nbacksolve(qr.R(qrx), f)\n\n\n            [,1]\n[1,] -28.8767696\n[2,]   0.3277413\n[3,]   3.9118411\n[4,]  19.6705434\n\n\n\n\n\n\n\n\n\n\nCode\ndata(wafer, package = \"faraway\")\nhead(wafer)\n\n\n  x1 x2 x3 x4 resist\n1  -  -  -  -  193.4\n2  +  -  -  -  247.6\n3  -  +  -  -  168.2\n4  +  +  -  -  205.0\n5  -  -  +  -  303.4\n6  +  -  +  -  339.9\n\n\n\n\nCode\nlm_q6a &lt;- lm(resist ~ x1 + x2 + x3 + x4, data = wafer)\nsummary(lm_q6a, cor = T)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3 + x4, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.381 -17.119   4.825  16.644  33.769 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   236.78      14.77  16.032 5.65e-09 ***\nx1+            25.76      13.21   1.950 0.077085 .  \nx2+           -69.89      13.21  -5.291 0.000256 ***\nx3+            43.59      13.21   3.300 0.007083 ** \nx4+           -14.49      13.21  -1.097 0.296193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.42 on 11 degrees of freedom\nMultiple R-squared:  0.7996,    Adjusted R-squared:  0.7267 \nF-statistic: 10.97 on 4 and 11 DF,  p-value: 0.0007815\n\nCorrelation of Coefficients:\n    (Intercept) x1+   x2+   x3+  \nx1+ -0.45                        \nx2+ -0.45        0.00            \nx3+ -0.45        0.00  0.00      \nx4+ -0.45        0.00  0.00  0.00\n\n\n\n\nCode\nX &lt;- model.matrix(lm_q6a)\nhead(X)\n\n\n  (Intercept) x1+ x2+ x3+ x4+\n1           1   0   0   0   0\n2           1   1   0   0   0\n3           1   0   1   0   0\n4           1   1   1   0   0\n5           1   0   0   1   0\n6           1   1   0   1   0\n\n\nThe function automatically coded - and + as \\(0\\) and \\(1\\) respectively.\n\n\n\n\n\nCode\ncor(X)\n\n\nWarning in cor(X): the standard deviation is zero\n\n\n            (Intercept) x1+ x2+ x3+ x4+\n(Intercept)           1  NA  NA  NA  NA\nx1+                  NA   1   0   0   0\nx2+                  NA   0   1   0   0\nx3+                  NA   0   0   1   0\nx4+                  NA   0   0   0   1\n\n\nThe missing values are due to the intercept being constant across all observations. A constant variable has a standard deviation of zero. Because the correlation coefficient formula requires dividing by the product of the two variables’ standard deviations, the zero value in the denominator makes the correlation mathematically undefined.\nIn other words:\n\\[\\bar{x} = \\frac{1 + 1 + 1 + \\ldots}{n} = 1\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum{(x_i - \\bar{x})^2}}{n - 1}}\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum{(1 - 1)^2}}{n - 1}} = \\sqrt{\\frac{\\sum{(0)^2}}{n - 1}} = 0\\]\nGiven the formula:\n\\[\\text{Correlation} = \\frac{cov(x, y)}{\\sigma_x \\sigma_y}\\]\nwe can see that when the denominator is \\(0\\), it will leads to a division by zero error.\n\n\n\n\n\nCode\nround(summary(lm_q6a)$coefficients[2], 2)\n\n\n[1] 25.76\n\n\nAn increase in 25.76.\n\n\n\n\n\nCode\nlm_q6d &lt;- lm(resist ~ x1 + x2 + x3, data = wafer)\nsummary(lm_q6d)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.137 -20.550   3.575  18.463  41.012 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   229.54      13.32  17.231 7.88e-10 ***\nx1+            25.76      13.32   1.934 0.077047 .  \nx2+           -69.89      13.32  -5.246 0.000206 ***\nx3+            43.59      13.32   3.272 0.006677 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.64 on 12 degrees of freedom\nMultiple R-squared:  0.7777,    Adjusted R-squared:  0.7221 \nF-statistic: 13.99 on 3 and 12 DF,  p-value: 0.0003187\n\n\n\nEstimates for x1, x2, and x3 remain the same, but the intercept term’s estimate has slightly decreased.\nStandard errors have increased slightly for all estimands.\n\n\n\n\n\n\nCode\nsummary(lm_q6a, cor = T)\n\n\n\nCall:\nlm(formula = resist ~ x1 + x2 + x3 + x4, data = wafer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.381 -17.119   4.825  16.644  33.769 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   236.78      14.77  16.032 5.65e-09 ***\nx1+            25.76      13.21   1.950 0.077085 .  \nx2+           -69.89      13.21  -5.291 0.000256 ***\nx3+            43.59      13.21   3.300 0.007083 ** \nx4+           -14.49      13.21  -1.097 0.296193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.42 on 11 degrees of freedom\nMultiple R-squared:  0.7996,    Adjusted R-squared:  0.7267 \nF-statistic: 10.97 on 4 and 11 DF,  p-value: 0.0007815\n\nCorrelation of Coefficients:\n    (Intercept) x1+   x2+   x3+  \nx1+ -0.45                        \nx2+ -0.45        0.00            \nx3+ -0.45        0.00  0.00      \nx4+ -0.45        0.00  0.00  0.00\n\n\nThe design matrix \\(X\\) is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix. Orthogonality ensures that the estimated effect of each predictor is decoupled from the others. Therefore, the coefficient estimate for any given predictor is invariant to the inclusion or exclusion of other predictors in the model.\n\n\n\n\n\n\n\n\nCode\ndata(truck, package = \"faraway\")\nhead(truck)\n\n\n  B C D E O height\n1 - - - - -   7.78\n2 + - - + -   8.15\n3 - + - + -   7.50\n4 + + - - -   7.59\n5 - - + + -   7.94\n6 + - + - -   7.69\n\n\n\n\nCode\ntruck$B &lt;- sapply(truck$B, function(x) ifelse(x == \"-\", -1, 1))\ntruck$C &lt;- sapply(truck$C, function(x) ifelse(x == \"-\", -1, 1))\ntruck$D &lt;- sapply(truck$D, function(x) ifelse(x == \"-\", -1, 1))\ntruck$E &lt;- sapply(truck$E, function(x) ifelse(x == \"-\", -1, 1))\ntruck$O &lt;- sapply(truck$O, function(x) ifelse(x == \"-\", -1, 1))\n\n\n\n\nCode\nlm_q7a &lt;- lm(height ~ B + C + D + E + O, data = truck)\nsummary(lm_q7a, cor = T)\n\n\n\nCall:\nlm(formula = height ~ B + C + D + E + O, data = truck)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.33125 -0.09427  0.01625  0.11917  0.25875 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.63604    0.02291 333.316  &lt; 2e-16 ***\nB            0.11062    0.02291   4.829 1.85e-05 ***\nC           -0.08813    0.02291  -3.847   0.0004 ***\nD           -0.01437    0.02291  -0.627   0.5337    \nE            0.05187    0.02291   2.264   0.0288 *  \nO           -0.12979    0.02291  -5.665 1.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1587 on 42 degrees of freedom\nMultiple R-squared:  0.6433,    Adjusted R-squared:  0.6008 \nF-statistic: 15.15 on 5 and 42 DF,  p-value: 1.681e-08\n\nCorrelation of Coefficients:\n  (Intercept) B    C    D    E   \nB 0.00                           \nC 0.00        0.00               \nD 0.00        0.00 0.00          \nE 0.00        0.00 0.00 0.00     \nO 0.00        0.00 0.00 0.00 0.00\n\n\nCode\ncoef(lm_q7a)\n\n\n(Intercept)           B           C           D           E           O \n  7.6360417   0.1106250  -0.0881250  -0.0143750   0.0518750  -0.1297917 \n\n\n\n\n\n\n\nCode\nlm_q7b &lt;- lm(height ~ B + C + D + E, data = truck)\nsummary(lm_q7b, cor = T)\n\n\n\nCall:\nlm(formula = height ~ B + C + D + E, data = truck)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46104 -0.12479 -0.00479  0.14396  0.34896 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.63604    0.03007 253.915  &lt; 2e-16 ***\nB            0.11062    0.03007   3.679 0.000648 ***\nC           -0.08813    0.03007  -2.930 0.005402 ** \nD           -0.01437    0.03007  -0.478 0.635071    \nE            0.05187    0.03007   1.725 0.091717 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2084 on 43 degrees of freedom\nMultiple R-squared:  0.3706,    Adjusted R-squared:  0.3121 \nF-statistic: 6.331 on 4 and 43 DF,  p-value: 0.0004258\n\nCorrelation of Coefficients:\n  (Intercept) B    C    D   \nB 0.00                      \nC 0.00        0.00          \nD 0.00        0.00 0.00     \nE 0.00        0.00 0.00 0.00\n\n\nCode\ncoef(lm_q7b)\n\n\n(Intercept)           B           C           D           E \n   7.636042    0.110625   -0.088125   -0.014375    0.051875 \n\n\nThe coefficients remains unchanged.\nExamining the \\(X\\) matrix:\n\n\nCode\ncor(model.matrix(lm_q7a))\n\n\nWarning in cor(model.matrix(lm_q7a)): the standard deviation is zero\n\n\n            (Intercept)  B  C  D  E  O\n(Intercept)           1 NA NA NA NA NA\nB                    NA  1  0  0  0  0\nC                    NA  0  1  0  0  0\nD                    NA  0  0  1  0  0\nE                    NA  0  0  0  1  0\nO                    NA  0  0  0  0  1\n\n\nThe design matrix \\(X\\) is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix\n\n\n\n\n\nCode\ntruck_transformed &lt;- transform(truck, A = B + C + D + E)\nhead(truck_transformed)\n\n\n   B  C  D  E  O height  A\n1 -1 -1 -1 -1 -1   7.78 -4\n2  1 -1 -1  1 -1   8.15  0\n3 -1  1 -1  1 -1   7.50  0\n4  1  1 -1 -1 -1   7.59  0\n5 -1 -1  1  1 -1   7.94  0\n6  1 -1  1 -1 -1   7.69  0\n\n\nCode\nlm_q7c &lt;- lm(height ~ A + B + C + D + E + O, data = truck_transformed)\ncoef(lm_q7c)\n\n\n(Intercept)           A           B           C           D           E \n  7.6360417   0.0518750   0.0587500  -0.1400000  -0.0662500          NA \n          O \n -0.1297917 \n\n\nE is not estimable; A = B + C + D + E makes the predictors collinear, so R excluded E due to identifiability issues (from pg31: Predictors occurring later in the model formula are preferred for removal …)\n\n\n\n\n\nCode\nX &lt;- model.matrix(lm_q7c)\nX\n\n\n   (Intercept)  A  B  C  D  E  O\n1            1 -4 -1 -1 -1 -1 -1\n2            1  0  1 -1 -1  1 -1\n3            1  0 -1  1 -1  1 -1\n4            1  0  1  1 -1 -1 -1\n5            1  0 -1 -1  1  1 -1\n6            1  0  1 -1  1 -1 -1\n7            1  0 -1  1  1 -1 -1\n8            1  4  1  1  1  1 -1\n9            1 -4 -1 -1 -1 -1  1\n10           1  0  1 -1 -1  1  1\n11           1  0 -1  1 -1  1  1\n12           1  0  1  1 -1 -1  1\n13           1  0 -1 -1  1  1  1\n14           1  0  1 -1  1 -1  1\n15           1  0 -1  1  1 -1  1\n16           1  4  1  1  1  1  1\n17           1 -4 -1 -1 -1 -1 -1\n18           1  0  1 -1 -1  1 -1\n19           1  0 -1  1 -1  1 -1\n20           1  0  1  1 -1 -1 -1\n21           1  0 -1 -1  1  1 -1\n22           1  0  1 -1  1 -1 -1\n23           1  0 -1  1  1 -1 -1\n24           1  4  1  1  1  1 -1\n25           1 -4 -1 -1 -1 -1  1\n26           1  0  1 -1 -1  1  1\n27           1  0 -1  1 -1  1  1\n28           1  0  1  1 -1 -1  1\n29           1  0 -1 -1  1  1  1\n30           1  0  1 -1  1 -1  1\n31           1  0 -1  1  1 -1  1\n32           1  4  1  1  1  1  1\n33           1 -4 -1 -1 -1 -1 -1\n34           1  0  1 -1 -1  1 -1\n35           1  0 -1  1 -1  1 -1\n36           1  0  1  1 -1 -1 -1\n37           1  0 -1 -1  1  1 -1\n38           1  0  1 -1  1 -1 -1\n39           1  0 -1  1  1 -1 -1\n40           1  4  1  1  1  1 -1\n41           1 -4 -1 -1 -1 -1  1\n42           1  0  1 -1 -1  1  1\n43           1  0 -1  1 -1  1  1\n44           1  0  1  1 -1 -1  1\n45           1  0 -1 -1  1  1  1\n46           1  0  1 -1  1 -1  1\n47           1  0 -1  1  1 -1  1\n48           1  4  1  1  1  1  1\nattr(,\"assign\")\n[1] 0 1 2 3 4 5 6\n\n\nCode\ny &lt;- truck$height\ny\n\n\n [1] 7.78 8.15 7.50 7.59 7.94 7.69 7.56 7.56 7.50 7.88 7.50 7.63 7.32 7.56 7.18\n[16] 7.81 7.78 8.18 7.56 7.56 8.00 8.09 7.62 7.81 7.25 7.88 7.56 7.75 7.44 7.69\n[31] 7.18 7.50 7.81 7.88 7.50 7.75 7.88 8.06 7.44 7.69 7.12 7.44 7.50 7.56 7.44\n[46] 7.62 7.25 7.59\n\n\n\n\nCode\ntry(solve(t(X) %*% X) %*% t(X) %*% y)\n\n\nError in solve.default(t(X) %*% X) : \n  Lapack routine dgesv: system is exactly singular: U[6,6] = 0\n\n\nThe model cannot be estimated due to linear dependence among the predictors, which makes the design matrix rank-deficient.\n\n\n\n\n\nCode\nqrx &lt;- qr(X)\ndim(qr.Q(qrx))\n\n\n[1] 48  7\n\n\nCode\nf &lt;- t(qr.Q(qrx)) %*% y\nbacksolve(qr.R(qrx), f)\n\n\n              [,1]\n[1,]  7.636042e+00\n[2,] -1.853252e+13\n[3,]  1.853252e+13\n[4,]  1.853252e+13\n[5,]  1.853252e+13\n[6,] -1.267479e-01\n[7,]  1.853252e+13\n\n\n\n\n\nWe will obtain the same results as coef(lm_q7c) running\n\n\nCode\nqr.coef(qrx, truck$height)\n\n\n(Intercept)           A           B           C           D           E \n  7.6360417   0.0518750   0.0587500  -0.1400000  -0.0662500          NA \n          O \n -0.1297917 \n\n\n\n\n\n\n\n\n\\[\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 0\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\n\n\nGiven the matrix \\(X\\) and vector \\(y\\):\n\\[\nX = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n\\]\nWe want to find the Least Squares estimator: \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\).\n\\[\nX^T = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\]\n\\[\nX^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 3 \\\\\n3 & 3\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} = \\frac{1}{9} \\begin{pmatrix}\n3 & -3 \\\\\n-3 & 6\n\\end{pmatrix} = \\begin{pmatrix}\n3/9 & -3/9 \\\\\n-3/9 & 6/9\n\\end{pmatrix} = \\begin{pmatrix}\n1/3 & -1/3 \\\\\n-1/3 & 2/3\n\\end{pmatrix}\n\\]\n\\[\nX^T y = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\\\\ny_4 + y_5 + y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} (X^T y) =\n\\hat{\\beta}\n= \\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1/3 & -1/3 \\\\\n-1/3 & 2/3\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6\\\\\ny_4 + y_5 + y_6\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{y_1 + y_2 + y_3}{3}\\\\\n\\frac{y_4 + y_5 + y_6}{3} - \\frac{y_1 + y_2 + y_3}{3}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{3} \\sum_{i = 1}^3 y_i\\\\\n\\frac{1}{3} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\end{pmatrix}\n\\]\n\\[\n\\hat{\\beta}_0 = \\frac{y_1 + y_2 + y_3}{3} = \\frac{1}{3} \\sum_{i = 1}^3 y_i\n\\]\nNotice that \\(\\hat{\\beta}_0\\) is the mean of all the observations from the first group. (Think of it as the reference group in linear regression)\n\\[\n\\hat{\\beta}_1 = \\frac{1}{3} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\]\nNotice that \\(\\hat{\\beta}_1\\) is the difference between the means of both groups.\n\n\n\n\\[\n\\begin{pmatrix}\n1 & -1\\\\\n1 & -1\\\\\n1 & -1\\\\\n1 & 1\\\\\n1 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\n\n\nGiven the matrix \\(X\\) and vector \\(y\\) (where Group 1 is -1 and Group 2 is +1):\n\\[\nX = \\begin{pmatrix}\n1 & -1 \\\\\n1 & -1 \\\\\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n\\]\nWe want to find the Least Squares estimator: \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\).\n\\[\nX^T = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\]\n\\[\nX^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & -1 \\\\\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 0 \\\\\n0 & 6\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} = \\begin{pmatrix}\n1/6 & 0 \\\\\n0 & 1/6\n\\end{pmatrix}\n\\]\n\\[\nX^T y = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\\\\n-y_1 - y_2 - y_3 + y_4 + y_5 + y_6\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i - \\sum_{i=1}^3 y_i\n\\end{pmatrix}\n\\]\n\\[\n(X^T X)^{-1} (X^T y) =\n\\hat{\\beta}\n= \\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1/6 & 0 \\\\\n0 & 1/6\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{i=1}^6 y_i \\\\\n\\sum_{i=4}^6 y_i - \\sum_{i=1}^3 y_i\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{6} \\sum_{i = 1}^6 y_i \\\\\n\\frac{1}{6} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\end{pmatrix}\n\\]\n\\[\n\\hat{\\beta}_0 = \\frac{1}{6} \\sum_{i = 1}^6 y_i\n\\]\nNotice that \\(\\hat{\\beta}_0\\) is now the grand mean (the average of all observations combined). All because -1 and 1 are centred around 0, and the intercept represents the centre of the data.\n\\[\n\\hat{\\beta}_1 = \\frac{1}{6} ( \\sum_{i = 4}^6 y_i - \\sum_{i = 1}^3 y_i )\n\\]\nNotice that \\(\\hat{\\beta}_1\\) is half the difference between the means of the two groups. (Since the difference in the x-axis “run” is now 2 units (from -1 to 1), the slope is halved compared to the 0/1 coding).\n\nThe first coding computes the mean of one group (\\(\\hat{\\beta}_0\\)) and then the difference in their means (\\(\\hat{\\beta}_1\\)).\nThe second computes an overall mean (\\(\\hat{\\beta}_0\\)) while the second parameter represents the difference from that overall mean (\\(\\hat{\\beta}_1\\)).\n\nThe second coding is more symmetrical (but both result in the same fitted values and residuals)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\n\n\n\nCode\ndata(teengamb, package = \"faraway\")\n\n\n\n\nCode\nhead(teengamb)\n\n\n  sex status income verbal gamble\n1   1     51   2.00      8    0.0\n2   1     28   2.50      8    0.0\n3   1     37   2.00      6    0.0\n4   1     28   7.00      4    7.3\n5   1     65   2.00      8   19.6\n6   1     61   3.47      6    0.1\n\n\n\n\nCode\nteengamb$sex = factor(teengamb$sex)\nlevels(teengamb$sex) = c(\"male\", \"female\")\nsummary(teengamb$sex)\n\n\n  male female \n    28     19 \n\n\nThere are 28 males and 19 females.\n\n\n\n\n\nCode\n# Base graphics\nboxplot(\n  status ~ sex,\n  data = teengamb,\n  main = \"Boxplot of Sexes\",\n  xlab = \"Sex\",\n  ylab = \"Count\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = sex, y = status)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nBoth methods differ only in appearance.\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = sex, y = status)) + geom_point()\n\n\n\n\n\n\n\n\n\nNo. We see fewer points due to overplotting.\n\n\n\n\n\nCode\nhist(x = teengamb$verbal, xlab = \"Verbal Score\")\n\n\n\n\n\n\n\n\n\nThe x-axis is difficult to interpret because the ticks marks align with the boundaries of the blocks, rather than being centred on them.\n\n\nCode\nplot(table(teengamb$verbal), xlab = \"Verbal Score\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nCleaner but the thin lines are non-standard way of visualising histogram.\n\n\nCode\nbarplot(table(teengamb$verbal), xlab = \"Verbal Score\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThe plot does not leave a gap for the verbal score of 3, even though this value is missing from the dataset.\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = income, y = gamble, colour = sex)) +\n  geom_point(size = 5, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = teengamb,\n  aes(x = income, y = gamble, shape = sex, colour = sex)\n) +\n  geom_point(size = 4, alpha = 0.3) +\n  facet_grid(~sex)\n\n\n\n\n\n\n\n\n\nThe faceted plot, because it is easier to distinguish the two sexes.\n\n\n\n\n\nCode\nsummary(teengamb)\n\n\n     sex         status          income           verbal          gamble     \n male  :28   Min.   :18.00   Min.   : 0.600   Min.   : 1.00   Min.   :  0.0  \n female:19   1st Qu.:28.00   1st Qu.: 2.000   1st Qu.: 6.00   1st Qu.:  1.1  \n             Median :43.00   Median : 3.250   Median : 7.00   Median :  6.0  \n             Mean   :45.23   Mean   : 4.642   Mean   : 6.66   Mean   : 19.3  \n             3rd Qu.:61.50   3rd Qu.: 6.210   3rd Qu.: 8.00   3rd Qu.: 19.4  \n             Max.   :75.00   Max.   :15.000   Max.   :10.00   Max.   :156.0  \n\n\nThe gamble variable is the most skewed, because its minimum and lower quartile are much closer to the median than its upper quartile and maximum are.\n\n\n\n\n\n\n\n\nCode\ndata(uswages, package = \"faraway\")\nhead(uswages)\n\n\n        wage educ exper race smsa ne mw so we pt\n6085  771.60   18    18    0    1  1  0  0  0  0\n23701 617.28   15    20    0    1  0  0  0  1  0\n16208 957.83   16     9    0    1  0  0  1  0  0\n2720  617.28   12    24    0    1  1  0  0  0  0\n9723  902.18   14    12    0    1  0  1  0  0  0\n22239 299.15   12    33    0    1  0  0  0  1  0\n\n\nCode\nusw = uswages[, c(\"wage\", \"ne\", \"mw\", \"we\", \"so\")]\nhead(usw)\n\n\n        wage ne mw we so\n6085  771.60  1  0  0  0\n23701 617.28  0  0  1  0\n16208 957.83  0  0  0  1\n2720  617.28  1  0  0  0\n9723  902.18  0  1  0  0\n22239 299.15  0  0  1  0\n\n\n\n\n\n\n\nCode\nsum(usw$ne * usw$wage) / sum(usw$ne)\n\n\n[1] 631.6591\n\n\n\n\n\n\n\nCode\ntapply(usw$wage, usw$ne, mean)[\"1\"]\n\n\n       1 \n631.6591 \n\n\n\n\nCode\nall.equal(\n  sum(usw$ne * usw$wage) / sum(usw$ne),\n  unname(tapply(usw$wage, usw$ne, mean)[\"1\"])\n)\n\n\n[1] TRUE\n\n\nYes it does.\n\n\n\n\n\nCode\nhead(rowSums(usw[, -1]))\n\n\n 6085 23701 16208  2720  9723 22239 \n    1     1     1     1     1     1 \n\n\nEach row should have exactly one 1 because these columns are indicators for mutually exclusive geographic regions (one region per individual).\n\n\n\n\n\nCode\nusw$area = c(\"ne\", \"mw\", \"we\", \"so\")[apply(usw[, -1], 1, which.max)]\nhead(usw)\n\n\n        wage ne mw we so area\n6085  771.60  1  0  0  0   ne\n23701 617.28  0  0  1  0   we\n16208 957.83  0  0  0  1   so\n2720  617.28  1  0  0  0   ne\n9723  902.18  0  1  0  0   mw\n22239 299.15  0  0  1  0   we\n\n\n\n\n\n\n\nCode\nboxplot(formula = wage ~ area, data = usw)\n\n\n\n\n\n\n\n\n\nThe distributions are highly skewed.\n\n\n\n\n\nCode\nboxplot(log(wage) ~ area, data = usw)\n\n\n\n\n\n\n\n\n\nThis is better because it is now easier to distinguish differences in the distributions.\n\n\n\n\n\n\n\n\nCode\ndata(prostate, package = \"faraway\")\nhead(prostate)\n\n\n      lcavol lweight age      lbph svi      lcp gleason pgg45     lpsa\n1 -0.5798185  2.7695  50 -1.386294   0 -1.38629       6     0 -0.43078\n2 -0.9942523  3.3196  58 -1.386294   0 -1.38629       6     0 -0.16252\n3 -0.5108256  2.6912  74 -1.386294   0 -1.38629       7    20 -0.16252\n4 -1.2039728  3.2828  58 -1.386294   0 -1.38629       6     0 -0.16252\n5  0.7514161  3.4324  62 -1.386294   0 -1.38629       6     0  0.37156\n6 -1.0498221  3.2288  50 -1.386294   0 -1.38629       6     0  0.76547\n\n\n\n\nCode\npairs(prostate[, 1:4])\n\n\n\n\n\n\n\n\n\nlbph has many identical values.\n\n\n\n\n\nCode\ncor(prostate[, 1:4])\n\n\n            lcavol   lweight       age       lbph\nlcavol  1.00000000 0.1941284 0.2249999 0.02734971\nlweight 0.19412839 1.0000000 0.3075247 0.43493174\nage     0.22499988 0.3075247 1.0000000 0.35018592\nlbph    0.02734971 0.4349317 0.3501859 1.00000000\n\n\nThere are four assumptions to check before performing a Pearson correlation test. - The two variables (the variables of interest) need to be using a continuous scale. - The two variables of interest should have a linear relationship, which you can check with a scatterplot. - There should be no spurious outliers. - The variables should be normally or near-to-normally distributed.\nGiven the non-normal distribution of lbph, its Pearson correlation coefficients are influenced by it, which can misrepresent the overall strength of the relationships to other variables.\n\n\n\n\n\nCode\nnrow(prostate[prostate$lbph == min(prostate$lbph), ]) / nrow(prostate)\n\n\n[1] 0.443299\n\n\n44%.\n\n\nCode\nexp(min(prostate$lbph))\n\n\n[1] 0.2500001\n\n\nThe transformation \\(log(x + 0.25)\\) was employed to manage zero values in the bph variable. A constant of 0.25 was added to the raw data to prevent the mathematical singularity of \\(log(0)\\) as the data contained numerous instances where bph equal zero.\n\n\n\n\n\nCode\nage_range &lt;- range(prostate$age)\none_year_breaks &lt;- seq(\n  from = age_range[1],\n  to = age_range[2] + 1,\n  by = 1\n)\npar(mfrow = c(1, 2))\n\nhist(x = prostate$age, xlab = \"Age\")\n\nhist(x = prostate$age, xlab = \"Age\", breaks = one_year_breaks)\n\n\n\n\n\n\n\n\n\n\nDefault Plot: This plot provides a smoother, more general overview of the age distribution. It’s easy to see the central tendency and the overall shape. This plot is good for quickly understanding the general pattern.\nOne-Year Bin Plot: This plot is much more jagged, noisy, and granular. The overall shape is still visible but is obscured by the high degree of variation between individual years. It looks less like a smooth distribution and more like a collection of sharp spikes.\n\nA larger binwidth is chosen to emphasise the overall structure of the age distribution. This approach is appropriate as we are not concerned with the fine-grained detail in the data.\n\n\n\n\n\nCode\nxtabs(~ gleason + svi, data = prostate)\n\n\n       svi\ngleason  0  1\n      6 35  0\n      7 37 19\n      8  1  0\n      9  3  2\n\n\nThe most common combination is a Gleason score of 7 with no seminal vesicle invasion, which occurred in 37 of the 97 patients in the study.\n\n\n\n\n\n\n\n\nCode\ndata(sat, package = \"faraway\")\nhead(sat)\n\n\n           expend ratio salary takers verbal math total\nAlabama     4.405  17.2 31.144      8    491  538  1029\nAlaska      8.963  17.6 47.951     47    445  489   934\nArizona     4.778  19.3 32.175     27    448  496   944\nArkansas    4.459  17.1 28.934      6    482  523  1005\nCalifornia  4.992  24.0 41.078     45    417  485   902\nColorado    5.443  18.4 34.571     29    462  518   980\n\n\n\n\nCode\ntable(sat$verbal + sat$math == sat$total)\n\n\n\nTRUE \n  50 \n\n\n\n\n\n\n\nCode\nplot(\n  x = sat$verbal,\n  y = sat$math,\n  main = \"Math Against Verbal Scores\",\n  xlab = \"Verbal\",\n  ylab = \"Math\"\n)\nabline(0, 1) # y = x is equal to intercept = 0 and slope = 1\n\n\n\n\n\n\n\n\n\nThe distributions differ; students generally score higher on their maths tests than on their verbal tests.\nThere is a strong, positive, linear relationship between verbal and math scores. As students’ verbal scores increase, their math scores also tend to increase.\n\n\nCode\n# hist(x = sat$verbal)\n# plot(density(x = sat$verbal))\n\n\n\n\n\n\n\nCode\nplot(scale(sat$math) ~ scale(sat$verbal), xlab = \"Verbal\", ylab = \"Math\")\nabline(a = 0, b = 1, lty = 2, lwd = 2, col = \"blue\")\ntext(\n  x = -1.0,\n  y = 1.5,\n  label = paste0(\"Correlation: \", round(cor(sat$verbal, sat$math), 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nq4d_lm_coef &lt;- coef(lm(sat$math ~ sat$verbal))\nplot(math ~ verbal, data = sat)\nabline(lm(sat$math ~ sat$verbal), col = \"red\", lty = 2, lwd = 2)\ntext(\n  x = -0.5,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4d_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -0.5,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4d_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nq4e_lm_coef &lt;- coef(lm(sat$verbal ~ sat$math))\nplot(math ~ verbal, data = sat)\nabline(lm(sat$verbal ~ sat$math), col = \"green\", lty = 2)\ntext(\n  x = -0.5,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4e_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -0.5,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4e_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nRemake the plot for clearer visual.\n\nThe model gives us: \\(\\text{verbal} = \\text{intercept} + \\text{slope} * \\text{math}\\)\nRearrange it to the equation below so we can plot it on our graph\n\\(\\text{verbal} - \\text{intercept} = \\text{slope} * \\text{math}\\)\n\\(\\text{math} = \\text{(verbal / slope)} - \\text{(intercept / slope)}\\)\n\\(\\text{math} = \\text{(-intercept / slope)} + (1 / \\text{slope}) * \\text{verbal}\\)\nSo, the new intercept is \\(-\\frac{\\text{intercept}}{\\text{slope}}\\) and the new slope is \\(\\frac{1}{\\text{slope}}\\)\n\n\n\nCode\nmean_verbal &lt;- mean(sat$verbal)\nmean_math &lt;- mean(sat$math)\n\n\nplot(\n  math ~ verbal,\n  data = sat,\n  main = \"Intersection of Two Regression Lines\",\n  xlab = \"Verbal Score\",\n  ylab = \"Math Score\",\n  pch = 19,\n  col = \"gray\"\n)\n\n\nlm_math_on_verbal &lt;- lm(math ~ verbal, data = sat)\nabline(lm_math_on_verbal, col = \"red\", lwd = 2)\n\n\nlm_verbal_on_math &lt;- lm(verbal ~ math, data = sat)\n\n\ncoeffs &lt;- coef(lm_verbal_on_math)\nintercept_for_plot &lt;- -coeffs[1] / coeffs[2]\nslope_for_plot &lt;- 1 / coeffs[2]\n\n# Now we can add this rearranged line to our plot\nabline(a = intercept_for_plot, b = slope_for_plot, col = \"blue\", lwd = 2)\n\n\n# This point should be exactly where the two lines cross\npoints(x = mean_verbal, y = mean_math, col = \"purple\", pch = 19, cex = 2.5)\n\ntext(\n  x = -1.0,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4d_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -1.0,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4d_lm_coef[2], 3)),\n  cex = 1\n)\n\ntext(\n  x = 0.8,\n  y = -0.75,\n  label = paste0(\"Intercept [Exchanged]: \", round(q4e_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = 0.8,\n  y = -0.5,\n  label = paste0(\"Slope [Exchanged]: \", round(q4e_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the equation \\(\\text{math} = 0.97 \\cdot \\text{verbal} + 3.41 \\times 10^{-16}\\)\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal) + 20) + q4d_lm_coef[1])\n\n\nsat$verbal \n  530.9593 \n\n\n\n\n\nBased on the equation \\(\\text{verbal} = 0.97 \\cdot \\text{math} - 3.08 \\times 10^{-16}\\)\n\n\nCode\nprint(q4e_lm_coef[2] * (mean(sat$math) + 20) + q4e_lm_coef[1])\n\n\nsat$math \n474.1179 \n\n\n\n\n\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal)) + q4d_lm_coef[1])\n\n\nsat$verbal \n    508.78 \n\n\n\n\n\nWithout any information about the verbal score, we can only use the mean verbal score.\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal)) + q4d_lm_coef[1])\n\n\nsat$verbal \n    508.78 \n\n\n\n\n\n\n\n\n\n\nCode\ndata(divusa, package = \"faraway\")\nhead(divusa)\n\n\n  year divorce unemployed femlab marriage birth military\n1 1920     8.0        5.2  22.70     92.0 117.9   3.2247\n2 1921     7.2       11.7  22.79     83.0 119.8   3.5614\n3 1922     6.6        6.7  22.88     79.7 111.2   2.4553\n4 1923     7.1        2.4  22.97     85.2 110.5   2.2065\n5 1924     7.2        5.0  23.06     80.3 110.9   2.2889\n6 1925     7.2        3.2  23.15     79.2 106.6   2.1735\n\n\n\n\nCode\nplot(x = divusa$year, y = divusa$divorce)\n\n\n\n\n\n\n\n\n\nCode\nplot(x = divusa$year, y = divusa$divorce, type = \"l\")\n\n\n\n\n\n\n\n\n\nLine plot is preferable for time ordered data.\n\n\n\n\n\nCode\nplot(\n  y = divusa$divorce[-1],\n  x = divusa$divorce[-nrow(divusa)],\n  xlab = \"Divorce rate in previous year\",\n  ylab = \"Divorce rate in current year\"\n)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nPrediction is possible; divorce rate in successive years is strongly correlated.\n\n\n\n\n\nCode\nq5c_coef &lt;- coef(lm(divorce ~ year, data = divusa))\nq5c_coef\n\n\n (Intercept)         year \n-422.9752984    0.2228009 \n\n\nBased on the equation \\(\\text{divorce} = 0.22 \\cdot \\text{year} - 422.98\\).\n\n\nCode\n(100 - q5c_coef[1]) / q5c_coef[2]\n\n\n(Intercept) \n   2347.277 \n\n\nAfter rearranging the equation, the year when divorce rate hits \\(100\\%\\) is \\(2347.277\\). It is not a realistic prediction, as this extrapolation over time is sure to stop at some point.\n\n\n\n\n\nCode\nggplot(data = divusa, aes(x = femlab, y = divorce, colour = year)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nHistorically, divorce rates were lower during periods when female participation in the labor force was also lower.\nAlso, both variables start low in the distant past as indicated by the darker coloured points and progress to higher values over time as indicated by the lighther coloured points.\nBut the change is not linear, especially during the Great Depression which caused a shift in the divorce rate."
  },
  {
    "objectID": "chapter1.html#exercises",
    "href": "chapter1.html#exercises",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\n\n\n\nCode\ndata(teengamb, package = \"faraway\")\n\n\n\n\nCode\nhead(teengamb)\n\n\n  sex status income verbal gamble\n1   1     51   2.00      8    0.0\n2   1     28   2.50      8    0.0\n3   1     37   2.00      6    0.0\n4   1     28   7.00      4    7.3\n5   1     65   2.00      8   19.6\n6   1     61   3.47      6    0.1\n\n\n\n\nCode\nteengamb$sex = factor(teengamb$sex)\nlevels(teengamb$sex) = c(\"male\", \"female\")\nsummary(teengamb$sex)\n\n\n  male female \n    28     19 \n\n\nThere are 28 males and 19 females.\n\n\n\n\n\nCode\n# Base graphics\nboxplot(\n  status ~ sex,\n  data = teengamb,\n  main = \"Boxplot of Sexes\",\n  xlab = \"Sex\",\n  ylab = \"Count\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = sex, y = status)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nBoth methods differ only in appearance.\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = sex, y = status)) + geom_point()\n\n\n\n\n\n\n\n\n\nNo. We see fewer points due to overplotting.\n\n\n\n\n\nCode\nhist(x = teengamb$verbal, xlab = \"Verbal Score\")\n\n\n\n\n\n\n\n\n\nThe x-axis is difficult to interpret because the ticks marks align with the boundaries of the blocks, rather than being centred on them.\n\n\nCode\nplot(table(teengamb$verbal), xlab = \"Verbal Score\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nCleaner but the thin lines are non-standard way of visualising histogram.\n\n\nCode\nbarplot(table(teengamb$verbal), xlab = \"Verbal Score\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThe plot does not leave a gap for the verbal score of 3, even though this value is missing from the dataset.\n\n\n\n\n\nCode\nggplot(data = teengamb, aes(x = income, y = gamble, colour = sex)) +\n  geom_point(size = 5, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n  data = teengamb,\n  aes(x = income, y = gamble, shape = sex, colour = sex)\n) +\n  geom_point(size = 4, alpha = 0.3) +\n  facet_grid(~sex)\n\n\n\n\n\n\n\n\n\nThe faceted plot, because it is easier to distinguish the two sexes.\n\n\n\n\n\nCode\nsummary(teengamb)\n\n\n     sex         status          income           verbal          gamble     \n male  :28   Min.   :18.00   Min.   : 0.600   Min.   : 1.00   Min.   :  0.0  \n female:19   1st Qu.:28.00   1st Qu.: 2.000   1st Qu.: 6.00   1st Qu.:  1.1  \n             Median :43.00   Median : 3.250   Median : 7.00   Median :  6.0  \n             Mean   :45.23   Mean   : 4.642   Mean   : 6.66   Mean   : 19.3  \n             3rd Qu.:61.50   3rd Qu.: 6.210   3rd Qu.: 8.00   3rd Qu.: 19.4  \n             Max.   :75.00   Max.   :15.000   Max.   :10.00   Max.   :156.0  \n\n\nThe gamble variable is the most skewed, because its minimum and lower quartile are much closer to the median than its upper quartile and maximum are.\n\n\n\n\n\n\n\n\nCode\ndata(uswages, package = \"faraway\")\nhead(uswages)\n\n\n        wage educ exper race smsa ne mw so we pt\n6085  771.60   18    18    0    1  1  0  0  0  0\n23701 617.28   15    20    0    1  0  0  0  1  0\n16208 957.83   16     9    0    1  0  0  1  0  0\n2720  617.28   12    24    0    1  1  0  0  0  0\n9723  902.18   14    12    0    1  0  1  0  0  0\n22239 299.15   12    33    0    1  0  0  0  1  0\n\n\nCode\nusw = uswages[, c(\"wage\", \"ne\", \"mw\", \"we\", \"so\")]\nhead(usw)\n\n\n        wage ne mw we so\n6085  771.60  1  0  0  0\n23701 617.28  0  0  1  0\n16208 957.83  0  0  0  1\n2720  617.28  1  0  0  0\n9723  902.18  0  1  0  0\n22239 299.15  0  0  1  0\n\n\n\n\n\n\n\nCode\nsum(usw$ne * usw$wage) / sum(usw$ne)\n\n\n[1] 631.6591\n\n\n\n\n\n\n\nCode\ntapply(usw$wage, usw$ne, mean)[\"1\"]\n\n\n       1 \n631.6591 \n\n\n\n\nCode\nall.equal(\n  sum(usw$ne * usw$wage) / sum(usw$ne),\n  unname(tapply(usw$wage, usw$ne, mean)[\"1\"])\n)\n\n\n[1] TRUE\n\n\nYes it does.\n\n\n\n\n\nCode\nhead(rowSums(usw[, -1]))\n\n\n 6085 23701 16208  2720  9723 22239 \n    1     1     1     1     1     1 \n\n\nEach row should have exactly one 1 because these columns are indicators for mutually exclusive geographic regions (one region per individual).\n\n\n\n\n\nCode\nusw$area = c(\"ne\", \"mw\", \"we\", \"so\")[apply(usw[, -1], 1, which.max)]\nhead(usw)\n\n\n        wage ne mw we so area\n6085  771.60  1  0  0  0   ne\n23701 617.28  0  0  1  0   we\n16208 957.83  0  0  0  1   so\n2720  617.28  1  0  0  0   ne\n9723  902.18  0  1  0  0   mw\n22239 299.15  0  0  1  0   we\n\n\n\n\n\n\n\nCode\nboxplot(formula = wage ~ area, data = usw)\n\n\n\n\n\n\n\n\n\nThe distributions are highly skewed.\n\n\n\n\n\nCode\nboxplot(log(wage) ~ area, data = usw)\n\n\n\n\n\n\n\n\n\nThis is better because it is now easier to distinguish differences in the distributions.\n\n\n\n\n\n\n\n\nCode\ndata(prostate, package = \"faraway\")\nhead(prostate)\n\n\n      lcavol lweight age      lbph svi      lcp gleason pgg45     lpsa\n1 -0.5798185  2.7695  50 -1.386294   0 -1.38629       6     0 -0.43078\n2 -0.9942523  3.3196  58 -1.386294   0 -1.38629       6     0 -0.16252\n3 -0.5108256  2.6912  74 -1.386294   0 -1.38629       7    20 -0.16252\n4 -1.2039728  3.2828  58 -1.386294   0 -1.38629       6     0 -0.16252\n5  0.7514161  3.4324  62 -1.386294   0 -1.38629       6     0  0.37156\n6 -1.0498221  3.2288  50 -1.386294   0 -1.38629       6     0  0.76547\n\n\n\n\nCode\npairs(prostate[, 1:4])\n\n\n\n\n\n\n\n\n\nlbph has many identical values.\n\n\n\n\n\nCode\ncor(prostate[, 1:4])\n\n\n            lcavol   lweight       age       lbph\nlcavol  1.00000000 0.1941284 0.2249999 0.02734971\nlweight 0.19412839 1.0000000 0.3075247 0.43493174\nage     0.22499988 0.3075247 1.0000000 0.35018592\nlbph    0.02734971 0.4349317 0.3501859 1.00000000\n\n\nThere are four assumptions to check before performing a Pearson correlation test. - The two variables (the variables of interest) need to be using a continuous scale. - The two variables of interest should have a linear relationship, which you can check with a scatterplot. - There should be no spurious outliers. - The variables should be normally or near-to-normally distributed.\nGiven the non-normal distribution of lbph, its Pearson correlation coefficients are influenced by it, which can misrepresent the overall strength of the relationships to other variables.\n\n\n\n\n\nCode\nnrow(prostate[prostate$lbph == min(prostate$lbph), ]) / nrow(prostate)\n\n\n[1] 0.443299\n\n\n44%.\n\n\nCode\nexp(min(prostate$lbph))\n\n\n[1] 0.2500001\n\n\nThe transformation \\(log(x + 0.25)\\) was employed to manage zero values in the bph variable. A constant of 0.25 was added to the raw data to prevent the mathematical singularity of \\(log(0)\\) as the data contained numerous instances where bph equal zero.\n\n\n\n\n\nCode\nage_range &lt;- range(prostate$age)\none_year_breaks &lt;- seq(\n  from = age_range[1],\n  to = age_range[2] + 1,\n  by = 1\n)\npar(mfrow = c(1, 2))\n\nhist(x = prostate$age, xlab = \"Age\")\n\nhist(x = prostate$age, xlab = \"Age\", breaks = one_year_breaks)\n\n\n\n\n\n\n\n\n\n\nDefault Plot: This plot provides a smoother, more general overview of the age distribution. It’s easy to see the central tendency and the overall shape. This plot is good for quickly understanding the general pattern.\nOne-Year Bin Plot: This plot is much more jagged, noisy, and granular. The overall shape is still visible but is obscured by the high degree of variation between individual years. It looks less like a smooth distribution and more like a collection of sharp spikes.\n\nA larger binwidth is chosen to emphasise the overall structure of the age distribution. This approach is appropriate as we are not concerned with the fine-grained detail in the data.\n\n\n\n\n\nCode\nxtabs(~ gleason + svi, data = prostate)\n\n\n       svi\ngleason  0  1\n      6 35  0\n      7 37 19\n      8  1  0\n      9  3  2\n\n\nThe most common combination is a Gleason score of 7 with no seminal vesicle invasion, which occurred in 37 of the 97 patients in the study.\n\n\n\n\n\n\n\n\nCode\ndata(sat, package = \"faraway\")\nhead(sat)\n\n\n           expend ratio salary takers verbal math total\nAlabama     4.405  17.2 31.144      8    491  538  1029\nAlaska      8.963  17.6 47.951     47    445  489   934\nArizona     4.778  19.3 32.175     27    448  496   944\nArkansas    4.459  17.1 28.934      6    482  523  1005\nCalifornia  4.992  24.0 41.078     45    417  485   902\nColorado    5.443  18.4 34.571     29    462  518   980\n\n\n\n\nCode\ntable(sat$verbal + sat$math == sat$total)\n\n\n\nTRUE \n  50 \n\n\n\n\n\n\n\nCode\nplot(\n  x = sat$verbal,\n  y = sat$math,\n  main = \"Math Against Verbal Scores\",\n  xlab = \"Verbal\",\n  ylab = \"Math\"\n)\nabline(0, 1) # y = x is equal to intercept = 0 and slope = 1\n\n\n\n\n\n\n\n\n\nThe distributions differ; students generally score higher on their maths tests than on their verbal tests.\nThere is a strong, positive, linear relationship between verbal and math scores. As students’ verbal scores increase, their math scores also tend to increase.\n\n\nCode\n# hist(x = sat$verbal)\n# plot(density(x = sat$verbal))\n\n\n\n\n\n\n\nCode\nplot(scale(sat$math) ~ scale(sat$verbal), xlab = \"Verbal\", ylab = \"Math\")\nabline(a = 0, b = 1, lty = 2, lwd = 2, col = \"blue\")\ntext(\n  x = -1.0,\n  y = 1.5,\n  label = paste0(\"Correlation: \", round(cor(sat$verbal, sat$math), 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nq4d_lm_coef &lt;- coef(lm(sat$math ~ sat$verbal))\nplot(math ~ verbal, data = sat)\nabline(lm(sat$math ~ sat$verbal), col = \"red\", lty = 2, lwd = 2)\ntext(\n  x = -0.5,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4d_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -0.5,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4d_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nq4e_lm_coef &lt;- coef(lm(sat$verbal ~ sat$math))\nplot(math ~ verbal, data = sat)\nabline(lm(sat$verbal ~ sat$math), col = \"green\", lty = 2)\ntext(\n  x = -0.5,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4e_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -0.5,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4e_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nRemake the plot for clearer visual.\n\nThe model gives us: \\(\\text{verbal} = \\text{intercept} + \\text{slope} * \\text{math}\\)\nRearrange it to the equation below so we can plot it on our graph\n\\(\\text{verbal} - \\text{intercept} = \\text{slope} * \\text{math}\\)\n\\(\\text{math} = \\text{(verbal / slope)} - \\text{(intercept / slope)}\\)\n\\(\\text{math} = \\text{(-intercept / slope)} + (1 / \\text{slope}) * \\text{verbal}\\)\nSo, the new intercept is \\(-\\frac{\\text{intercept}}{\\text{slope}}\\) and the new slope is \\(\\frac{1}{\\text{slope}}\\)\n\n\n\nCode\nmean_verbal &lt;- mean(sat$verbal)\nmean_math &lt;- mean(sat$math)\n\n\nplot(\n  math ~ verbal,\n  data = sat,\n  main = \"Intersection of Two Regression Lines\",\n  xlab = \"Verbal Score\",\n  ylab = \"Math Score\",\n  pch = 19,\n  col = \"gray\"\n)\n\n\nlm_math_on_verbal &lt;- lm(math ~ verbal, data = sat)\nabline(lm_math_on_verbal, col = \"red\", lwd = 2)\n\n\nlm_verbal_on_math &lt;- lm(verbal ~ math, data = sat)\n\n\ncoeffs &lt;- coef(lm_verbal_on_math)\nintercept_for_plot &lt;- -coeffs[1] / coeffs[2]\nslope_for_plot &lt;- 1 / coeffs[2]\n\n# Now we can add this rearranged line to our plot\nabline(a = intercept_for_plot, b = slope_for_plot, col = \"blue\", lwd = 2)\n\n\n# This point should be exactly where the two lines cross\npoints(x = mean_verbal, y = mean_math, col = \"purple\", pch = 19, cex = 2.5)\n\ntext(\n  x = -1.0,\n  y = 1.25,\n  label = paste0(\"Intercept: \", round(q4d_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = -1.0,\n  y = 1.0,\n  label = paste0(\"Slope: \", round(q4d_lm_coef[2], 3)),\n  cex = 1\n)\n\ntext(\n  x = 0.8,\n  y = -0.75,\n  label = paste0(\"Intercept [Exchanged]: \", round(q4e_lm_coef[1], 18)),\n  cex = 1\n)\ntext(\n  x = 0.8,\n  y = -0.5,\n  label = paste0(\"Slope [Exchanged]: \", round(q4e_lm_coef[2], 3)),\n  cex = 1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the equation \\(\\text{math} = 0.97 \\cdot \\text{verbal} + 3.41 \\times 10^{-16}\\)\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal) + 20) + q4d_lm_coef[1])\n\n\nsat$verbal \n  530.9593 \n\n\n\n\n\nBased on the equation \\(\\text{verbal} = 0.97 \\cdot \\text{math} - 3.08 \\times 10^{-16}\\)\n\n\nCode\nprint(q4e_lm_coef[2] * (mean(sat$math) + 20) + q4e_lm_coef[1])\n\n\nsat$math \n474.1179 \n\n\n\n\n\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal)) + q4d_lm_coef[1])\n\n\nsat$verbal \n    508.78 \n\n\n\n\n\nWithout any information about the verbal score, we can only use the mean verbal score.\n\n\nCode\nprint(q4d_lm_coef[2] * (mean(sat$verbal)) + q4d_lm_coef[1])\n\n\nsat$verbal \n    508.78 \n\n\n\n\n\n\n\n\n\n\nCode\ndata(divusa, package = \"faraway\")\nhead(divusa)\n\n\n  year divorce unemployed femlab marriage birth military\n1 1920     8.0        5.2  22.70     92.0 117.9   3.2247\n2 1921     7.2       11.7  22.79     83.0 119.8   3.5614\n3 1922     6.6        6.7  22.88     79.7 111.2   2.4553\n4 1923     7.1        2.4  22.97     85.2 110.5   2.2065\n5 1924     7.2        5.0  23.06     80.3 110.9   2.2889\n6 1925     7.2        3.2  23.15     79.2 106.6   2.1735\n\n\n\n\nCode\nplot(x = divusa$year, y = divusa$divorce)\n\n\n\n\n\n\n\n\n\nCode\nplot(x = divusa$year, y = divusa$divorce, type = \"l\")\n\n\n\n\n\n\n\n\n\nLine plot is preferable for time ordered data.\n\n\n\n\n\nCode\nplot(\n  y = divusa$divorce[-1],\n  x = divusa$divorce[-nrow(divusa)],\n  xlab = \"Divorce rate in previous year\",\n  ylab = \"Divorce rate in current year\"\n)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nPrediction is possible; divorce rate in successive years is strongly correlated.\n\n\n\n\n\nCode\nq5c_coef &lt;- coef(lm(divorce ~ year, data = divusa))\nq5c_coef\n\n\n (Intercept)         year \n-422.9752984    0.2228009 \n\n\nBased on the equation \\(\\text{divorce} = 0.22 \\cdot \\text{year} - 422.98\\).\n\n\nCode\n(100 - q5c_coef[1]) / q5c_coef[2]\n\n\n(Intercept) \n   2347.277 \n\n\nAfter rearranging the equation, the year when divorce rate hits \\(100\\%\\) is \\(2347.277\\). It is not a realistic prediction, as this extrapolation over time is sure to stop at some point.\n\n\n\n\n\nCode\nggplot(data = divusa, aes(x = femlab, y = divorce, colour = year)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nHistorically, divorce rates were lower during periods when female participation in the labor force was also lower.\nAlso, both variables start low in the distant past as indicated by the darker coloured points and progress to higher values over time as indicated by the lighther coloured points.\nBut the change is not linear, especially during the Great Depression which caused a shift in the divorce rate."
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "Linear Model with R: Chapter 4",
    "section": "",
    "text": "Show the code\nset.seed(101)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\ny &lt;- x + rcauchy(n = n, location = 0, scale = 1)\nlm_q1a &lt;- lm(y ~ x)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5235 -1.1795 -0.1967  1.4101  7.2071 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.7567     0.9167  -0.826    0.416\nx             2.2328     1.5742   1.418    0.167\n\nResidual standard error: 2.573 on 28 degrees of freedom\nMultiple R-squared:  0.06703,   Adjusted R-squared:  0.03371 \nF-statistic: 2.012 on 1 and 28 DF,  p-value: 0.1671\n\n\nShow the code\nsummary(lm_q1a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n 2.2327980  1.5741752  1.4183923  0.1671113 \n\n\nShow the code\nplot(x, y)\nabline(lm_q1a)\n\n\n\n\n\n\n\n\n\nShow the code\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 2)\n\nfor (i in 1:n_simulations) {\n  y &lt;- x + rcauchy(n = n, location = 0, scale = 1)\n  lm_q1b &lt;- lm(y ~ x)\n  slope[i, 1] &lt;- summary(lm_q1b)$coef[2, 1] # First column contains estimates\n  slope[i, 2] &lt;- summary(lm_q1b)$coef[2, 2] # Second column contains std errors\n}\n\n\n\n\nShow the code\nsummary(slope[, 1])\n\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-4203.4762    -2.0135     0.8765     1.4804     3.7912  7815.6476 \n\n\n\n\nShow the code\nsummary(slope[, 2])\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.5636    2.2580    3.9197   21.2135    8.2978 4509.8138 \n\n\n\nAlthough the median of \\(0.8765\\) and the mean of \\(1.4804\\) align reasonably well with the expected value of one, the minimum value of \\(-4203.4762\\) and the maximum value of \\(7815.6476\\) indicate the presence of extreme outlier estimates.\nThe estimated standard errors display a large mean of \\(21.2135\\) and presence of extremely large values with a maximum ceiling of \\(4509.8138\\).\nThe theoretical properties of the least squares estimator require the error distribution to have finite mean and variance, but the Cauchy distribution violates these conditions, leading to unstable and sometimes extreme estimation results.\n\nIn statistics, the Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n\nLong-tailed error distributions are problematic because most simulated observations appear ordinary, creating an impression of stability. However, rare but severe outliers inevitably occur, and the estimator is not robust enough to handle these extreme values.\n\n\n\n\n\n\nThe problem uses a Student’s t-distribution with three degrees of freedom. The formula for the variance of a t-distribution with \\(\\nu\\) degree(s) of freedom is\n\\[\n\\operatorname{Var}(X) = \\frac{\\nu}{\\nu - 2}, \\qquad \\nu &gt; 2\n\\]\nReference: Statistical Inference (Casella and Berger, 2nd ed., p. 507).\nPlugging in \\(\\nu = 3\\):\n\\[\n\\operatorname{Var}(t_3) = \\frac{3}{3 - 2} = 3\n\\]\nTo force our error term \\(\\epsilon\\) to have a standard variance of one, we use the property of variance scaling:\n\\[\n\\operatorname{Var}(\\frac{X}{c}) = \\frac{1}{c^2} \\cdot \\operatorname{Var}(X)\n\\]\n\\[\n\\operatorname{Var}(\\frac{t_3}{\\sqrt{3}}) = \\frac{1}{(\\sqrt{3})^2} \\cdot \\operatorname{Var}(t_3)\n\\]\n\\[\n\\operatorname{Var}(\\frac{t_3}{\\sqrt{3}}) = \\frac{1}{3} \\cdot 3 = 1\n\\]\nBy dividing the generated numbers by \\(\\sqrt{3}\\), we retain the shape of the t-distribution (heavier tails, where the probability mass in the tails is controlled by the parameter \\(\\nu\\)). For \\(\\nu = 1\\), the Student’s t-distribution \\(t_v\\) becomes the standard Cauchy distributions, which has very fat tails; whereas for \\(\\nu \\rightarrow \\infty\\), it converges to the standard normal distribution \\(N (0, 1)\\)\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- rt(n = n, df = 3) / sqrt(3)\n  lm_q2a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q2a)$coef[2, ]\n  # slope[i, 1] &lt;- summary(lm_q2a)$coef[2, 1]  # First column contains estimates\n  # slope[i, 2] &lt;- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors\n  # slope[i, 3] &lt;- summary(lm_q2a)$coef[2, 3]  # Third column contains t values\n  # slope[i, 4] &lt;- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values\n}\n\nsummary(lm_q2a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.1193180  0.3959897 -0.3013158  0.7654001 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q2a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmean(slope[, 1])\n\n\n[1] 0.01451257\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = 0.0145\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\\[\n\\bar{\\hat{\\beta_1}} \\approx \\beta_1\n\\]\n\\[\n\\bar{\\hat{\\beta_1}} \\approx E(\\hat{\\beta_1})\n\\]\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 0.6064486\n\n\nThe standard deviation of \\(\\hat{\\beta_1} = 0.6064\\), which is close to the square root of the theoretical variance that was computed with Equation 2.21, \\(\\operatorname{Var}(\\hat{\\beta}) = (X^T X)^{-1} \\sigma^2\\), on page 57.\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.5622422\n\n\nThe mean of simulated \\(SE(\\hat{\\beta_1}) = 0.5622\\) does not agree with computed standard deviation of \\(\\hat{\\beta_1} = 0.6064\\).\nIn fact, it reported a value less than our previously computed result.\nRemember, \\(0.6064\\) is the actual standard deviation generated from our estimates and \\(0.5622\\) is the standard errors reported by the lm() function in each run. Take note that lm() assumes that the errors are normally distributed and does not have outliers (However, it is more precise to say that Normal distribution has “thin tails” which suggests outliers are possible but rare).\nNow, a skeptic might ask: “Maybe \\(0.5622\\) is just different from \\(0.6064\\) because of random chance (simulation noise).”\nTo address this, we compute a \\(95\\%\\) confidence interval:\n\n\nShow the code\nt.test(slope[, 2])$conf\n\n\n[1] 0.5558914 0.5685930\nattr(,\"conf.level\")\n[1] 0.95\n\n\nIn this specific case, the null hypothesis for the above test was: “The lm() model’s reported error is accurate (equal to 0.6064)”\nThe confidence interval rests between \\(0.5559\\) and \\(0.5686\\) of which \\(0.6064\\) is effectively far below the actual standard deviation. The difference between the “Reported Error and the”Actual Error” is statistically significant, proving that when errors have heavy tails (\\(t_3\\)), the standard linear model underestimates the uncertainty.\nIf we use standard least squares regression on data that has heavy-tailed errors (\\(t_3\\)):\n\nOur slope estimates are still approximately unbiased (close to zero).\nBut our standard errors are too small (biased downwards).\nThis meant that our confidence intervals will be too narrow and we will claim statistical significance too often, as our p-values will be artificially low due to presence of outlier(s).\n\n\n\n\n\n\nShow the code\nmean(slope[, 4] &lt; 0.05)\n\n\n[1] 0.042\n\n\n\n\n\n\nInitially, we generated the data with the assumption that there is no relationship to be found.\n\nNull hypothesis: \\(\\beta = 0\\).\n\nIf the model finds a “statistically significant” relationship, it is making a Type I error, or false positive error (Defined as the incorrect rejection of the true null hypothesis in statistical hypothesis testing).\nIf we set our confidence level to \\(95\\%, \\quad (\\alpha = 0.05)\\), we implicitly agreed to accept a \\(5\\%\\) error rate.\n\nThat is, we expect to be wrong \\(5\\%\\) of the time (nominal size of the error rate).\n\nIn \\(4.2\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant (actual size of the error rate).\nSince the figures are close to each another, the hypothesis test is behaving exactly as we initially assumed.\nDo take note, that in part (c), we have proved that the standard errors were biased downwards.\n\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\nHowever, the result here indicates that despite the errors being non-normal (\\(t_3\\)) and the standard errors being biased, the t-test is remarkly robust as it managed to preserve the correct Type I error rate.\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- arima.sim(\n    model = list(\n      order = c(1, 0, 0),\n      ar = 0.9\n    ),\n    n = 30,\n  )\n\n  lm_q3a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q3a)$coef[2, ]\n  # slope[i, 1] &lt;- summary(lm_q2a)$coef[2, 1]  # First column contains estimates\n  # slope[i, 2] &lt;- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors\n  # slope[i, 3] &lt;- summary(lm_q2a)$coef[2, 3]  # Third column contains t values\n  # slope[i, 4] &lt;- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values\n}\n\nsummary(lm_q3a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n1.61061670 0.77926910 2.06682994 0.04810891 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q3a)\n\n\n\n\n\n\n\n\n\nYes, the correlated errors are apparent.\n\n\n\n\n\nShow the code\nmean(slope[, 1])\n\n\n[1] -0.01671882\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = -0.0167\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 3.461398\n\n\nSD of the simulated \\(\\hat{\\beta_1}\\) is \\(3.4614\\).\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.8354084\n\n\nMean of simulated \\(\\SE(hat{\\beta_1})\\) is \\(0.8354\\).\nThe estimated standard error generated by lm() substantially underestimate the true standard error.\n\n\n\n\n\nShow the code\nround(mean(slope[, 4] &lt; 0.05) * 100, 1)\n\n\n[1] 62.6\n\n\n\nIn \\(62.6\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated SE for \\(\\beta_1\\).\nThe lm() function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\n\n\n\n\n\nIn statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor (referred to as a “common response variable”, “confounding factor”, or “lurking variable”).\nIn \\(62.6\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated SE for \\(\\beta_1\\).\nThe lm() function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\nThis time, unlike the previous question, we observed that when errors are strongly correlated, inference to a divergence from the uncorrelated errors assumption becomes inaccurate.\n\n\n\n\n\n\n\n\nTo enforce the requirement “SD proportional to \\(x\\)”, we multiply the standard deviation with \\(x\\) such that as \\(x\\) gets larger, the spread also get larger\n\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- rnorm(n = n, mean = 0, sd = x * 1)\n  lm_q4a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q4a)$coef[2, ]\n}\n\nsummary(lm_q4a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.2774077  0.3058648 -0.9069618  0.3721666 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q4a)\n\n\n\n\n\n\n\n\n\nYes, as \\(x\\) increases, the dispersion becomes much more apparent. In other words, the data exhibits clear heteroscedasticity.\n\n\n\n\n\nShow the code\nround(mean(slope[, 1]), 4)\n\n\n[1] -0.0027\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = -0.0027\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 0.3960678\n\n\nThe standard deviation of the simulated \\(\\hat{\\beta_1}\\) is \\(0.3961\\).\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.3502269\n\n\nThe mean of the simulated \\(SE(hat{\\beta_1})\\) is 0.3502269, which is slightly smaller than standard deviation of the simulated \\(\\hat{\\beta_1}\\). Thus, it underestimates the true standard error.\n\n\n\n\n\nShow the code\nround(mean(slope[, 4] &lt; 0.05) * 100, 1)\n\n\n[1] 8.2\n\n\n\nIn 8.2% of our 4000 simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated standard error for \\(\\beta_1\\).\nThe lm() function assumes homoscedasticity. It treats the high-variance data points (large \\(x\\)) as having the same precision as the low-variance data points. Because it fails to account for the extra noise introduced as \\(x\\) increases, the standard error formula yields a value that is too small.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\n\n\n\n\n\n\n\n\n\nShow the code\nedf &lt;- data.frame(\n  y &lt;- c(1.21, 1.13, 1.42, 1.01, 1.11, 0.94, 1.23, 1.04),\n  g &lt;- c(0, 0, 0, 0, 1, 1, 1, 1)\n)\n\n\n\n\nShow the code\nlm_q5a &lt;- lm(y ~ g, data = edf)\nlm_q5a_summary &lt;- summary(lm_q5a)\nlm_q5a_summary\n\n\n\nCall:\nlm(formula = y ~ g, data = edf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18250 -0.08188 -0.01125  0.06000  0.22750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19250    0.07469  15.967 3.83e-06 ***\ng           -0.11250    0.10562  -1.065    0.328    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1494 on 6 degrees of freedom\nMultiple R-squared:  0.159, Adjusted R-squared:  0.01885 \nF-statistic: 1.134 on 1 and 6 DF,  p-value: 0.3278\n\n\n\n\nShow the code\nlm_q5a_summary$fstatistic[1]\n\n\n   value \n1.134454 \n\n\n\\(\\text{F-statistics} = 1.1344538\\)\n\n\nShow the code\n1 -\n  pf(\n    q = lm_q5a_summary$fstatistic[1],\n    df1 = lm_q5a_summary$fstatistic[2],\n    df2 = lm_q5a_summary$fstatistic[3]\n  )\n\n\n    value \n0.3278007 \n\n\n\\(\\text{p-value} = 0.3278007\\)\n\n\n\n\n\nShow the code\nallcomb &lt;- combn(x = 8, m = 4)\nncol(allcomb)\n\n\n[1] 70\n\n\n70 combinations exist.\n\n\n\nWe use combinations instead of permutations because order within the group does not matter, the results are identical regardless of whether group 1 is defined as \\(\\{1, 3, 5, 9\\}\\) or \\(\\{9, 5, 1, 3\\}\\). Using combinations reduces the computational load significantly without losing information.\n\n\n\n\n\nShow the code\nfstats &lt;- numeric(ncol(allcomb))\nfor (i in 1:ncol(allcomb)) {\n  col_i &lt;- allcomb[, i]\n  selected_y_based_on_col_i &lt;- c(edf$y[col_i], edf$y[-col_i]) # The first 4 items belong to g = 0, the rest belong to g = 1\n  fstats[i] &lt;- summary(lm(selected_y_based_on_col_i ~ edf$g))$fstatistic[1]\n}\n\n\n\n\n\n\n\nShow the code\nmean(fstats &gt; lm_q5a_summary$fstatistic[1])\n\n\n[1] 0.3714286\n\n\nAs discussed in section 4.3, since the p-values computed based on the assumption of normal errors and those based on permutations (in this case, by means of combinations) are so close, many would prefer the normal assumption-based tests because they are quicker and easier to compute while the latter approach requires more work to compute despite making fewer assumptions.\n\n\n\n\n\n\n\n\nShow the code\ndata(happy, package = \"faraway\")\nlm_q6a &lt;- lm(happy ~ ., data = happy)\nsummary(lm_q6a)\n\n\n\nCall:\nlm(formula = happy ~ ., data = happy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7186 -0.5779 -0.1172  0.6340  2.0651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.072081   0.852543  -0.085   0.9331    \nmoney        0.009578   0.005213   1.837   0.0749 .  \nsex         -0.149008   0.418525  -0.356   0.7240    \nlove         1.919279   0.295451   6.496 1.97e-07 ***\nwork         0.476079   0.199389   2.388   0.0227 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.058 on 34 degrees of freedom\nMultiple R-squared:  0.7102,    Adjusted R-squared:  0.6761 \nF-statistic: 20.83 on 4 and 34 DF,  p-value: 9.364e-09\n\n\nOnly the predictor love was statistically significant at the \\(1\\%\\) level. The predictor work was significant at the \\(5\\%\\) level, but not at the \\(1\\%\\) level.”\n\n\n\n\n\nShow the code\ntable(happy$happy)\n\n\n\n 2  3  4  5  6  7  8  9 10 \n 1  1  4  5  2  8 14  3  1 \n\n\nThe summary shows the response variable is discrete, consisting only of integers. Since linear regression assumes a continuous response to generate normally distributed errors, this assumption is questionable here.\n\n\n\n\n\nShow the code\nset.seed(37)\nn_repetitions &lt;- 4000\ntstats &lt;- numeric(n_repetitions)\nfor (i in 1:n_repetitions) {\n  lm_q6c &lt;- lm(happy ~ sample(money) + sex + love + work, data = happy)\n  tstats[i] &lt;- summary(lm_q6c)$coef[2, 3]\n}\n\nmean(abs(tstats) &gt; abs(summary(lm_q6a)$coef[2, 3]))\n\n\n[1] 0.08125\n\n\nThe outcome returns 0.08125, which is very close to the observed normal-based p-value of 0.0749.\n\n\n\n\n\nShow the code\nhist(tstats, freq = FALSE, main = \"Distribution of Permuted t-Statistics\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nhist(tstats, freq = FALSE, main = \"Distribution of Permuted t-Statistics\")\ngrid &lt;- seq(-3, 3, length = 300)\nlines(grid, dt(grid, 34))"
  },
  {
    "objectID": "chapter4.html#exercises",
    "href": "chapter4.html#exercises",
    "title": "Linear Model with R: Chapter 4",
    "section": "",
    "text": "Show the code\nset.seed(101)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\ny &lt;- x + rcauchy(n = n, location = 0, scale = 1)\nlm_q1a &lt;- lm(y ~ x)\nsummary(lm_q1a)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5235 -1.1795 -0.1967  1.4101  7.2071 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.7567     0.9167  -0.826    0.416\nx             2.2328     1.5742   1.418    0.167\n\nResidual standard error: 2.573 on 28 degrees of freedom\nMultiple R-squared:  0.06703,   Adjusted R-squared:  0.03371 \nF-statistic: 2.012 on 1 and 28 DF,  p-value: 0.1671\n\n\nShow the code\nsummary(lm_q1a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n 2.2327980  1.5741752  1.4183923  0.1671113 \n\n\nShow the code\nplot(x, y)\nabline(lm_q1a)\n\n\n\n\n\n\n\n\n\nShow the code\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 2)\n\nfor (i in 1:n_simulations) {\n  y &lt;- x + rcauchy(n = n, location = 0, scale = 1)\n  lm_q1b &lt;- lm(y ~ x)\n  slope[i, 1] &lt;- summary(lm_q1b)$coef[2, 1] # First column contains estimates\n  slope[i, 2] &lt;- summary(lm_q1b)$coef[2, 2] # Second column contains std errors\n}\n\n\n\n\nShow the code\nsummary(slope[, 1])\n\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-4203.4762    -2.0135     0.8765     1.4804     3.7912  7815.6476 \n\n\n\n\nShow the code\nsummary(slope[, 2])\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.5636    2.2580    3.9197   21.2135    8.2978 4509.8138 \n\n\n\nAlthough the median of \\(0.8765\\) and the mean of \\(1.4804\\) align reasonably well with the expected value of one, the minimum value of \\(-4203.4762\\) and the maximum value of \\(7815.6476\\) indicate the presence of extreme outlier estimates.\nThe estimated standard errors display a large mean of \\(21.2135\\) and presence of extremely large values with a maximum ceiling of \\(4509.8138\\).\nThe theoretical properties of the least squares estimator require the error distribution to have finite mean and variance, but the Cauchy distribution violates these conditions, leading to unstable and sometimes extreme estimation results.\n\nIn statistics, the Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n\nLong-tailed error distributions are problematic because most simulated observations appear ordinary, creating an impression of stability. However, rare but severe outliers inevitably occur, and the estimator is not robust enough to handle these extreme values.\n\n\n\n\n\n\nThe problem uses a Student’s t-distribution with three degrees of freedom. The formula for the variance of a t-distribution with \\(\\nu\\) degree(s) of freedom is\n\\[\n\\operatorname{Var}(X) = \\frac{\\nu}{\\nu - 2}, \\qquad \\nu &gt; 2\n\\]\nReference: Statistical Inference (Casella and Berger, 2nd ed., p. 507).\nPlugging in \\(\\nu = 3\\):\n\\[\n\\operatorname{Var}(t_3) = \\frac{3}{3 - 2} = 3\n\\]\nTo force our error term \\(\\epsilon\\) to have a standard variance of one, we use the property of variance scaling:\n\\[\n\\operatorname{Var}(\\frac{X}{c}) = \\frac{1}{c^2} \\cdot \\operatorname{Var}(X)\n\\]\n\\[\n\\operatorname{Var}(\\frac{t_3}{\\sqrt{3}}) = \\frac{1}{(\\sqrt{3})^2} \\cdot \\operatorname{Var}(t_3)\n\\]\n\\[\n\\operatorname{Var}(\\frac{t_3}{\\sqrt{3}}) = \\frac{1}{3} \\cdot 3 = 1\n\\]\nBy dividing the generated numbers by \\(\\sqrt{3}\\), we retain the shape of the t-distribution (heavier tails, where the probability mass in the tails is controlled by the parameter \\(\\nu\\)). For \\(\\nu = 1\\), the Student’s t-distribution \\(t_v\\) becomes the standard Cauchy distributions, which has very fat tails; whereas for \\(\\nu \\rightarrow \\infty\\), it converges to the standard normal distribution \\(N (0, 1)\\)\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- rt(n = n, df = 3) / sqrt(3)\n  lm_q2a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q2a)$coef[2, ]\n  # slope[i, 1] &lt;- summary(lm_q2a)$coef[2, 1]  # First column contains estimates\n  # slope[i, 2] &lt;- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors\n  # slope[i, 3] &lt;- summary(lm_q2a)$coef[2, 3]  # Third column contains t values\n  # slope[i, 4] &lt;- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values\n}\n\nsummary(lm_q2a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.1193180  0.3959897 -0.3013158  0.7654001 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q2a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmean(slope[, 1])\n\n\n[1] 0.01451257\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = 0.0145\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\\[\n\\bar{\\hat{\\beta_1}} \\approx \\beta_1\n\\]\n\\[\n\\bar{\\hat{\\beta_1}} \\approx E(\\hat{\\beta_1})\n\\]\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 0.6064486\n\n\nThe standard deviation of \\(\\hat{\\beta_1} = 0.6064\\), which is close to the square root of the theoretical variance that was computed with Equation 2.21, \\(\\operatorname{Var}(\\hat{\\beta}) = (X^T X)^{-1} \\sigma^2\\), on page 57.\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.5622422\n\n\nThe mean of simulated \\(SE(\\hat{\\beta_1}) = 0.5622\\) does not agree with computed standard deviation of \\(\\hat{\\beta_1} = 0.6064\\).\nIn fact, it reported a value less than our previously computed result.\nRemember, \\(0.6064\\) is the actual standard deviation generated from our estimates and \\(0.5622\\) is the standard errors reported by the lm() function in each run. Take note that lm() assumes that the errors are normally distributed and does not have outliers (However, it is more precise to say that Normal distribution has “thin tails” which suggests outliers are possible but rare).\nNow, a skeptic might ask: “Maybe \\(0.5622\\) is just different from \\(0.6064\\) because of random chance (simulation noise).”\nTo address this, we compute a \\(95\\%\\) confidence interval:\n\n\nShow the code\nt.test(slope[, 2])$conf\n\n\n[1] 0.5558914 0.5685930\nattr(,\"conf.level\")\n[1] 0.95\n\n\nIn this specific case, the null hypothesis for the above test was: “The lm() model’s reported error is accurate (equal to 0.6064)”\nThe confidence interval rests between \\(0.5559\\) and \\(0.5686\\) of which \\(0.6064\\) is effectively far below the actual standard deviation. The difference between the “Reported Error and the”Actual Error” is statistically significant, proving that when errors have heavy tails (\\(t_3\\)), the standard linear model underestimates the uncertainty.\nIf we use standard least squares regression on data that has heavy-tailed errors (\\(t_3\\)):\n\nOur slope estimates are still approximately unbiased (close to zero).\nBut our standard errors are too small (biased downwards).\nThis meant that our confidence intervals will be too narrow and we will claim statistical significance too often, as our p-values will be artificially low due to presence of outlier(s).\n\n\n\n\n\n\nShow the code\nmean(slope[, 4] &lt; 0.05)\n\n\n[1] 0.042\n\n\n\n\n\n\nInitially, we generated the data with the assumption that there is no relationship to be found.\n\nNull hypothesis: \\(\\beta = 0\\).\n\nIf the model finds a “statistically significant” relationship, it is making a Type I error, or false positive error (Defined as the incorrect rejection of the true null hypothesis in statistical hypothesis testing).\nIf we set our confidence level to \\(95\\%, \\quad (\\alpha = 0.05)\\), we implicitly agreed to accept a \\(5\\%\\) error rate.\n\nThat is, we expect to be wrong \\(5\\%\\) of the time (nominal size of the error rate).\n\nIn \\(4.2\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant (actual size of the error rate).\nSince the figures are close to each another, the hypothesis test is behaving exactly as we initially assumed.\nDo take note, that in part (c), we have proved that the standard errors were biased downwards.\n\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\nHowever, the result here indicates that despite the errors being non-normal (\\(t_3\\)) and the standard errors being biased, the t-test is remarkly robust as it managed to preserve the correct Type I error rate.\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- arima.sim(\n    model = list(\n      order = c(1, 0, 0),\n      ar = 0.9\n    ),\n    n = 30,\n  )\n\n  lm_q3a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q3a)$coef[2, ]\n  # slope[i, 1] &lt;- summary(lm_q2a)$coef[2, 1]  # First column contains estimates\n  # slope[i, 2] &lt;- summary(lm_q2a)$coef[2, 2]  # Second column contains std errors\n  # slope[i, 3] &lt;- summary(lm_q2a)$coef[2, 3]  # Third column contains t values\n  # slope[i, 4] &lt;- summary(lm_q2a)$coef[2, 4]  # Fourth column contains p-values\n}\n\nsummary(lm_q3a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n1.61061670 0.77926910 2.06682994 0.04810891 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q3a)\n\n\n\n\n\n\n\n\n\nYes, the correlated errors are apparent.\n\n\n\n\n\nShow the code\nmean(slope[, 1])\n\n\n[1] -0.01671882\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = -0.0167\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 3.461398\n\n\nSD of the simulated \\(\\hat{\\beta_1}\\) is \\(3.4614\\).\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.8354084\n\n\nMean of simulated \\(\\SE(hat{\\beta_1})\\) is \\(0.8354\\).\nThe estimated standard error generated by lm() substantially underestimate the true standard error.\n\n\n\n\n\nShow the code\nround(mean(slope[, 4] &lt; 0.05) * 100, 1)\n\n\n[1] 62.6\n\n\n\nIn \\(62.6\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated SE for \\(\\beta_1\\).\nThe lm() function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\n\n\n\n\n\nIn statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor (referred to as a “common response variable”, “confounding factor”, or “lurking variable”).\nIn \\(62.6\\%\\) of our \\(4000\\) simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated SE for \\(\\beta_1\\).\nThe lm() function does not know about the correlation and calculates a standard error that is far too small as it sees the thirty datapoints and assumes each and every one of them provides a fresh, independent piece of information.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\nThis time, unlike the previous question, we observed that when errors are strongly correlated, inference to a divergence from the uncorrelated errors assumption becomes inaccurate.\n\n\n\n\n\n\n\n\nTo enforce the requirement “SD proportional to \\(x\\)”, we multiply the standard deviation with \\(x\\) such that as \\(x\\) gets larger, the spread also get larger\n\n\n\nShow the code\nset.seed(37)\nn &lt;- 30\nn_simulations &lt;- 4000\nx &lt;- seq(from = 0, to = 1, length.out = n)\nslope &lt;- matrix(data = NA, nrow = n_simulations, ncol = 4)\n\nfor (i in 1:n_simulations) {\n  y &lt;- rnorm(n = n, mean = 0, sd = x * 1)\n  lm_q4a &lt;- lm(y ~ x)\n  slope[i, ] &lt;- summary(lm_q4a)$coef[2, ]\n}\n\nsummary(lm_q4a)$coef[2, ]\n\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.2774077  0.3058648 -0.9069618  0.3721666 \n\n\n\n\nShow the code\nplot(x, y)\nabline(lm_q4a)\n\n\n\n\n\n\n\n\n\nYes, as \\(x\\) increases, the dispersion becomes much more apparent. In other words, the data exhibits clear heteroscedasticity.\n\n\n\n\n\nShow the code\nround(mean(slope[, 1]), 4)\n\n\n[1] -0.0027\n\n\nThe mean of our simulated estimates, \\(\\bar{\\hat{\\beta_1}} = -0.0027\\), is close to the true theoretical value of \\(\\beta_1 = 0\\) and the theoretical expectation \\(E(\\hat{\\beta_1}) = 0\\).\n\n\n\n\n\nShow the code\nsd(slope[, 1])\n\n\n[1] 0.3960678\n\n\nThe standard deviation of the simulated \\(\\hat{\\beta_1}\\) is \\(0.3961\\).\n\n\nShow the code\nmean(slope[, 2])\n\n\n[1] 0.3502269\n\n\nThe mean of the simulated \\(SE(hat{\\beta_1})\\) is 0.3502269, which is slightly smaller than standard deviation of the simulated \\(\\hat{\\beta_1}\\). Thus, it underestimates the true standard error.\n\n\n\n\n\nShow the code\nround(mean(slope[, 4] &lt; 0.05) * 100, 1)\n\n\n[1] 8.2\n\n\n\nIn 8.2% of our 4000 simulations, the model claimed that the slope was statistically significant even though the null hypothesis is true.\n\nThis is a consequence of the underestimated standard error for \\(\\beta_1\\).\nThe lm() function assumes homoscedasticity. It treats the high-variance data points (large \\(x\\)) as having the same precision as the low-variance data points. Because it fails to account for the extra noise introduced as \\(x\\) increases, the standard error formula yields a value that is too small.\nStandard errors are too small \\(\\longrightarrow\\) t-values are too big \\(\\longrightarrow\\) p-values are too small \\(\\longrightarrow\\) rejection rate should be high.\n\n\n\n\n\n\n\n\n\n\nShow the code\nedf &lt;- data.frame(\n  y &lt;- c(1.21, 1.13, 1.42, 1.01, 1.11, 0.94, 1.23, 1.04),\n  g &lt;- c(0, 0, 0, 0, 1, 1, 1, 1)\n)\n\n\n\n\nShow the code\nlm_q5a &lt;- lm(y ~ g, data = edf)\nlm_q5a_summary &lt;- summary(lm_q5a)\nlm_q5a_summary\n\n\n\nCall:\nlm(formula = y ~ g, data = edf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18250 -0.08188 -0.01125  0.06000  0.22750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19250    0.07469  15.967 3.83e-06 ***\ng           -0.11250    0.10562  -1.065    0.328    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1494 on 6 degrees of freedom\nMultiple R-squared:  0.159, Adjusted R-squared:  0.01885 \nF-statistic: 1.134 on 1 and 6 DF,  p-value: 0.3278\n\n\n\n\nShow the code\nlm_q5a_summary$fstatistic[1]\n\n\n   value \n1.134454 \n\n\n\\(\\text{F-statistics} = 1.1344538\\)\n\n\nShow the code\n1 -\n  pf(\n    q = lm_q5a_summary$fstatistic[1],\n    df1 = lm_q5a_summary$fstatistic[2],\n    df2 = lm_q5a_summary$fstatistic[3]\n  )\n\n\n    value \n0.3278007 \n\n\n\\(\\text{p-value} = 0.3278007\\)\n\n\n\n\n\nShow the code\nallcomb &lt;- combn(x = 8, m = 4)\nncol(allcomb)\n\n\n[1] 70\n\n\n70 combinations exist.\n\n\n\nWe use combinations instead of permutations because order within the group does not matter, the results are identical regardless of whether group 1 is defined as \\(\\{1, 3, 5, 9\\}\\) or \\(\\{9, 5, 1, 3\\}\\). Using combinations reduces the computational load significantly without losing information.\n\n\n\n\n\nShow the code\nfstats &lt;- numeric(ncol(allcomb))\nfor (i in 1:ncol(allcomb)) {\n  col_i &lt;- allcomb[, i]\n  selected_y_based_on_col_i &lt;- c(edf$y[col_i], edf$y[-col_i]) # The first 4 items belong to g = 0, the rest belong to g = 1\n  fstats[i] &lt;- summary(lm(selected_y_based_on_col_i ~ edf$g))$fstatistic[1]\n}\n\n\n\n\n\n\n\nShow the code\nmean(fstats &gt; lm_q5a_summary$fstatistic[1])\n\n\n[1] 0.3714286\n\n\nAs discussed in section 4.3, since the p-values computed based on the assumption of normal errors and those based on permutations (in this case, by means of combinations) are so close, many would prefer the normal assumption-based tests because they are quicker and easier to compute while the latter approach requires more work to compute despite making fewer assumptions.\n\n\n\n\n\n\n\n\nShow the code\ndata(happy, package = \"faraway\")\nlm_q6a &lt;- lm(happy ~ ., data = happy)\nsummary(lm_q6a)\n\n\n\nCall:\nlm(formula = happy ~ ., data = happy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7186 -0.5779 -0.1172  0.6340  2.0651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.072081   0.852543  -0.085   0.9331    \nmoney        0.009578   0.005213   1.837   0.0749 .  \nsex         -0.149008   0.418525  -0.356   0.7240    \nlove         1.919279   0.295451   6.496 1.97e-07 ***\nwork         0.476079   0.199389   2.388   0.0227 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.058 on 34 degrees of freedom\nMultiple R-squared:  0.7102,    Adjusted R-squared:  0.6761 \nF-statistic: 20.83 on 4 and 34 DF,  p-value: 9.364e-09\n\n\nOnly the predictor love was statistically significant at the \\(1\\%\\) level. The predictor work was significant at the \\(5\\%\\) level, but not at the \\(1\\%\\) level.”\n\n\n\n\n\nShow the code\ntable(happy$happy)\n\n\n\n 2  3  4  5  6  7  8  9 10 \n 1  1  4  5  2  8 14  3  1 \n\n\nThe summary shows the response variable is discrete, consisting only of integers. Since linear regression assumes a continuous response to generate normally distributed errors, this assumption is questionable here.\n\n\n\n\n\nShow the code\nset.seed(37)\nn_repetitions &lt;- 4000\ntstats &lt;- numeric(n_repetitions)\nfor (i in 1:n_repetitions) {\n  lm_q6c &lt;- lm(happy ~ sample(money) + sex + love + work, data = happy)\n  tstats[i] &lt;- summary(lm_q6c)$coef[2, 3]\n}\n\nmean(abs(tstats) &gt; abs(summary(lm_q6a)$coef[2, 3]))\n\n\n[1] 0.08125\n\n\nThe outcome returns 0.08125, which is very close to the observed normal-based p-value of 0.0749.\n\n\n\n\n\nShow the code\nhist(tstats, freq = FALSE, main = \"Distribution of Permuted t-Statistics\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nhist(tstats, freq = FALSE, main = \"Distribution of Permuted t-Statistics\")\ngrid &lt;- seq(-3, 3, length = 300)\nlines(grid, dt(grid, 34))"
  },
  {
    "objectID": "chapter3.html#proof-equivalence-of-f-test-and-t-test-for-a2",
    "href": "chapter3.html#proof-equivalence-of-f-test-and-t-test-for-a2",
    "title": "Linear_Model_with_R",
    "section": "",
    "text": "Goal: We aim to prove that for a one-way ANOVA with \\(a=2\\) groups, the F-statistic is exactly equal to the square of the t-statistic: \\[F_{1, N-2} = t_{N-2}^2\\]\n\n\n\nSample sizes: \\(n_1\\) and \\(n_2\\).\nTotal observations: \\(N = n_1 + n_2\\).\nGroup Means: \\(\\bar{y}_1\\) and \\(\\bar{y}_2\\).\nGlobal Mean: \\(\\bar{y}_{..} = \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_2}{N}\\).\n\n\n\n\n\nRecall that the Sum of Squares Error (SSE) is the sum of the squared deviations within each group. This can be written in terms of the individual group sample variances (\\(S_1^2\\) and \\(S_2^2\\)):\n\\[SSE = (n_1 - 1)S_1^2 + (n_2 - 1)S_2^2\\]\nThe Mean Square Error (\\(MSE\\)) is calculated by dividing \\(SSE\\) by its degrees of freedom (\\(N - a = N - 2\\)):\n\\[MSE = \\frac{SSE}{N-2} = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\\]\nKey Observation: This expression is the exact definition of the Pooled Variance (\\(S_p^2\\)) used in the two-sample t-test.\n\\[\\boxed{MSE = S_p^2}\\]\n\n\n\n\nThe Sum of Squares Treatment (SST) measures the weighted deviation of group means from the global mean. Since \\(a=2\\), the degrees of freedom is \\(2-1=1\\), so \\(MST = SST/1 = SST\\).\n\\[MST = SST = n_1(\\bar{y}_1 - \\bar{y}_{..})^2 + n_2(\\bar{y}_2 - \\bar{y}_{..})^2\\]\nTo simplify this efficiently, let’s analyze the deviation term \\((\\bar{y}_1 - \\bar{y}_{..})\\) separately before squaring.\n\n\nSubstitute the definition of the global mean:\n\\[\n\\begin{aligned}\n\\bar{y}_1 - \\bar{y}_{..} &= \\bar{y}_1 - \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_2}{N} \\\\\n&= \\frac{N\\bar{y}_1 - (n_1\\bar{y}_1 + n_2\\bar{y}_2)}{N} \\\\\n\\text{Since } N = n_1 + n_2: \\\\\n&= \\frac{(n_1 + n_2)\\bar{y}_1 - n_1\\bar{y}_1 - n_2\\bar{y}_2}{N} \\\\\n&= \\frac{n_1\\bar{y}_1 + n_2\\bar{y}_1 - n_1\\bar{y}_1 - n_2\\bar{y}_2}{N} \\\\\n&= \\frac{n_2(\\bar{y}_1 - \\bar{y}_2)}{N}\n\\end{aligned}\n\\]\nBy symmetry, the deviation for the second group is: \\[\\bar{y}_2 - \\bar{y}_{..} = -\\frac{n_1}{N}(\\bar{y}_1 - \\bar{y}_2)\\]\n\n\n\nNow, substitute these simplified deviations back into the SST equation:\n\\[\n\\begin{aligned}\nSST &= n_1 \\left[ \\frac{n_2}{N}(\\bar{y}_1 - \\bar{y}_2) \\right]^2 + n_2 \\left[ -\\frac{n_1}{N}(\\bar{y}_1 - \\bar{y}_2) \\right]^2 \\\\\n&= n_1 \\frac{n_2^2}{N^2}(\\bar{y}_1 - \\bar{y}_2)^2 + n_2 \\frac{n_1^2}{N^2}(\\bar{y}_1 - \\bar{y}_2)^2\n\\end{aligned}\n\\]\nFactor out the common term \\(\\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2}\\) and \\(n_1 n_2\\):\n\\[\n\\begin{aligned}\nSST &= \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2} \\left( n_1 n_2^2 + n_2 n_1^2 \\right) \\\\\n&= \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{N^2} \\cdot n_1 n_2 (n_2 + n_1)\n\\end{aligned}\n\\]\nSince \\((n_2 + n_1) = N\\), one \\(N\\) cancels out:\n\\[SST = \\frac{n_1 n_2}{N} (\\bar{y}_1 - \\bar{y}_2)^2\\]\nTo match the t-test format, we rewrite the coefficient \\(\\frac{n_1 n_2}{N}\\) using the reciprocal identity: \\[\\frac{n_1 n_2}{n_1 + n_2} = \\frac{1}{\\frac{n_1 + n_2}{n_1 n_2}} = \\frac{1}{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\nThus, we have our final expression for the Numerator:\n\\[\\boxed{MST = \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\n\n\n\n\nThe F-statistic is the ratio of the Numerator (Section 3) and the Denominator (Section 2):\n\\[\n\\begin{aligned}\nF &= \\frac{MST}{MSE} \\\\\nF &= \\frac{ \\left( \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{\\frac{1}{n_1} + \\frac{1}{n_2}} \\right) }{ S_p^2 }\n\\end{aligned}\n\\]\nWe can rearrange this fraction to isolate the squared terms:\n\\[F = \\frac{(\\bar{y}_1 - \\bar{y}_2)^2}{ S_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) }\\]\nThis is mathematically equivalent to the square of the t-statistic definition:\n\\[F = \\left( \\frac{\\bar{y}_1 - \\bar{y}_2}{ S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} } \\right)^2 = t^2\\]\nConclusion: We have proven that for \\(a=2\\), the ANOVA F-test yields the same result as the squared two-sample t-test.\n\n\nCode\nnear(summary(lm_q4b)$coefficients[5, 3]^2, anova(lm_q4a_i, lm_q4b)$F[2])\n\n\n[1] TRUE\n\n\n\n\n\n\\[\nF = \\frac{\\frac{TSS - RSS}{p - 1}}{\\frac{RSS}{n - p}}\n\\]\n\\[\nR^2 = 1 - \\frac{RSS}{TSS}\n\\]\nRearranging \\(F\\)\n\\[\nF \\cdot \\frac{RSS}{n - p} = \\frac{TSS - RSS}{p - 1}\n\\]\n\\[ F \\cdot \\frac{p - 1}{n - p} = \\frac{TSS - RSS}{RSS} \\]\n\\[\nF \\cdot \\frac{p - 1}{n - p} = \\frac{TSS}{RSS} - 1\n\\]\n\\[\n\\frac{TSS}{RSS} = F \\cdot \\frac{p - 1}{n - p} + 1\n\\]\n\\[\n\\frac{RSS}{TSS} = \\frac{n - p}{F(p - 1) + n - p}\n\\]\nSubstituting \\(\\frac{RSS}{TSS}\\)\n\\[\nR^2 = 1 - \\frac{n - p}{F(p - 1) + n - p}\n\\]\n\\[ R^2 = 1 - \\frac{n - p}{F(p - 1) + n - p} \\]\n\\[  F(p - 1) + n - p = \\frac{n - p}{1 - R^2} \\]\n\\[\nF(p - 1) = \\frac{n - p}{1 - R^2} + p - n\n\\]\n\\[\nF(p - 1) = \\frac{n - p}{1 - R^2} + \\frac{(p - n)(1 - R^2)}{1 - R^2}\n\\]\n\\[\nF(p - 1) = \\frac{n - p + p  - pR^2 - n + nR^2}{1 - R^2}\n\\]\n\\[\nF(p - 1) = \\frac{nR^2 - pR^2}{1 - R^2}\n\\]\n\\[\nF = \\frac{R^2 (n - p)}{(1 - R^2)(p - 1)}\n\\]\n\n\n\n\n\n\n\nCode\ndata(punting, package = \"faraway\")\n\n\n\n\nCode\nlm_q6a &lt;- lm(Distance ~ RStr + LStr + RFlex + LFlex, data = punting)\nsummary(lm_q6a)\n\n\n\nCall:\nlm(formula = Distance ~ RStr + LStr + RFlex + LFlex, data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.941  -8.958  -4.441  13.523  17.016 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -79.6236    65.5935  -1.214    0.259\nRStr          0.5116     0.4856   1.054    0.323\nLStr         -0.1862     0.5130  -0.363    0.726\nRFlex         2.3745     1.4374   1.652    0.137\nLFlex        -0.5277     0.8255  -0.639    0.541\n\nResidual standard error: 16.33 on 8 degrees of freedom\nMultiple R-squared:  0.7365,    Adjusted R-squared:  0.6047 \nF-statistic:  5.59 on 4 and 8 DF,  p-value: 0.01902\n\n\nNone.\n\n\n\n\\(\\text{p-value}_{F-test} = 0.01902 &lt; 0.05\\), hence we reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr} = \\beta_{RFlex} = \\beta_{LFlex} = 0\\).\n\n\n\n\n\nCode\nlm_q6c &lt;- lm(Distance ~ I(RStr + LStr) + RFlex + LFlex, data = punting)\nsummary(lm_q6c)\n\n\n\nCall:\nlm(formula = Distance ~ I(RStr + LStr) + RFlex + LFlex, data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.698  -9.494  -5.155   9.081  20.611 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    -71.2694    63.1447  -1.129    0.288\nI(RStr + LStr)   0.1741     0.1940   0.898    0.393\nRFlex            2.3137     1.4013   1.651    0.133\nLFlex           -0.5772     0.8035  -0.718    0.491\n\nResidual standard error: 15.94 on 9 degrees of freedom\nMultiple R-squared:  0.7174,    Adjusted R-squared:  0.6232 \nF-statistic: 7.615 on 3 and 9 DF,  p-value: 0.00769\n\n\nCode\nanova(lm_q6c, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ I(RStr + LStr) + RFlex + LFlex\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      9 2287.4                           \n2      8 2132.6  1    154.72 0.5804  0.468\n\n\n\\(\\text{p-value}_{F-test} = 0.468 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr}\\). We conclude that there is no significant difference between left leg strength and right leg strength.\n\n\n\n\n\nCode\nplot(ellipse(lm_q6a, c(2, 3)), type = \"l\")\npoints(lm_q6a$coef[\"RStr\"], lm_q6a$coef[\"LStr\"])\nabline(v = confint(lm_q6a)[2, c(1, 2)], lty = 4)\nabline(h = confint(lm_q6a)[3, c(1, 2)], lty = 4)\nabline(v = 0, lty = 2, col = \"blue\")\nabline(h = 0, lty = 2, col = \"blue\")\npoints(0, 0, pch = 21)\n\n\n\n\n\n\n\n\n\n\nOrigin (\\(\\beta_RStr = 0\\), \\(\\beta_LStr = 0\\)) falls within both the \\(95\\%\\) joint confidence ellipse and the rectangular region defined by the individual \\(95\\%\\) univariate confidence intervals.\nNeither predictor is statistically significant when considered in isolation within the full model.\nCrucially, because the origin lies inside the joint confidence ellipse, we do not reject the null hypothesis that both coefficients are simultaneously zero. This is consistent with our previous analysis.\n\n\n\n\nWe will obtain the same results as in q6c: Failing to reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr}\\) and return to the same conclusion that there is no significant difference between left leg strength and right leg strength, in other words, the total leg strength is sufficient (Total leg strength is defined by adding the right and left leg strengths together).\n\n\n\n\n\nCode\nlm_q6f &lt;- lm(Distance ~ RStr + LStr + I(RFlex + LFlex), data = punting)\nsummary(lm_q6f)\n\n\n\nCall:\nlm(formula = Distance ~ RStr + LStr + I(RFlex + LFlex), data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.510 -13.417   2.165   7.988  23.316 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      -44.26189   63.52790  -0.697    0.504\nRStr               0.70392    0.48904   1.439    0.184\nLStr               0.01518    0.51703   0.029    0.977\nI(RFlex + LFlex)   0.46194    0.43975   1.050    0.321\n\nResidual standard error: 17.15 on 9 degrees of freedom\nMultiple R-squared:  0.6728,    Adjusted R-squared:  0.5637 \nF-statistic: 6.168 on 3 and 9 DF,  p-value: 0.01451\n\n\nCode\nanova(lm_q6f, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ RStr + LStr + I(RFlex + LFlex)\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      9 2648.4                           \n2      8 2132.6  1    515.72 1.9346 0.2017\n\n\n\\(\\text{p-value}_{F-test} = 0.2017 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RFlex} = \\beta_{LFlex}\\). We conclude that there is no significant difference between left leg flexibility and right leg flexibility.\n\n\n\n\n\nCode\nlm_q6e &lt;- lm(Distance ~ I(RStr + LStr) + I(RFlex + LFlex), data = punting)\nsummary(lm_q6e)\n\n\n\nCall:\nlm(formula = Distance ~ I(RStr + LStr) + I(RFlex + LFlex), data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.948 -13.929   1.020   9.795  29.111 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)      -36.1525    60.9655  -0.593    0.566  \nI(RStr + LStr)     0.3700     0.1430   2.588    0.027 *\nI(RFlex + LFlex)   0.4093     0.4228   0.968    0.356  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.73 on 10 degrees of freedom\nMultiple R-squared:  0.6541,    Adjusted R-squared:  0.585 \nF-statistic: 9.457 on 2 and 10 DF,  p-value: 0.004948\n\n\nCode\nanova(lm_q6e, lm_q6a)\n\n\nAnalysis of Variance Table\n\nModel 1: Distance ~ I(RStr + LStr) + I(RFlex + LFlex)\nModel 2: Distance ~ RStr + LStr + RFlex + LFlex\n  Res.Df    RSS Df Sum of Sq    F Pr(&gt;F)\n1     10 2799.1                         \n2      8 2132.6  2    666.43 1.25  0.337\n\n\n\\(\\text{p-value}_{F-test} = 0.337 &gt; 0.05\\), hence we do not reject the null hypothesis \\(H_0: \\beta_{RStr} = \\beta_{LStr} AND \\beta_{RFlex} = \\beta_{LFlex}\\). We conclude that there is no significant difference between left-right symmetry for both strength and flexibility. Hence, symmetry is a reasonable claim.\n\n\n\n\n\nCode\nlm_q6f &lt;- lm(Hang ~ RStr + LStr + RFlex + LFlex, data = punting)\nsummary(lm_q6f)\n\n\n\nCall:\nlm(formula = Hang ~ RStr + LStr + RFlex + LFlex, data = punting)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36297 -0.13528 -0.07849  0.09938  0.35893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.225239   1.032784  -0.218    0.833\nRStr         0.005153   0.007645   0.674    0.519\nLStr         0.007697   0.008077   0.953    0.369\nRFlex        0.019404   0.022631   0.857    0.416\nLFlex        0.004614   0.012998   0.355    0.732\n\nResidual standard error: 0.2571 on 8 degrees of freedom\nMultiple R-squared:  0.8156,    Adjusted R-squared:  0.7235 \nF-statistic: 8.848 on 4 and 8 DF,  p-value: 0.004925\n\n\nNo, because they have different responses."
  }
]