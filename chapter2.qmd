---
title: "Linear_Model_with_R"
author: Han Chenyue
editor: source
format: pdf
---

# Chapter 2

## Exercises

### Q1

#### q1a

```{r}
data(teengamb, package = "faraway")
lm_q1a <- lm(gamble ~ sex + status + income + verbal,
            data = teengamb) 
summary(lm_q1a)
```

#### q1b

```{r}
cat(round(summary(lm_q1a)$r.squared * 100, 2), "%")
```

`Multiple R-square: 52.67%`

#### q1c

```{r}
cat("Index: ", which.max(residuals(lm_q1a)),
    "\nResidual", max(residuals(lm_q1a)))
```

#### q1d

```{r}
cat("Mean of residuals: ",
    signif(mean(residuals(lm_q1a)), 3),
    "\nMedian of residuals: ",
    signif(median(residuals(lm_q1a)), 3))
```

We expect the mean of the residuals from a linear model to be always zero (*assuming we do not drop the intercept term*, $\beta_0$), $\bar{\hat{\epsilon}} = 0$. Unlike the mean of the residuals, which is mathematically guaranteed to be zero in an OLS model, the median is not. Consequently, the median is often close to zero but rarely, if ever, exactly zero.

#### q1e

```{r}
cat("Correlation of the residuals with the fitted values: ",
signif(cor(residuals(lm_q1a), predict(lm_q1a)), 3))
```

The correlation is effectively zero because the residual vector $\hat{\epsilon}$ and fitted value vector $\hat{y}$ are orthogonal to each other.

#### q1f

```{r}
cat("Correlation of the residuals with income: ",
signif(cor(residuals(lm_q1a), teengamb$income), 3))
```

Yes, it will always be zero because geometrically, the OLS model projects the outcome vector $y$ onto the plane spanned by the set of all predictor variables. In our case, the `income` vector is part of that plane. Consequently, the fitted values vector $\hat{y}$ lies on this plane while the residuals vector $\hat{\epsilon}$ is orthogonal to the plane.

#### q1g

Female is coded as one and male is coded as zero.

```{r}
cat("For females, the difference in average weekly gambling between females and males, holding other factors constant, is", signif(summary(lm_q1a)$coefficients[2], 3), "pounds.")
```

### Q2

#### q2a

```{r}
data(uswages, package = "faraway")
lm_q2a <- lm(wage ~ educ + exper, data = uswages)
summary(lm_q2a)
```

#### q2b

```{r}
apply(uswages[, 1:3], 2, min)
```

#### q2c

```{r}
# When predictors are zero, the intercept is the prediction.
cat("Predicted wage for a worker with no education and no experience:",
format(round(summary(lm_q2a)$coef[1], 2), nsmall = 2))
```

It doesn't make sense to report a negative wage value.

```{r}
subset(uswages, educ == 0 | exper == 0)
```

```{r}
subset(uswages, educ == 0 & exper == 0)
```

According to the above output, the data confirms that while some workers have either zero education or zero experience, no worker has both simultaneously.

Therefore, the model's prediction for this scenario is a pure extrapolation, as it's forecasting for a data point that does not exist in the dataset.

#### q2d

```{r}
all.equal(cor(predict(lm_q2a), uswages$wage)^2, summary(lm_q2a)$r.squared)
```

#### q2e

```{r}
lm_q2e <- lm(wage ~ educ + exper - 1, data = uswages)
summary(lm_q2e)
```

$R^2$ has increased.

```{r}
cor(predict(lm_q2e), uswages$wage)^2
```

But direct calculation provides a similar value to the initial model.

#### q2f

```{r}
cat("RSS of Initial Model:",
sum(residuals(lm_q2a)^2),
"\nRSS of Intercept Removed Model:",
sum(residuals(lm_q2e)^2))
```

RSS of Initial Model (More parameter, more flexibility) \< RSS of Intercept Removed Model (One less parameter)

#### q2g

```{r}
round(coef(lm_q2a)['educ'], 2)
```

#### q2h

```{r}
lm_q2h <- lm(log(wage) ~ educ + exper, data = uswages)
summary(lm_q2h)
```

-   The `Residual standard error` is only 427.9 in the first model but in the logged response model, it is 0.6615 due to scaling.
-   $R^{2}_{\text{first model}} = 0.1351 < R^{2}_{\text{logged model}} = 0.1749$
    -   Since $R^2$ is unit free, the logged model is preferable.

#### q2i

First we need to know the value of `experience` on the unlogged wage scale by exponentiating both side.

$$
\text{wage} = e^{\beta_0 + \beta_1 \cdot \text{education} + \beta_2 \cdot \text{experience}}
$$

$$
\text{wage} = e^{\beta_0} \times e^{\beta_1 \cdot \text{education}} \times e^{\beta_2 \cdot \text{experience}}
$$

```{r}
# Need to unlogged the scaled wage values
round(exp(coef(lm_q2h)['educ']), 2)
```

An increase of one in education corresponds to multiplying the predicted response by $1.09$. This indicates that if education were to be increased by one, holding experience constant, we expect a 9% increase in wage.

#### q2j

```{r}
lm_q2j <- lm(wage ~ educ + exper + ne + mw + we + so, data = uswages)
summary(lm_q2j)
```

Lack of identifiability.

#### q2k

```{r}
rowsum <- uswages$ne + uswages$mw + uswages$we + uswages$so
head(rowsum)
sd(rowsum)
```

-   All four regional indicators sum to one because all mean are in one and only one region, thus by including all four indicators in the same model will run into the identifiability problem.
-   To resolve this, either drop one of the variable or drop the intercept term from the model.

### Q3

#### q3a

```{r}
data(prostate, package = "faraway")
head(prostate)
```

```{r}
lm_q3a <- lm(lpsa ~ lcavol, data = prostate)
lm_q3a_summary <- summary(lm_q3a)
sapply(lm_q3a_summary[c('sigma', 'r.squared')], round, 2)
```

$\sigma = 0.79$ $R^2 = 0.54$

#### q3b

```{r}
sigmas <- lm_q3a_summary$sigma
rsquares <- lm_q3a_summary$r.squared

lm_q3b <- lm(lpsa ~ lcavol + lweight, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

sapply(data.frame(npreds = 1:8, sigmas, rsquares), round, 4)
```

#### q3c

```{r}
plot(
    1:8,
    sigmas,
    xlab = "Number of Predictors",
    ylab = "Residual Standard Error",
    type = "l")
```

The RSE shows a sharp initial decrease with the number of predictors. The reduction continues, though not monotonically, reaching its minimum value when 7 predictors are included."

```{r}
plot(
    1:8,
    rsquares,
    xlab = "Number of Predictors",
    ylab = "R-Squared",
    type = "l")
```

Because the Residual Sum of Squares (RSS) can only decrease or stay the same when predictors are added, R-squared is monotonically non-decreasing with the number of predictors

### Q4

#### q4

```{r}
lm_q4a <- lm(lpsa ~ lcavol, data = prostate)

plot(lpsa ~ lcavol, data = prostate)
abline(lm_q4a)
```

Since it is a `lpsa` against `lcavol` plot, we cannot simply call `abline(lm_q4a_ii)` for the 2nd regression line since `lm(lcavol ~ lpsa, data = prostate)` predicts $x$ from $y$, that is, $x = a + by$. Thus, to plot it against the existing $y$ vs $x$ plot, the equation needs to be re-arranged to:

$$x = a + by$$

$$by = x - a$$

$$y = \frac{-a + x}{b}$$

$$y = \frac{-a}{b} + \frac{1}{b} \cdot x$$

Since $a$ is the intercept and $b$ is the slope.

```{r}
abline(a = -coef(lm_q4a_ii)[1] / coef(lm_q4a_ii)[2], b = 1 / coef(lm_q4a_ii)[2], col = "red")
```

The point of intersection for these two regression lines is the point of the means $\bar{x}, \bar{y}$ because this point is a fundamental property of any simple linear regression model calculated using the Ordinary Least Squares (OLS) method.

```{r}
mean_lcavol = mean(prostate$lcavol)
mean_lpsa = mean(prostate$lpsa)

points(
    x = mean_lcavol,
    y = mean_lpsa,
    pch = 19,
    col = "darkgreen",
    cex = 2.5
)

label_text = paste(
    "Mean (x̄, ȳ): (", 
    round(mean_lcavol, 2),
    ", ", 
    round(mean_lpsa, 2), 
    ")",
    sep = ""
)

text(
    x = -0.1,              # X-coordinate: Place it near the left side of the plot
    y = 3,              # Y-coordinate: Place it near the top of the plot
    labels = label_text,
    col = "darkgreen",  # Set a distinct color
    adj = 0,            # Left-align the text (0 = left, 0.5 = center, 1 = right)
    cex = 1.5           # Character expansion (text size)
)
```

### Q5

#### q5a

```{r}
data(cheddar, package = "faraway")
head(cheddar)
```

```{r}
lm_q5a <- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)
summary(lm_q5a)
```

#### q5b

```{r}
cor(fitted(lm_q5a), cheddar$taste)^2
```

This value appears in as the $R^2$ of `lm_q5a`'s output.

```{r}
library(dplyr)
near(summary(lm_q5a)$r.squared, cor(fitted(lm_q5a), cheddar$taste)^2)
```

#### q5c

```{r}
lm_q5c <- lm(taste ~ Acetic + H2S + Lactic - 1, data = cheddar)
round(summary(lm_q5c)$r.squared, 3)
```

Without the intercept term, $R^2 = 0.888$.

When compared to the correlation between the fitted values and the response squared, the latter is a much more plausible value.

####q5d

```{r}
X <- model.matrix(
    ~ Acetic + H2S + Lactic,
    data = cheddar
)
y <- cheddar$taste
qrx <- qr(X)
f = t(qr.Q(qrx)) %*% y
backsolve(qr.R(qrx), f)
```

### Q6

#### q6a

```{r}
data(wafer, package = "faraway")
head(wafer)
```

```{r}
lm_q6a <- lm(formula ~ resist ~ x1 + x2 + x3 + x4, data = wafer)
summary(lm_q6a, cor = T)
```

```{r}
X <- model.matrix(lm_q6a)
head(X)
```

The function automatically coded `-` and `+` as $0$ and $1$ respectively.

#### q6b

```{r}
cor(X)
```

The missing values are due to the intercept being constant across all observations. A constant variable has a standard deviation of zero. Because the correlation coefficient formula requires dividing by the product of the two variables' standard deviations, the zero value in the denominator makes the correlation mathematically undefined.

In other words:

$$\bar{x} = \frac{1 + 1 + 1 + \ldots}{n} = 1$$

$$\sigma = \sqrt{\frac{\sum{(x_i - \bar{x})^2}}{n - 1}}$$

$$\sigma = \sqrt{\frac{\sum{(1 - 1)^2}}{n - 1}} = \sqrt{\frac{\sum{(0)^2}}{n - 1}} = 0$$

Given the formula:

$$\text{Correlation} = \frac{cov(x, y)}{\sigma_x \sigma_y}$$

we can see that when the denominator is $0$, it will leads to a division by zero error.

#### q6c

```{r}
round(summary(lm_q6a)$coefficients[2], 2)
```

An increase in 25.76.

#### q6d

```{r}
lm_q6d <- lm(resist ~ x1 + x2 + x3, data = wafer)
summary(lm_q6d)
```

-   Estimates for `x1`, `x2`, and `x3` remain the same, but the intercept term's estimate has slightly decreased.
-   Standard errors have increased slightly for all estimands.

#### q6e

```{r}
summary(lm_q6a, cor = T)
```

The design matrix $X$ is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix. Orthogonality ensures that the estimated effect of each predictor is decoupled from the others. Therefore, the coefficient estimate for any given predictor is invariant to the inclusion or exclusion of other predictors in the model.

### Q7

#### q7a

```{r}
data(truck, package = "faraway")
head(truck)
```

```{r}
truck$B <- sapply(truck$B, function(x) ifelse(x == "-", -1, 1))
truck$C <- sapply(truck$C, function(x) ifelse(x == "-", -1, 1))
truck$D <- sapply(truck$D, function(x) ifelse(x == "-", -1, 1))
truck$E <- sapply(truck$E, function(x) ifelse(x == "-", -1, 1))
truck$O <- sapply(truck$O, function(x) ifelse(x == "-", -1, 1))
```

```{r}
lm_q7a <- lm(height ~ B + C + D + E + O, data = truck)
summary(lm_q7a, cor = T)
coef(lm_q7a)
```

#### q7b

```{r}
lm_q7b <- lm(height ~ B + C + D + E , data = truck)
summary(lm_q7b, cor = T)
coef(lm_q7b)
```

The coefficients remains unchanged.

Examining the $X$ matrix:

```{r}
cor(model.matrix(lm_q7a))
```

The design matrix $X$ is orthogonal, as evidenced by the zero off-diagonal entries in its correlation matrix

#### q7c

```{r}
truck_transformed <- transform(truck, A = B + C + D + E)
head(truck_transformed)
lm_q7c <- lm(height ~ A + B + C + D + E + O, data = truck_transformed)
coef(lm_q7c)
```

E is not estimable; A = B + C + D + E makes the predictors collinear, so R excluded E due to identifiability issues (*from pg31: Predictors occurring later in the model formula are preferred for removal ...*)

#### q7d

```{r}
X <- model.matrix(lm_q7c)
X
y <- truck$height
y
```

```{r}
try(solve(t(X) %*% X) %*% t(X) %*% y)
```

The model cannot be estimated due to linear dependence among the predictors, which makes the design matrix rank-deficient.

#### q7e

```{r}
qrx <- qr(X)
dim(qr.Q(qrx))
f <- t(qr.Q(qrx)) %*% y
backsolve(qr.R(qrx), f)
```

#### q7f

We will obtain the same results as `coef(lm_q7c)` running

```{r}
qr.coef(qrx, truck$height)
```

### Q8

#### q8a

$$
\begin{pmatrix}
1 & 0\\
1 & 0\\
1 & 0\\
1 & 1\\
1 & 1\\
1 & 1
\end{pmatrix}
$$

#### q8b

Given the matrix $X$ and vector $y$:

$$
X = \begin{pmatrix}
1 & 0 \\
1 & 0 \\
1 & 0 \\
1 & 1 \\
1 & 1 \\
1 & 1
\end{pmatrix}, \quad
y = \begin{pmatrix}
y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6
\end{pmatrix}
$$

We want to find the Least Squares estimator: $\hat{\beta} = (X^T X)^{-1} X^T y$.

$$
X^T = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1
\end{pmatrix}
$$

$$
X^T X = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 0 \\
1 & 0 \\
1 & 1 \\
1 & 1 \\
1 & 1
\end{pmatrix}
= \begin{pmatrix}
6 & 3 \\
3 & 3
\end{pmatrix}
$$

$$
(X^T X)^{-1} = \frac{1}{9} \begin{pmatrix}
3 & -3 \\
-3 & 6
\end{pmatrix} = \begin{pmatrix}
3/9 & -3/9 \\
-3/9 & 6/9
\end{pmatrix} = \begin{pmatrix}
1/3 & -1/3 \\
-1/3 & 2/3
\end{pmatrix}
$$

$$
X^T y = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6
\end{pmatrix}
= \begin{pmatrix}
y_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\
y_4 + y_5 + y_6
\end{pmatrix}
= \begin{pmatrix}
\sum_{i=1}^6 y_i \\
\sum_{i=4}^6 y_i
\end{pmatrix}
$$

$$
(X^T X)^{-1} (X^T y) =
\hat{\beta}
= \begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1
\end{pmatrix}
= \begin{pmatrix}
1/3 & -1/3 \\
-1/3 & 2/3
\end{pmatrix}
\begin{pmatrix}
y_1 + y_2 + y_3 + y_4 + y_5 + y_6\\
y_4 + y_5 + y_6
\end{pmatrix}
$$

$$
\begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1
\end{pmatrix}
= \begin{pmatrix}
\frac{y_1 + y_2 + y_3}{3}\\
\frac{y_4 + y_5 + y_6}{3} - \frac{y_1 + y_2 + y_3}{3}
\end{pmatrix}
= \begin{pmatrix} 
\frac{1}{3} \sum_{i = 1}^3 y_i\\
\frac{1}{3} ( \sum_{i = 4}^6 y_i - \sum_{i = 1}^3 y_i )
\end{pmatrix}
$$

$$
\hat{\beta}_0 = \frac{y_1 + y_2 + y_3}{3} = \frac{1}{3} \sum_{i = 1}^3 y_i
$$

Notice that $\hat{\beta}_0$ is the mean of all the observations from the first group. (Think of it as the reference group in linear regression)

$$
\hat{\beta}_1 = \frac{1}{3} ( \sum_{i = 4}^6 y_i - \sum_{i = 1}^3 y_i )
$$

Notice that $\hat{\beta}_1$ is the difference between the means of both groups.

#### q8c

$$
\begin{pmatrix}
1 & -1\\
1 & -1\\
1 & -1\\
1 & 1\\
1 & 1\\
1 & 1
\end{pmatrix}
$$

#### q8d

Given the matrix $X$ and vector $y$ (where Group 1 is -1 and Group 2 is +1):

$$
X = \begin{pmatrix}
1 & -1 \\
1 & -1 \\
1 & -1 \\
1 & 1 \\
1 & 1 \\
1 & 1
\end{pmatrix}, \quad
y = \begin{pmatrix}
y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6
\end{pmatrix}
$$

We want to find the Least Squares estimator: $\hat{\beta} = (X^T X)^{-1} X^T y$.

$$
X^T = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
-1 & -1 & -1 & 1 & 1 & 1
\end{pmatrix}
$$

$$
X^T X = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
-1 & -1 & -1 & 1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
1 & -1 \\
1 & -1 \\
1 & -1 \\
1 & 1 \\
1 & 1 \\
1 & 1
\end{pmatrix}
= \begin{pmatrix}
6 & 0 \\
0 & 6
\end{pmatrix}
$$

$$
(X^T X)^{-1} = \begin{pmatrix}
1/6 & 0 \\
0 & 1/6
\end{pmatrix}
$$

$$
X^T y = \begin{pmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
-1 & -1 & -1 & 1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6
\end{pmatrix}
= \begin{pmatrix}
y_1 + y_2 + y_3 + y_4 + y_5 + y_6 \\
-y_1 - y_2 - y_3 + y_4 + y_5 + y_6
\end{pmatrix}
= \begin{pmatrix}
\sum_{i=1}^6 y_i \\
\sum_{i=4}^6 y_i - \sum_{i=1}^3 y_i
\end{pmatrix}
$$

$$
(X^T X)^{-1} (X^T y) =
\hat{\beta}
= \begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1
\end{pmatrix}
= \begin{pmatrix}
1/6 & 0 \\
0 & 1/6
\end{pmatrix}
\begin{pmatrix}
\sum_{i=1}^6 y_i \\
\sum_{i=4}^6 y_i - \sum_{i=1}^3 y_i
\end{pmatrix}
$$

$$
\begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1
\end{pmatrix}
= \begin{pmatrix}
\frac{1}{6} \sum_{i = 1}^6 y_i \\
\frac{1}{6} ( \sum_{i = 4}^6 y_i - \sum_{i = 1}^3 y_i )
\end{pmatrix}
$$

$$
\hat{\beta}_0 = \frac{1}{6} \sum_{i = 1}^6 y_i
$$

Notice that $\hat{\beta}_0$ is now the grand mean (the average of all observations combined). All because -1 and 1 are centred around 0, and the intercept represents the centre of the data.

$$
\hat{\beta}_1 = \frac{1}{6} ( \sum_{i = 4}^6 y_i - \sum_{i = 1}^3 y_i )
$$

Notice that $\hat{\beta}_1$ is **half the difference** between the means of the two groups. (Since the difference in the x-axis "run" is now 2 units (from -1 to 1), the slope is halved compared to the 0/1 coding).

-   The first coding computes the mean of one group ($\hat{\beta}_0$) and then the difference in their means ($\hat{\beta}_1$).
-   The second computes an overall mean ($\hat{\beta}_0$) while the second parameter represents the difference from that overall mean ($\hat{\beta}_1$).
    -   The second coding is more symmetrical (but both result in the same fitted values and residuals).