---
title: "Linear_Model_with_R"
author: Han Chenyue
editor: source
format: pdf
---

# Chapter 2

## Exercises

### Q1

#### q1a

```{r}
data(teengamb, package = "faraway")
lm1 <- lm(gamble ~ sex + status + income + verbal,
            data = teengamb) 
summary(lm1)
```

#### q1b

```{r}
cat(round(summary(lm1)$r.squared * 100, 2), "%")
```

`Multiple R-square: 52.67%`

#### q1c

```{r}
cat("Index: ", which.max(residuals(lm1)),
    "\nResidual", max(residuals(lm1)))
```

#### q1d

```{r}
cat("Mean of residuals: ",
    signif(mean(residuals(lm1)), 3),
    "\nMedian of residuals: ",
    signif(median(residuals(lm1)), 3))
```

We expect the mean of the residuals from a linear model to be always zero (*assuming we do not drop the intercept term*, $\beta_0$), $\bar{\hat{\epsilon}} = 0$. Unlike the mean of the residuals, which is mathematically guaranteed to be zero in an OLS model, the median is not. Consequently, the median is often close to zero but rarely, if ever, exactly zero.

#### q1e

```{r}
cat("Correlation of the residuals with the fitted values: ",
signif(cor(residuals(lm1), predict(lm1)), 3))
```

The correlation is effectively zero because the residual vector $\hat{\epsilon}$ and fitted value vector $\hat{y}$ are orthogonal to each other.

#### q1f

```{r}
cat("Correlation of the residuals with income: ",
signif(cor(residuals(lm1), teengamb$income), 3))
```

Yes, it will always be zero because geometrically, the OLS model projects the outcome vector $y$ onto the plane spanned by the set of all predictor variables. In our case, the `income` vector is part of that plane. Consequently, the fitted values vector $hat{y}$ lies on this plane while the residuals vector $\hat{\epsilon}$ is orthogonal to the plane.

#### q1g

Female is coded as one and male is coded as zero.

```{r}
cat("Females gamble",
signif(summary(lm1)$coefficients[2], 3),
"pounds per week than males.")
```

### Q2

#### q2a

```{r}
data(uswages, package = "faraway")
lm_q2a <- lm(wage ~ educ + exper, data = uswages)
summary(lm_q2a)
```

#### q2b

```{r}
apply(uswages[, 1:3], 2, min)
```

#### q2c

```{r}
# When predictors are zero, the intercept is the prediction.
cat("Predicted wage for a worker with no education and no experience:",
format(round(summary(lm_q2a)$coef[1], 2), nsmall = 2))
```

It doesn't make sense to report a negative wage value.

```{r}
subset(uswages, educ == 0 | exper == 0)
```

```{r}
subset(uswages, educ == 0 & exper == 0)
```

According to the above output, the data confirms that while some workers have either zero education or zero experience, no worker has both simultaneously.

Therefore, the model's prediction for this scenario is a pure extrapolation, as it's forecasting for a data point that does not exist in the dataset.

#### q2d

```{r}
all.equal(cor(predict(lm_q2a), uswages$wage)^2, summary(lm_q2a)$r.squared)
```

#### q2e

```{r}
lm_q2e <- lm(wage ~ educ + exper - 1, data = uswages)
summary(lm_q2e)
```

$R^2$ has increased.

```{r}
cor(predict(lm_q2e), uswages$wage)^2
```

But direct calculation provides a similar value to the initial model.

#### q2f

```{r}
cat("RSS of Initial Model:",
sum(residuals(lm_q2a)^2),
"\nRSS of Intercept Removed Model:",
sum(residuals(lm_q2e)^2))
```

RSS of Initial Model (More parameter, more flexibility) \< RSS of Intercept Removed Model (One less parameter)

#### q2g

```{r}
round(coef(lm_q2a)['educ'], 2)
```

#### q2h

```{r}
lm_q2h <- lm(log(wage) ~ educ + exper, data = uswages)
summary(lm_q2h)
```

-   The `Residual standard error` is only 427.9 in the first model but in the logged response model, it is 0.6615 due to scaling.
-   $R^{2}_{\text{first model}} = 0.1351 < R^{2}_{\text{logged model}} = 0.1749$
    -   Since $R^2$ is unit free, the logged model is preferable.

#### q2i

First we need to know the value of `experience` on the unlogged wage scale by exponentiating both side.

$$
\text{wage} = e^{\beta_0 + \beta_1 \cdot \text{education} + \beta_2 \cdot \text{experience}}
$$

$$
\text{wage} = e^{\beta_0} \times e^{\beta_1 \cdot \text{education}} \times e^{\beta_2 \cdot \text{experience}}
$$

```{r}
# Need to unlogged the scaled wage values
round(exp(coef(lm_q2h)['educ']), 2)
```

An increase of one in education corresponds to multiplying the predicted response by $1.09$. This indicates that if education were to be increased by one, holding experience constant, we expect a 9% increase in wage.

#### q2j

```{r}
lm_q2j <- lm(wage ~ educ + exper + ne + mw + we + so, data = uswages)
summary(lm_q2j)
```

Lack of identifiability.

#### q2k

```{r}
rowsum <- uswages$ne + uswages$mw + uswages$we + uswages$so
head(rowsum)
sd(rowsum)
```

-   All four regional indicators sum to one because all mean are in one and only one region, thus by including all four indicators in the same model will run into the identifiability problem.
-   To resolve this, either drop one of the variable or drop the intercept term from the model.

### Q3

#### q3a

```{r}
data(prostate, package = "faraway")
head(prostate)
```

```{r}
lm_q3a <- lm(lpsa ~ lcavol, data = prostate)
lm_q3a_summary <- summary(lm_q3a)
sapply(lm_q3a_summary[c('sigma', 'r.squared')], round, 2)
```

$\sigma = 0.79$ $R^2 = 0.54$

#### q3b

```{r}
sigmas <- lm_q3a_summary$sigma
rsquares <- lm_q3a_summary$r.squared

lm_q3b <- lm(lpsa ~ lcavol + lweight, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

lm_q3b <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason, data = prostate)
lm_q3b_summary <- summary(lm_q3b)
sigmas <- c(sigmas, lm_q3b_summary$sigma)
rsquares <- c(rsquares, lm_q3b_summary$r.squared)

sapply(data.frame(npreds = 1:8, sigmas, rsquares), round, 4)
```

#### q3c

```{r}
plot(
    1:8,
    sigmas,
    xlab = "Number of Predictors",
    ylab = "Residual Standard Error",
    type = "l")
```

The RSE shows a sharp initial decrease with the number of predictors. The reduction continues, though not monotonically, reaching its minimum value when 7 predictors are included."

```{r}
plot(
    1:8,
    rsquares,
    xlab = "Number of Predictors",
    ylab = "R-Squared",
    type = "l")
```

Because the Residual Sum of Squares (RSS) can only decrease or stay the same when predictors are added, R-squared is monotonically non-decreasing with the number of predictors

### Q4

#### q4

```{r}
lm_q4a <- lm(lpsa ~ lcavol, data = prostate)

plot(lpsa ~ lcavol, data = prostate)
abline(lm_q4a)
```

Since it is a `lpsa` against `lcavol` plot, we cannot simply call `abline(lm_q4a_ii)` for the 2nd regression line since `lm(lcavol ~ lpsa, data = prostate)` predicts $x$ from $y$, that is, $x = a + by$. Thus, to plot it against the existing $y$ vs $x$ plot, the equation needs to be re-arranged to:

$$x = a + by$$

$$by = x - a$$

$$y = \frac{-a + x}{b}$$

$$y = \frac{-a}{b} + \frac{1}{b} \cdot x$$

Since $a$ is the intercept and $b$ is the slope.

```{r}
abline(a = -coef(lm_q4a_ii)[1] / coef(lm_q4a_ii)[2], b = 1 / coef(lm_q4a_ii)[2], col = "red")
```

The point of intersection for these two regression lines is the point of the means $\bar{x}, \bar{y}$ because this point is a fundamental property of any simple linear regression model calculated using the Ordinary Least Squares (OLS) method.

```{r}
mean_lcavol = mean(prostate$lcavol)
mean_lpsa = mean(prostate$lpsa)

points(
    x = mean_lcavol,
    y = mean_lpsa,
    pch = 19,
    col = "darkgreen",
    cex = 2.5
)

label_text = paste(
    "Mean (x̄, ȳ): (", 
    round(mean_lcavol, 2),
    ", ", 
    round(mean_lpsa, 2), 
    ")",
    sep = ""
)

text(
    x = -0.1,              # X-coordinate: Place it near the left side of the plot
    y = 3,              # Y-coordinate: Place it near the top of the plot
    labels = label_text,
    col = "darkgreen",  # Set a distinct color
    adj = 0,            # Left-align the text (0 = left, 0.5 = center, 1 = right)
    cex = 1.5           # Character expansion (text size)
)
```

### Q5

#### q5a

```{r}
data(cheddar, package = "faraway")
head(cheddar)
```

```{r}
lm_q5a <- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)
summary(lm_q5a)
```

#### q5b

```{r}
cor(fitted(lm_q5a), cheddar$taste)^2
```

This value appears in as the $R^2$ of `lm_q5a`'s output.

```{r}
library(dplyr)
near(summary(lm_q5a)$r.squared, cor(fitted(lm_q5a), cheddar$taste)^2)
```

#### q5c

```{r}
lm_q5c <- lm(taste ~ Acetic + H2S + Lactic - 1, data = cheddar)
round(summary(lm_q5c)$r.squared, 3)
```

Without the intercept term, $R^2 = 0.888$.

When compared to the correlation between the fitted values and the response squared, the latter is a much more plausible value.

####q5d

```{r}
X <- model.matrix(
    ~ Acetic + H2S + Lactic,
    data = cheddar
)
y <- cheddar$taste
qrx <- qr(X)
f = t(qr.Q(qrx)) %*% y
backsolve(qr.R(qrx), f)
```
