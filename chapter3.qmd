---
title: "Linear_Model_with_R"
author: Han Chenyue
editor: source
format: pdf
---

# Chapter 3

## Exercises

### Q1

#### q1a

```{r}
data(prostate, package = "faraway")
lm_q1a <- lm(lpsa ~ ., data = prostate) 
summary(lm_q1a)
```

```{r}
cat(
  "90% CI for age: ",
  round(
    summary(lm_q1a)$coef[4, 1] +
      c(-1, 1) * qt(0.95, 97 - 9) * summary(lm_q1a)$coef[4, 2],
    4
  )
)
cat("\n")
cat(
  "95% CI for age: ",
  round(
    summary(lm_q1a)$coef[4, 1] +
      c(-1, 1) * qt(0.975, 97 - 9) * summary(lm_q1a)$coef[4, 2],
    4
  )
```

Visualising it:

```{r}
library(ggplot2)
library(broom)
library(dplyr)

ci_90 <- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.90) |>
  filter(term == "age") |>
  mutate(level = "90% CI (p-value <= 0.10, alpha = 0.10)")

ci_95 <- tidy(lm_q1a, conf.int = TRUE, conf.level = 0.95) |>
  filter(term == "age") |>
  mutate(level = "95% CI (p-value <= 0.05, alpha = 0.05)")

ci_combined <- bind_rows(ci_90, ci_95)

ggplot(ci_combined, aes(x = estimate, y = level, colour = level)) +
  geom_errorbarh(
    aes(xmin = conf.low, xmax = conf.high),
    height = 0.1,
    linewidth = 1.5
  ) +
  geom_point(size = 5, shape = 19) +
  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    colour = "black",
    linewidth = 1
  ) +
  labs(
    title = "Bracketing the p-value for the 'age' Coefficient",
    x = "Coefficient Value",
  ) +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "none")
```

90% CI for age does not include 0 but 95% CI for age does, therefore the p-value lies within 0.05 and 0.10.

#### q1b

```{r}
library(ellipse)
plot(ellipse(lm_q1a, c(4, 5)), type = "l")
points(coef(lm_q1a)[4], coef(lm_q1a)[5])
abline(v = confint(lm_q1a)[4, c(1, 2)], lty = 4)
abline(h = confint(lm_q1a)[5, c(1, 2)], lty = 4)
abline(v = 0, lty = 2, col = "blue")
abline(h = 0, lty = 2, col = "blue")
points(0, 0, pch = 21)
```

-   Origin ($\beta_age = 0$, $\beta_lbph = 0$) falls within both the $95\%$ joint confidence ellipse and the rectangular region defined by the individual $95\%$ univariate confidence intervals.
-   Neither predictor is statistically significant when considered in isolation within the full model.
-   Crucially, because the origin lies inside the joint confidence ellipse, we do not reject the null hypothesis that both coefficients are simultaneously zero. This is consistent with the result of the formal F-test for nested models:

```{r}
lm_q1b <- lm(
  lpsa ~ lcavol + lweight + svi + lcp + gleason + pgg45,
  data = prostate
)
anova(lm_q1b, lm_q1a)
```

-   The non-significant p-value from this F-test confirms the visual evidence from the ellipse: we fail to find sufficient statistical evidence to reject the null hypothesis.

#### q1c

```{r}
lm_q1c <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
summary(lm_q1c)
anova(lm_q1c, lm_q1a)
```

Since `p-value` is much larger than $\alpha = 0.05$, we fail to reject the null hypothesis, $H_0: \beta_{age} = \beta_{lbph} = \beta_{lcp} = \beta_{gleason} = \beta_{pgg45} = 0$.

We conclude that the five predictors removed from the full model **do not collectively contribute statistically significant explanatory power** to the model, hence the smaller model is preferable.

### Q2

#### q2a

```{r}
data(cheddar, package = "faraway")
```

```{r}
lm_q2a <- lm(taste ~ ., data = cheddar)
summary(lm_q2a)
```

`H2S` and `Lactic` predictors are statistically significant at the $5\%$ level.

#### q2b

```{r}
lm_q2b <- lm(taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)
summary(lm_q2b)
```

`Lactic` predictor is statistically significant at the $5\%$ level.

#### q2c

```{r}
anova(lm_q2b, lm_q2a)
```

-   No, because an F-test is only valid for comparing nested models, where one model is a restricted version of the other (i.e. the predictors of the smaller model are a smaller subset of the larger model).
-   In this case, the models rely on different functional forms of the predictor \[$log(x)$ vs $x$\], thus the column space of one model is not a linear subspace the other.
-   Since $R^2_{lm\_q2a} = 0.6518 > R^2_{lm\_q2b} = 0.5754$ and $RSS_{lm\_q2a} = 2668.4 < RSS_{lm\_q2b} = 3253.6$, the first model provides a better fit to the data.

#### q2d

```{r}
cat("Taste would increase by", round(0.01*coef(lm_q2a)['H2S'], 3))
```

#### q2e

```{r}
cat(
  "The H2S on the original scale will increase by ",
  (exp(0.01) - 1) * 100,
  "% when adding 0.01 on the (natural) log scale as adding 0.01 to the log scale is equivalent to multiplying by exp(0.01) on the original scale.",
  sep = ""
)
```

### Q3

#### q3a

```{r}
data(teengamb, package = "faraway")
lm_q3a <- lm(gamble ~ ., data = teengamb)
summary(lm_q3a)
```

`Sex` and `Income` predictors are statistically significant at the $5\%$ level.

#### q3b

```{r}
cat(
  "For females, the difference in average weekly gambling between females and males, holding other factors constant, is",
  signif(summary(lm_q3a)$coefficients[2], 3),
  "pounds."
)
```

#### q3c

```{r}
lm_q3c <- lm(gamble ~ income, data = teengamb)
summary(lm_q3c)
```

```{r}
anova(lm_q3c, lm_q3a)
```

Since `p-value` is much smaller than $\alpha = 0.05$, we reject the null hypothesis, $H_0: \beta_{sex} = \beta_{status} = \beta_{verbal} = 0$.

We conclude that the full model is preferred.

### Q4

#### q4a

```{r}
data(sat, package = "faraway")
```

```{r}
lm_q4a_i <- lm(total ~ expend + ratio + salary, data = sat)
summary(lm_q4a_i)
```

Optional:

```{r}
# Dropping salary
lm_q4a_ii <- lm(total ~ expend + ratio, data = sat)
anova(lm_q4a_ii, lm_q4a_i)
```

```{r}
# Dropping all predictors
lm_q4a_iii <- lm(total ~ 1, data = sat)
anova(lm_q4a_iii, lm_q4a_i)
```

-   `salary` is not statistically significant ($\text{p-value}_\text{salary} = 0.0667 < 0.05$).
-   All the individual t-tests are not statistically significant yet the overall F-test is.
-   The p-value for the F-statistics is less than $\text{p-value}_\text{F-statistics} = 0.01209 < 0.05$, we reject the null hypothesis $H_0: \beta_\text{salary} = \beta_\text{ratio} = \beta_\text{expend} = 0$.
-   This indicates that at least one of the predictor has a statistically significant effect on the response.
-   This usually happens when the predictors are highly correlated with each other and we have a situation where the whole is greater than the sum of its parts.

#### q4b

```{r}
lm_q4b <- lm(total ~ expend + ratio + salary + takers, data = sat)
summary(lm_q4b)
```

```{r}
anova(lm_q4a_i, lm_q4b)
```

They output the same p-value.

Since $t_i = \frac{\hat{\beta}_i}{se(\hat{\beta}_i)}$

```{r}
library(dplyr)
near(
  summary(lm_q4b)$coefficients[5, 1] / summary(lm_q4b)$coefficients[5, 2],
  summary(lm_q4b)$coefficients[5, 3]
```

Since the sum-of-squares decomposition and F-statistics reduces to the usual equal-variance (pooled) two sample t-test in the case of $\alpha = 2$ treatments - with the realisation that an F-statistics with one numerator and $k$ denominator degrees of freedom is equivalent to a t-statistics with $k$ degrees of freedom, viz: $F_{1,k} = t^2_{k}$

[Link to Proof](https://canovasjm.netlify.app/2018/10/29/when-does-the-f-test-reduce-to-t-test/)

```{r}
near(summary(lm_q4b)$coefficients[5, 3]^2, anova(lm_q4a_i, lm_q4b)$F[2])
```

### Q5

$$
F = \frac{\frac{TSS - RSS}{p - 1}}{\frac{RSS}{n - p}}
$$

$$
R^2 = 1 - \frac{RSS}{TSS}
$$

Rearranging $F$

$$
F \cdot \frac{RSS}{n - p} = \frac{TSS - RSS}{p - 1}
$$

$$ F \cdot \frac{p - 1}{n - p} = \frac{TSS - RSS}{RSS} $$

$$
F \cdot \frac{p - 1}{n - p} = \frac{TSS}{RSS} - 1
$$

$$
\frac{TSS}{RSS} = F \cdot \frac{p - 1}{n - p} + 1
$$

$$
\frac{RSS}{TSS} = \frac{n - p}{F(p - 1) + n - p}
$$

Substituting $\frac{RSS}{TSS}$

$$
R^2 = 1 - \frac{n - p}{F(p - 1) + n - p}
$$

$$ R^2 = 1 - \frac{n - p}{F(p - 1) + n - p} $$

$$  F(p - 1) + n - p = \frac{n - p}{1 - R^2} $$

$$
F(p - 1) = \frac{n - p}{1 - R^2} + p - n
$$

$$
F(p - 1) = \frac{n - p}{1 - R^2} + \frac{(p - n)(1 - R^2)}{1 - R^2}
$$

$$
F(p - 1) = \frac{n - p + p  - pR^2 - n + nR^2}{1 - R^2}
$$

$$
F(p - 1) = \frac{nR^2 - pR^2}{1 - R^2}
$$

$$
F = \frac{R^2 (n - p)}{(1 - R^2)(p - 1)}
$$

### Q6

#### q6a

```{r}
data(punting, package = "faraway")
```

```{r}
lm_q6a <- lm(Distance ~ RStr + LStr + RFlex + LFlex, data = punting)
summary(lm_q6a)
```

None.

#### q6b

$\text{p-value}_{F-test} = 0.01902 < 0.05$, hence we reject the null hypothesis $H_0: \beta_{RStr} = \beta_{LStr} = \beta_{RFlex} = \beta_{LFlex} = 0$.

#### q6c

```{r}

```